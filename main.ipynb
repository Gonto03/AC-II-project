{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (project title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, InputLayer, Dropout, BatchNormalization, Convolution2D, MaxPooling2D, GlobalMaxPool2D\n",
    "from keras import activations, models, optimizers, losses\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving and analyzing the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 200)               800200    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 200)              800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 884,210\n",
      "Trainable params: 883,410\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(200, activation='relu',input_shape=(4000, ))) # input layer  #4000 = sample rate 1000 * 4sec audio\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(200,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(200,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))    # output layer\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            optimizer='adam')\n",
    "\n",
    "# summary\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 40, 334, 1)]      0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 40, 334, 1)       4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 38, 332, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 19, 166, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 19, 166, 32)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 17, 164, 64)       18496     \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 64)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,182\n",
      "Trainable params: 70,412\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nclass = 10\n",
    "inp = Input(shape=(40, 334, 1))        # MFCCs\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "audio = Convolution2D(32, kernel_size=(3, 3), activation=activations.relu)(norm_inp)\n",
    "audio = MaxPooling2D(pool_size=(2, 2))(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "audio = Convolution2D(64, kernel_size=(3, 3), activation=activations.relu)(audio)\n",
    "audio = GlobalMaxPool2D()(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(256, activation=activations.relu)(audio))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "cnn = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "cnn.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/audio_df.pkl\", 'rb') as f:\n",
    "    audio_df = pickle.load(f)\n",
    "    \n",
    "with open(\"../datasets/mfcc_df.pkl\", 'rb') as f:\n",
    "    mfcc_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 2.5408 - accuracy: 0.1510 - val_loss: 2.2556 - val_accuracy: 0.1134\n",
      "Epoch 2/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.2902 - accuracy: 0.1650 - val_loss: 2.2389 - val_accuracy: 0.1443\n",
      "Epoch 3/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.1952 - accuracy: 0.1892 - val_loss: 2.2340 - val_accuracy: 0.1592\n",
      "Epoch 4/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.1409 - accuracy: 0.1993 - val_loss: 2.2105 - val_accuracy: 0.1718\n",
      "Epoch 5/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 2.1043 - accuracy: 0.2192 - val_loss: 2.2152 - val_accuracy: 0.1730\n",
      "Epoch 6/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 2.0642 - accuracy: 0.2339 - val_loss: 2.2121 - val_accuracy: 0.1581\n",
      "Epoch 7/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.0111 - accuracy: 0.2578 - val_loss: 2.2050 - val_accuracy: 0.1913\n",
      "Epoch 8/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.9595 - accuracy: 0.2796 - val_loss: 2.1806 - val_accuracy: 0.1615\n",
      "Epoch 9/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.8928 - accuracy: 0.3114 - val_loss: 2.2124 - val_accuracy: 0.1489\n",
      "Epoch 10/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.8355 - accuracy: 0.3297 - val_loss: 2.2081 - val_accuracy: 0.1661\n",
      "Epoch 11/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.7664 - accuracy: 0.3639 - val_loss: 2.2100 - val_accuracy: 0.1684\n",
      "Epoch 12/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.6973 - accuracy: 0.3922 - val_loss: 2.2409 - val_accuracy: 0.1592\n",
      "Epoch 13/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.6394 - accuracy: 0.4093 - val_loss: 2.2512 - val_accuracy: 0.1638\n",
      "Epoch 14/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.5395 - accuracy: 0.4502 - val_loss: 2.3202 - val_accuracy: 0.1581\n",
      "Epoch 15/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.5082 - accuracy: 0.4544 - val_loss: 2.3134 - val_accuracy: 0.1775\n",
      "Epoch 16/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 1.4508 - accuracy: 0.4871 - val_loss: 2.3446 - val_accuracy: 0.1627\n",
      "Epoch 17/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.3816 - accuracy: 0.5050 - val_loss: 2.3924 - val_accuracy: 0.1523\n",
      "Epoch 18/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.3625 - accuracy: 0.5178 - val_loss: 2.4337 - val_accuracy: 0.1615\n",
      "Epoch 19/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2977 - accuracy: 0.5348 - val_loss: 2.4180 - val_accuracy: 0.1546\n",
      "Epoch 20/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.2646 - accuracy: 0.5531 - val_loss: 2.4396 - val_accuracy: 0.1558\n",
      "Epoch 21/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2216 - accuracy: 0.5622 - val_loss: 2.4726 - val_accuracy: 0.1627\n",
      "Epoch 22/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.1772 - accuracy: 0.5884 - val_loss: 2.6149 - val_accuracy: 0.1466\n",
      "Epoch 23/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1681 - accuracy: 0.5829 - val_loss: 2.5490 - val_accuracy: 0.1523\n",
      "Epoch 24/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1341 - accuracy: 0.6040 - val_loss: 2.4806 - val_accuracy: 0.1638\n",
      "Epoch 25/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1258 - accuracy: 0.6055 - val_loss: 2.6085 - val_accuracy: 0.1523\n",
      "Epoch 26/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0682 - accuracy: 0.6201 - val_loss: 2.5787 - val_accuracy: 0.1535\n",
      "Epoch 27/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0456 - accuracy: 0.6342 - val_loss: 2.5948 - val_accuracy: 0.1546\n",
      "Epoch 28/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0301 - accuracy: 0.6368 - val_loss: 2.6676 - val_accuracy: 0.1638\n",
      "Epoch 29/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0076 - accuracy: 0.6479 - val_loss: 2.5424 - val_accuracy: 0.1649\n",
      "Epoch 30/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0067 - accuracy: 0.6507 - val_loss: 2.7331 - val_accuracy: 0.1592\n",
      "Epoch 31/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9835 - accuracy: 0.6534 - val_loss: 2.5518 - val_accuracy: 0.1569\n",
      "Epoch 32/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9448 - accuracy: 0.6703 - val_loss: 2.6933 - val_accuracy: 0.1615\n",
      "Epoch 33/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9235 - accuracy: 0.6726 - val_loss: 2.6640 - val_accuracy: 0.1627\n",
      "Epoch 34/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9176 - accuracy: 0.6779 - val_loss: 2.6507 - val_accuracy: 0.1627\n",
      "Epoch 35/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9203 - accuracy: 0.6852 - val_loss: 2.6385 - val_accuracy: 0.1649\n",
      "Epoch 36/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9094 - accuracy: 0.6902 - val_loss: 2.6028 - val_accuracy: 0.1649\n",
      "Epoch 37/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8634 - accuracy: 0.7028 - val_loss: 2.6341 - val_accuracy: 0.1661\n",
      "Epoch 38/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8721 - accuracy: 0.6960 - val_loss: 2.7531 - val_accuracy: 0.1684\n",
      "Epoch 39/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8538 - accuracy: 0.7014 - val_loss: 2.7571 - val_accuracy: 0.1649\n",
      "Epoch 40/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8468 - accuracy: 0.7066 - val_loss: 2.7986 - val_accuracy: 0.1581\n",
      "Epoch 41/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8372 - accuracy: 0.7129 - val_loss: 2.7505 - val_accuracy: 0.1592\n",
      "Epoch 42/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8302 - accuracy: 0.7123 - val_loss: 2.7164 - val_accuracy: 0.1592\n",
      "Epoch 43/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8348 - accuracy: 0.7179 - val_loss: 2.6683 - val_accuracy: 0.1627\n",
      "Epoch 44/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8181 - accuracy: 0.7182 - val_loss: 2.7761 - val_accuracy: 0.1546\n",
      "Epoch 45/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7793 - accuracy: 0.7342 - val_loss: 2.7415 - val_accuracy: 0.1592\n",
      "Epoch 46/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7988 - accuracy: 0.7249 - val_loss: 2.7049 - val_accuracy: 0.1638\n",
      "Epoch 47/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7968 - accuracy: 0.7224 - val_loss: 2.5818 - val_accuracy: 0.1695\n",
      "Epoch 48/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7913 - accuracy: 0.7333 - val_loss: 2.6977 - val_accuracy: 0.1615\n",
      "Epoch 49/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7791 - accuracy: 0.7281 - val_loss: 2.8535 - val_accuracy: 0.1546\n",
      "Epoch 50/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7587 - accuracy: 0.7392 - val_loss: 2.6171 - val_accuracy: 0.1661\n",
      "Epoch 51/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7736 - accuracy: 0.7399 - val_loss: 2.5541 - val_accuracy: 0.1661\n",
      "Epoch 52/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7853 - accuracy: 0.7311 - val_loss: 2.5553 - val_accuracy: 0.1730\n",
      "Epoch 53/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7506 - accuracy: 0.7442 - val_loss: 2.6120 - val_accuracy: 0.1604\n",
      "Epoch 54/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7484 - accuracy: 0.7491 - val_loss: 2.6503 - val_accuracy: 0.1638\n",
      "Epoch 55/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7391 - accuracy: 0.7474 - val_loss: 2.6086 - val_accuracy: 0.1672\n",
      "Epoch 56/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7332 - accuracy: 0.7477 - val_loss: 2.6993 - val_accuracy: 0.1718\n",
      "Epoch 57/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7204 - accuracy: 0.7493 - val_loss: 2.6455 - val_accuracy: 0.1649\n",
      "Epoch 58/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7030 - accuracy: 0.7548 - val_loss: 2.5786 - val_accuracy: 0.1649\n",
      "Epoch 59/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7098 - accuracy: 0.7547 - val_loss: 2.6606 - val_accuracy: 0.1661\n",
      "Epoch 60/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7065 - accuracy: 0.7594 - val_loss: 2.6153 - val_accuracy: 0.1753\n",
      "Epoch 61/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7111 - accuracy: 0.7576 - val_loss: 2.6548 - val_accuracy: 0.1649\n",
      "Epoch 62/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6938 - accuracy: 0.7610 - val_loss: 2.7974 - val_accuracy: 0.1695\n",
      "Epoch 63/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6955 - accuracy: 0.7589 - val_loss: 2.6029 - val_accuracy: 0.1672\n",
      "Epoch 64/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6809 - accuracy: 0.7683 - val_loss: 2.7314 - val_accuracy: 0.1558\n",
      "Epoch 65/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6916 - accuracy: 0.7753 - val_loss: 2.5383 - val_accuracy: 0.1684\n",
      "Epoch 66/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6785 - accuracy: 0.7669 - val_loss: 2.6644 - val_accuracy: 0.1798\n",
      "Epoch 67/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6829 - accuracy: 0.7623 - val_loss: 2.6244 - val_accuracy: 0.1798\n",
      "Epoch 68/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6702 - accuracy: 0.7717 - val_loss: 2.6492 - val_accuracy: 0.1684\n",
      "Epoch 69/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6658 - accuracy: 0.7685 - val_loss: 2.5973 - val_accuracy: 0.1787\n",
      "Epoch 70/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6551 - accuracy: 0.7754 - val_loss: 2.8326 - val_accuracy: 0.1672\n",
      "Epoch 71/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6697 - accuracy: 0.7708 - val_loss: 2.5509 - val_accuracy: 0.1787\n",
      "Epoch 72/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6393 - accuracy: 0.7797 - val_loss: 2.6645 - val_accuracy: 0.1764\n",
      "Epoch 73/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6302 - accuracy: 0.7833 - val_loss: 2.5527 - val_accuracy: 0.1707\n",
      "Epoch 74/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6213 - accuracy: 0.7833 - val_loss: 2.5672 - val_accuracy: 0.1661\n",
      "Epoch 75/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 0.6297 - accuracy: 0.7856 - val_loss: 2.6638 - val_accuracy: 0.1558\n",
      "Epoch 76/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6280 - accuracy: 0.7876 - val_loss: 2.6473 - val_accuracy: 0.1649\n",
      "Epoch 77/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6442 - accuracy: 0.7790 - val_loss: 2.7680 - val_accuracy: 0.1787\n",
      "Epoch 78/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6249 - accuracy: 0.7885 - val_loss: 2.7783 - val_accuracy: 0.1592\n",
      "Epoch 79/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.7899 - val_loss: 2.7821 - val_accuracy: 0.1684\n",
      "Epoch 80/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6012 - accuracy: 0.8004 - val_loss: 2.7993 - val_accuracy: 0.1604\n",
      "Epoch 81/100\n",
      "246/246 [==============================] - 2s 8ms/step - loss: 0.6275 - accuracy: 0.7845 - val_loss: 2.7596 - val_accuracy: 0.1638\n",
      "Epoch 82/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.6145 - accuracy: 0.7909 - val_loss: 2.5487 - val_accuracy: 0.1718\n",
      "Epoch 83/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.6148 - accuracy: 0.7902 - val_loss: 2.7239 - val_accuracy: 0.1523\n",
      "Epoch 84/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.6100 - accuracy: 0.7986 - val_loss: 2.5897 - val_accuracy: 0.1649\n",
      "Epoch 85/100\n",
      "246/246 [==============================] - 2s 8ms/step - loss: 0.6114 - accuracy: 0.7965 - val_loss: 2.6620 - val_accuracy: 0.1672\n",
      "Epoch 86/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6000 - accuracy: 0.7955 - val_loss: 2.6598 - val_accuracy: 0.1684\n",
      "Epoch 87/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6151 - accuracy: 0.7904 - val_loss: 2.7291 - val_accuracy: 0.1604\n",
      "Epoch 88/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6021 - accuracy: 0.7934 - val_loss: 2.6150 - val_accuracy: 0.1695\n",
      "Epoch 89/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6049 - accuracy: 0.7940 - val_loss: 2.7308 - val_accuracy: 0.1581\n",
      "Epoch 90/100\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.6000 - accuracy: 0.7972 - val_loss: 2.5997 - val_accuracy: 0.1661\n",
      "Epoch 91/100\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.5812 - accuracy: 0.8015 - val_loss: 2.6982 - val_accuracy: 0.1684\n",
      "Epoch 92/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 0.5934 - accuracy: 0.8000 - val_loss: 2.7395 - val_accuracy: 0.1615\n",
      "Epoch 93/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.8023 - val_loss: 2.6848 - val_accuracy: 0.1615\n",
      "Epoch 94/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.5811 - accuracy: 0.8010 - val_loss: 2.6649 - val_accuracy: 0.1592\n",
      "Epoch 95/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.5690 - accuracy: 0.8070 - val_loss: 2.7417 - val_accuracy: 0.1672\n",
      "Epoch 96/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5694 - accuracy: 0.8085 - val_loss: 2.6530 - val_accuracy: 0.1695\n",
      "Epoch 97/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.5870 - accuracy: 0.8051 - val_loss: 2.6931 - val_accuracy: 0.1695\n",
      "Epoch 98/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5646 - accuracy: 0.8147 - val_loss: 2.5763 - val_accuracy: 0.1707\n",
      "Epoch 99/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5786 - accuracy: 0.8048 - val_loss: 2.7046 - val_accuracy: 0.1707\n",
      "Epoch 100/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5439 - accuracy: 0.8150 - val_loss: 2.6415 - val_accuracy: 0.1627\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "[[ 1 21  0  2  4  4  1  1  0  2]\n",
      " [ 1 90  1  2  1  1  3  0  0  1]\n",
      " [ 1 80  1  5  1  1  0  6  3  2]\n",
      " [ 0 86  0  5  3  0  1  0  4  1]\n",
      " [ 1 63  0  4 13  2  4  3  2  4]\n",
      " [ 0 23  0  2  2  2  1  1  1  3]\n",
      " [ 4 28  2 29 24  1 10  3 11  8]\n",
      " [ 0 75  1  1  0  5  2  2  0  0]\n",
      " [ 1 52  3 14  8  3  8  0  6  5]\n",
      " [ 0 40  3 11 10  3  8  1 12 12]]\n",
      "\n",
      "MLP - fold 1 done\n",
      "\n",
      "Epoch 1/10\n",
      " 61/246 [======>.......................] - ETA: 41s - loss: 2.2882 - acc: 0.2305"
     ]
    }
   ],
   "source": [
    "mlp_accuracies = []\n",
    "cnn_accuracies = []\n",
    "mlp_conf_matrices = []\n",
    "cnn_conf_matrices = []\n",
    "for i in range(10):\n",
    "    tf.keras.backend.clear_session()\n",
    "    train_df = audio_df.drop(audio_df[audio_df['fold'] == i+1].index)\n",
    "    test_df = audio_df.drop(audio_df[audio_df['fold'] != i+1].index)\n",
    "\n",
    "    X_train = np.array(train_df['audio'].tolist())\n",
    "    X_test = np.array(test_df['audio'].tolist())\n",
    "    y_train = np.array(train_df['label'].tolist())\n",
    "    y_test = np.array(test_df['label'].tolist())\n",
    "    \n",
    "    # training the MLP\n",
    "    n_epochs_mlp = 100\n",
    "    n_batch_size_mlp = 32\n",
    "    mlp.fit(X_train, y_train, batch_size=n_batch_size_mlp, epochs=n_epochs_mlp, validation_data=(X_test, y_test))\n",
    "    y_pred = mlp.predict(X_test,n_batch_size_mlp)\n",
    "    \n",
    "    # performance metrics for MLP\n",
    "    accuracy = mlp.evaluate(X_test,y_test,verbose=0)[1]\n",
    "    mlp_accuracies.append(accuracy)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    mlp_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    mlp_conf_matrices.append(mlp_conf_matrix)\n",
    "    \n",
    "    print(f\"\\nMLP - fold {i+1} done\\n\")\n",
    "    \n",
    "    # getting training and test sets containing the MFCCs\n",
    "    train_df = mfcc_df.drop(mfcc_df[mfcc_df['fold'] == i+1].index)\n",
    "    test_df = mfcc_df.drop(mfcc_df[mfcc_df['fold'] != i+1].index)\n",
    "    \n",
    "    X_train = np.array(train_df['mfcc'].tolist())\n",
    "    X_test = np.array(test_df['mfcc'].tolist())\n",
    "    y_train = np.array(train_df['label'].tolist())\n",
    "    y_test = np.array(test_df['label'].tolist())\n",
    "    \n",
    "    # training the CNN\n",
    "    n_epochs_cnn = 10\n",
    "    n_batch_size_cnn = 32\n",
    "    cnn.fit(X_train,y_train,epochs=n_epochs_cnn,batch_size=n_batch_size_cnn)\n",
    "    y_pred = cnn.predict(X_test,n_batch_size_cnn)\n",
    "    \n",
    "    # performance metrics for CNN\n",
    "    accuracy = cnn.evaluate(X_test,y_test,verbose=0)[1]\n",
    "    cnn_accuracies.append(accuracy)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    cnn_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cnn_conf_matrices.append(cnn_conf_matrix)\n",
    "    \n",
    "    print(f\"\\nCNN - fold {i+1} done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UrbanSound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
