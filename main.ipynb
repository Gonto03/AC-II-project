{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban sound data classification using deep learning classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, InputLayer, Dropout, BatchNormalization, Convolution2D, MaxPooling2D, GlobalMaxPool2D\n",
    "from keras import activations, models, optimizers, losses\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving and analyzing the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('../UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is the representation of categorical variables as binary vectors. We apply it to the labels, so they are in a format suitable for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label_id):\n",
    "    onehot = [0]*10   # length of labels array\n",
    "    onehot[label_id-1]=1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflective padding: our alternative to zero padding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflective padding:\n",
    "\n",
    "    Reflecting the signal at its boundaries instead of zero padding can help preserve the continuity of the signal, preventing alterations of the signal's characteristics, something that might occur with zero padding.\n",
    "    It can also help reducing artifacts at the edges of the signal and providing a smooth transition from the original signal to the padded region.\n",
    "    We believe this approach is effective with sound data, due to its symmetric nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflective_padding(signal, target_duration, target_rate):\n",
    "    target_duration = target_duration*target_rate\n",
    "    current_duration = len(signal)\n",
    "    \n",
    "    # Calculate the required padding on each side\n",
    "    padding_needed = target_duration - current_duration\n",
    "    left_padding = padding_needed // 2\n",
    "    right_padding = padding_needed - left_padding\n",
    "    \n",
    "    # Reflective padding on both sides\n",
    "    padded_signal = np.pad(signal, (left_padding, right_padding), 'reflect')\n",
    "    \n",
    "    return padded_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and resampling all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_time = 4    # each signal will have 4 seconds of duration\n",
    "target_rate = 1000    # resampling frequence\n",
    "\n",
    "# MFCC parameters\n",
    "n_mfcc=40\n",
    "hop_length=round(target_rate*0.0125)\n",
    "win_length=round(target_rate*0.023)\n",
    "n_fft=2**14\n",
    "mfcc_time_size = 4*target_rate//hop_length+1\n",
    "\n",
    "dataset = []        # [audio, label, fold]\n",
    "dataset_mfcc = []   # [MFCCs, label, fold]\n",
    "labels = 10 * [\"\"]   # array that stores the labels by the order specified by the column \"classID\" in the metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample, we retrieve the audio signal, the MFCC features, the fold it belongs to and the label (one-hot encoded).\n",
    "We store these informations in two dataframes: one containing the audio signals and one containing the MFCC features.\n",
    "We also store the labels in an array where its indexes correspond to the \"classID\" retrieved from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in metadata.iterrows():\n",
    "    # for fold\n",
    "    fold = row[\"fold\"]\n",
    "\n",
    "    # for audio\n",
    "    signal, rate = librosa.load(f\"../UrbanSound8K/audio/fold{fold}/\"+row[\"slice_file_name\"], sr=None)\n",
    "    new_signal = librosa.resample(signal, orig_sr=rate, target_sr=target_rate)\n",
    "    if len(new_signal) < 4*target_rate:\n",
    "        new_signal = reflective_padding(new_signal, 4, target_rate)\n",
    "    audio = new_signal[:4000]\n",
    "    \n",
    "    # MFCCs\n",
    "    sig_mfcc = librosa.feature.mfcc(y=new_signal,sr=target_rate,n_fft=n_fft,hop_length=hop_length,win_length=win_length,n_mfcc=n_mfcc)\n",
    "    sig_mfcc = sig_mfcc[:,:334]\n",
    "\n",
    "    # for label\n",
    "    classID = row[\"classID\"]\n",
    "    label = one_hot_encode(classID)\n",
    "    if labels[classID] == \"\":\n",
    "        labels[classID] = row[\"class\"]\n",
    "\n",
    "    dataset.append([audio, label, fold])\n",
    "    dataset_mfcc.append([sig_mfcc, label, fold])\n",
    "\n",
    "audio_df = pd.DataFrame(dataset, columns=[\"audio\",\"label\",\"fold\"])\n",
    "mfcc_df = pd.DataFrame(dataset_mfcc, columns=[\"mfcc\",\"label\",\"fold\"])\n",
    "print(\"Dataframe containing the audio signals:\")\n",
    "print(audio_df.tail())\n",
    "print(\"\\nDataframe containing the MFCC features:\")\n",
    "print(mfcc_df.tail())\n",
    "print(f\"\\nOrdered labels: {labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/audio_df.pkl\", 'wb') as f:\n",
    "    pickle.dump(audio_df, f)\n",
    "\n",
    "with open(\"../datasets/mfcc_df.pkl\", 'wb') as f:\n",
    "    pickle.dump(mfcc_df, f)\n",
    "    \n",
    "with open(\"../datasets/labels.pkl\", 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 200)               800200    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 200)              800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 884,210\n",
      "Trainable params: 883,410\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(200, activation='relu',input_shape=(4000, ))) # input layer  #4000 = sample rate 1000 * 4sec audio\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(200,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(200,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax'))    # output layer\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            optimizer='adam')\n",
    "# summary\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlp():\n",
    "    mlp = Sequential()\n",
    "    mlp.add(Dense(200, activation='relu',input_shape=(4000, ))) # input layer  #4000 = sample rate 1000 * 4sec audio\n",
    "    mlp.add(Dropout(0.5))\n",
    "    mlp.add(BatchNormalization())\n",
    "    mlp.add(Dense(200,activation='relu'))\n",
    "    mlp.add(Dropout(0.5))\n",
    "    mlp.add(BatchNormalization())\n",
    "    mlp.add(Dense(200,activation='relu'))\n",
    "    mlp.add(Dropout(0.5))\n",
    "    mlp.add(Dense(10, activation='softmax'))    # output layer\n",
    "\n",
    "    mlp.compile(loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            optimizer='adam')\n",
    "    \n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 40, 334, 1)]      0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 40, 334, 1)       4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 38, 332, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 19, 166, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 19, 166, 32)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 17, 164, 64)       18496     \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 64)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,182\n",
      "Trainable params: 70,412\n",
      "Non-trainable params: 770\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nclass = 10\n",
    "inp = Input(shape=(40, 334, 1))        # MFCCs\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "audio = Convolution2D(32, kernel_size=(3, 3), activation=activations.relu)(norm_inp)\n",
    "audio = MaxPooling2D(pool_size=(2, 2))(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "audio = Convolution2D(64, kernel_size=(3, 3), activation=activations.relu)(audio)\n",
    "audio = GlobalMaxPool2D()(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(256, activation=activations.relu)(audio))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "cnn = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "cnn.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn():    \n",
    "    nclass = 10\n",
    "    inp = Input(shape=(40, 334, 1))        # MFCCs\n",
    "    norm_inp = BatchNormalization()(inp)\n",
    "    audio = Convolution2D(32, kernel_size=(3, 3), activation=activations.relu)(norm_inp)\n",
    "    audio = MaxPooling2D(pool_size=(2, 2))(audio)\n",
    "    audio = Dropout(rate=0.1)(audio)\n",
    "    audio = Convolution2D(64, kernel_size=(3, 3), activation=activations.relu)(audio)\n",
    "    audio = GlobalMaxPool2D()(audio)\n",
    "    audio = Dropout(rate=0.1)(audio)\n",
    "\n",
    "    dense_1 = BatchNormalization()(Dense(256, activation=activations.relu)(audio))\n",
    "    dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "    cnn = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam()\n",
    "\n",
    "    cnn.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/audio_df.pkl\", 'rb') as f:\n",
    "    audio_df = pickle.load(f)\n",
    "    \n",
    "with open(\"../datasets/mfcc_df.pkl\", 'rb') as f:\n",
    "    mfcc_df = pickle.load(f)\n",
    "    \n",
    "with open(\"../datasets/labels.pkl\", 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 2.5452 - accuracy: 0.1416 - val_loss: 2.2696 - val_accuracy: 0.1489\n",
      "Epoch 2/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.2914 - accuracy: 0.1729 - val_loss: 2.2490 - val_accuracy: 0.1649\n",
      "Epoch 3/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.1962 - accuracy: 0.1841 - val_loss: 2.2285 - val_accuracy: 0.1649\n",
      "Epoch 4/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 2.1551 - accuracy: 0.1934 - val_loss: 2.2119 - val_accuracy: 0.1546\n",
      "Epoch 5/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.1058 - accuracy: 0.2143 - val_loss: 2.2172 - val_accuracy: 0.1638\n",
      "Epoch 6/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 2.0648 - accuracy: 0.2322 - val_loss: 2.2061 - val_accuracy: 0.1856\n",
      "Epoch 7/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 2.0129 - accuracy: 0.2653 - val_loss: 2.2098 - val_accuracy: 0.1707\n",
      "Epoch 8/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.9609 - accuracy: 0.2839 - val_loss: 2.1883 - val_accuracy: 0.1592\n",
      "Epoch 9/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.8995 - accuracy: 0.3031 - val_loss: 2.1855 - val_accuracy: 0.1661\n",
      "Epoch 10/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.8337 - accuracy: 0.3355 - val_loss: 2.2038 - val_accuracy: 0.1764\n",
      "Epoch 11/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.7565 - accuracy: 0.3685 - val_loss: 2.2123 - val_accuracy: 0.1672\n",
      "Epoch 12/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.6996 - accuracy: 0.4008 - val_loss: 2.2305 - val_accuracy: 0.1627\n",
      "Epoch 13/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.6131 - accuracy: 0.4193 - val_loss: 2.2431 - val_accuracy: 0.1730\n",
      "Epoch 14/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.5647 - accuracy: 0.4413 - val_loss: 2.2863 - val_accuracy: 0.1604\n",
      "Epoch 15/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.4825 - accuracy: 0.4767 - val_loss: 2.2749 - val_accuracy: 0.1753\n",
      "Epoch 16/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.4413 - accuracy: 0.4906 - val_loss: 2.3125 - val_accuracy: 0.1638\n",
      "Epoch 17/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.3889 - accuracy: 0.5086 - val_loss: 2.3164 - val_accuracy: 0.1695\n",
      "Epoch 18/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.3495 - accuracy: 0.5253 - val_loss: 2.3793 - val_accuracy: 0.1627\n",
      "Epoch 19/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2835 - accuracy: 0.5451 - val_loss: 2.4609 - val_accuracy: 0.1627\n",
      "Epoch 20/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2446 - accuracy: 0.5716 - val_loss: 2.4050 - val_accuracy: 0.1627\n",
      "Epoch 21/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2002 - accuracy: 0.5796 - val_loss: 2.4558 - val_accuracy: 0.1558\n",
      "Epoch 22/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1669 - accuracy: 0.5902 - val_loss: 2.5017 - val_accuracy: 0.1604\n",
      "Epoch 23/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1567 - accuracy: 0.5927 - val_loss: 2.4308 - val_accuracy: 0.1684\n",
      "Epoch 24/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1395 - accuracy: 0.5983 - val_loss: 2.5270 - val_accuracy: 0.1592\n",
      "Epoch 25/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0980 - accuracy: 0.6310 - val_loss: 2.5467 - val_accuracy: 0.1592\n",
      "Epoch 26/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0636 - accuracy: 0.6301 - val_loss: 2.5301 - val_accuracy: 0.1592\n",
      "Epoch 27/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0820 - accuracy: 0.6271 - val_loss: 2.4964 - val_accuracy: 0.1638\n",
      "Epoch 28/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0357 - accuracy: 0.6421 - val_loss: 2.5995 - val_accuracy: 0.1615\n",
      "Epoch 29/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0012 - accuracy: 0.6516 - val_loss: 2.5789 - val_accuracy: 0.1604\n",
      "Epoch 30/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9829 - accuracy: 0.6595 - val_loss: 2.5628 - val_accuracy: 0.1604\n",
      "Epoch 31/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9809 - accuracy: 0.6638 - val_loss: 2.7130 - val_accuracy: 0.1581\n",
      "Epoch 32/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9758 - accuracy: 0.6640 - val_loss: 2.7875 - val_accuracy: 0.1592\n",
      "Epoch 33/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9614 - accuracy: 0.6679 - val_loss: 2.6409 - val_accuracy: 0.1615\n",
      "Epoch 34/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9141 - accuracy: 0.6866 - val_loss: 2.7026 - val_accuracy: 0.1523\n",
      "Epoch 35/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9073 - accuracy: 0.6888 - val_loss: 2.6892 - val_accuracy: 0.1546\n",
      "Epoch 36/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.9063 - accuracy: 0.6897 - val_loss: 2.6730 - val_accuracy: 0.1523\n",
      "Epoch 37/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8996 - accuracy: 0.6936 - val_loss: 2.5901 - val_accuracy: 0.1604\n",
      "Epoch 38/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8760 - accuracy: 0.6975 - val_loss: 2.6890 - val_accuracy: 0.1592\n",
      "Epoch 39/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8634 - accuracy: 0.7010 - val_loss: 2.5754 - val_accuracy: 0.1627\n",
      "Epoch 40/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8444 - accuracy: 0.7080 - val_loss: 2.6812 - val_accuracy: 0.1638\n",
      "Epoch 41/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8661 - accuracy: 0.7073 - val_loss: 2.7220 - val_accuracy: 0.1627\n",
      "Epoch 42/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.8363 - accuracy: 0.7154 - val_loss: 2.6061 - val_accuracy: 0.1558\n",
      "Epoch 43/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8270 - accuracy: 0.7164 - val_loss: 2.6544 - val_accuracy: 0.1592\n",
      "Epoch 44/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8291 - accuracy: 0.7179 - val_loss: 2.4993 - val_accuracy: 0.1649\n",
      "Epoch 45/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8027 - accuracy: 0.7235 - val_loss: 2.5835 - val_accuracy: 0.1615\n",
      "Epoch 46/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8146 - accuracy: 0.7178 - val_loss: 2.6057 - val_accuracy: 0.1569\n",
      "Epoch 47/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7868 - accuracy: 0.7258 - val_loss: 2.6681 - val_accuracy: 0.1581\n",
      "Epoch 48/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7982 - accuracy: 0.7262 - val_loss: 2.6036 - val_accuracy: 0.1569\n",
      "Epoch 49/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7905 - accuracy: 0.7294 - val_loss: 2.6733 - val_accuracy: 0.1707\n",
      "Epoch 50/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8095 - accuracy: 0.7213 - val_loss: 2.6202 - val_accuracy: 0.1661\n",
      "Epoch 51/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7547 - accuracy: 0.7367 - val_loss: 2.6465 - val_accuracy: 0.1695\n",
      "Epoch 52/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7778 - accuracy: 0.7371 - val_loss: 2.6152 - val_accuracy: 0.1695\n",
      "Epoch 53/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7719 - accuracy: 0.7301 - val_loss: 2.7938 - val_accuracy: 0.1592\n",
      "Epoch 54/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7562 - accuracy: 0.7463 - val_loss: 2.5564 - val_accuracy: 0.1661\n",
      "Epoch 55/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7517 - accuracy: 0.7413 - val_loss: 2.8787 - val_accuracy: 0.1581\n",
      "Epoch 56/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7337 - accuracy: 0.7454 - val_loss: 2.6525 - val_accuracy: 0.1581\n",
      "Epoch 57/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7437 - accuracy: 0.7441 - val_loss: 2.6417 - val_accuracy: 0.1672\n",
      "Epoch 58/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7056 - accuracy: 0.7552 - val_loss: 2.6786 - val_accuracy: 0.1615\n",
      "Epoch 59/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.7226 - accuracy: 0.7554 - val_loss: 2.5611 - val_accuracy: 0.1604\n",
      "Epoch 60/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.7181 - accuracy: 0.7473 - val_loss: 2.7126 - val_accuracy: 0.1592\n",
      "Epoch 61/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.7200 - accuracy: 0.7538 - val_loss: 2.7311 - val_accuracy: 0.1546\n",
      "Epoch 62/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.7015 - accuracy: 0.7591 - val_loss: 2.6966 - val_accuracy: 0.1569\n",
      "Epoch 63/100\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.7011 - accuracy: 0.7561 - val_loss: 2.5932 - val_accuracy: 0.1627\n",
      "Epoch 64/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.7117 - accuracy: 0.7599 - val_loss: 2.6135 - val_accuracy: 0.1535\n",
      "Epoch 65/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.7062 - accuracy: 0.7585 - val_loss: 2.6373 - val_accuracy: 0.1581\n",
      "Epoch 66/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6844 - accuracy: 0.7638 - val_loss: 2.5537 - val_accuracy: 0.1569\n",
      "Epoch 67/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6865 - accuracy: 0.7694 - val_loss: 2.5881 - val_accuracy: 0.1535\n",
      "Epoch 68/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6729 - accuracy: 0.7694 - val_loss: 2.6479 - val_accuracy: 0.1604\n",
      "Epoch 69/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6772 - accuracy: 0.7687 - val_loss: 2.6162 - val_accuracy: 0.1638\n",
      "Epoch 70/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6876 - accuracy: 0.7652 - val_loss: 2.5578 - val_accuracy: 0.1546\n",
      "Epoch 71/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6735 - accuracy: 0.7693 - val_loss: 2.6029 - val_accuracy: 0.1523\n",
      "Epoch 72/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.7626 - val_loss: 2.6018 - val_accuracy: 0.1535\n",
      "Epoch 73/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6580 - accuracy: 0.7808 - val_loss: 2.7613 - val_accuracy: 0.1489\n",
      "Epoch 74/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6508 - accuracy: 0.7804 - val_loss: 2.6376 - val_accuracy: 0.1478\n",
      "Epoch 75/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6664 - accuracy: 0.7680 - val_loss: 2.5877 - val_accuracy: 0.1466\n",
      "Epoch 76/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.7800 - val_loss: 2.4924 - val_accuracy: 0.1455\n",
      "Epoch 77/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6679 - accuracy: 0.7759 - val_loss: 2.6743 - val_accuracy: 0.1489\n",
      "Epoch 78/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.7838 - val_loss: 2.6175 - val_accuracy: 0.1512\n",
      "Epoch 79/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6578 - accuracy: 0.7777 - val_loss: 2.7195 - val_accuracy: 0.1489\n",
      "Epoch 80/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6253 - accuracy: 0.7878 - val_loss: 2.6320 - val_accuracy: 0.1569\n",
      "Epoch 81/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.7810 - val_loss: 2.6770 - val_accuracy: 0.1523\n",
      "Epoch 82/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6353 - accuracy: 0.7878 - val_loss: 2.5993 - val_accuracy: 0.1523\n",
      "Epoch 83/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6322 - accuracy: 0.7883 - val_loss: 2.6308 - val_accuracy: 0.1489\n",
      "Epoch 84/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6272 - accuracy: 0.7850 - val_loss: 2.5758 - val_accuracy: 0.1512\n",
      "Epoch 85/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6058 - accuracy: 0.7959 - val_loss: 2.6547 - val_accuracy: 0.1523\n",
      "Epoch 86/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6333 - accuracy: 0.7865 - val_loss: 2.5442 - val_accuracy: 0.1684\n",
      "Epoch 87/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6061 - accuracy: 0.7962 - val_loss: 2.6433 - val_accuracy: 0.1581\n",
      "Epoch 88/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6218 - accuracy: 0.7907 - val_loss: 2.5558 - val_accuracy: 0.1581\n",
      "Epoch 89/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.7940 - val_loss: 2.6680 - val_accuracy: 0.1558\n",
      "Epoch 90/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6052 - accuracy: 0.7936 - val_loss: 2.6452 - val_accuracy: 0.1615\n",
      "Epoch 91/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6107 - accuracy: 0.7923 - val_loss: 2.6104 - val_accuracy: 0.1627\n",
      "Epoch 92/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6016 - accuracy: 0.7954 - val_loss: 2.6149 - val_accuracy: 0.1489\n",
      "Epoch 93/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7996 - val_loss: 2.6700 - val_accuracy: 0.1546\n",
      "Epoch 94/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6031 - accuracy: 0.7993 - val_loss: 2.5508 - val_accuracy: 0.1546\n",
      "Epoch 95/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 0.5889 - accuracy: 0.8030 - val_loss: 2.7121 - val_accuracy: 0.1512\n",
      "Epoch 96/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5868 - accuracy: 0.8070 - val_loss: 2.5826 - val_accuracy: 0.1787\n",
      "Epoch 97/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5876 - accuracy: 0.8039 - val_loss: 2.6217 - val_accuracy: 0.1604\n",
      "Epoch 98/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5939 - accuracy: 0.7995 - val_loss: 2.6578 - val_accuracy: 0.1558\n",
      "Epoch 99/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5860 - accuracy: 0.7960 - val_loss: 2.6273 - val_accuracy: 0.1604\n",
      "Epoch 100/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5701 - accuracy: 0.8104 - val_loss: 2.7005 - val_accuracy: 0.1592\n",
      "28/28 [==============================] - 0s 2ms/step\n",
      "\n",
      "MLP - fold 1 done\n",
      "\n",
      "Epoch 1/10\n",
      "246/246 [==============================] - 52s 210ms/step - loss: 1.9176 - acc: 0.3387\n",
      "Epoch 2/10\n",
      "246/246 [==============================] - 50s 202ms/step - loss: 1.5909 - acc: 0.4401\n",
      "Epoch 3/10\n",
      "246/246 [==============================] - 51s 205ms/step - loss: 1.4082 - acc: 0.5085\n",
      "Epoch 4/10\n",
      "246/246 [==============================] - 49s 200ms/step - loss: 1.2895 - acc: 0.5614\n",
      "Epoch 5/10\n",
      "246/246 [==============================] - 50s 202ms/step - loss: 1.2219 - acc: 0.5820\n",
      "Epoch 6/10\n",
      "246/246 [==============================] - 50s 202ms/step - loss: 1.1524 - acc: 0.6076\n",
      "Epoch 7/10\n",
      "246/246 [==============================] - 49s 200ms/step - loss: 1.1010 - acc: 0.6272\n",
      "Epoch 8/10\n",
      "246/246 [==============================] - 53s 215ms/step - loss: 1.0501 - acc: 0.6464\n",
      "Epoch 9/10\n",
      "246/246 [==============================] - 56s 230ms/step - loss: 1.0193 - acc: 0.6545\n",
      "Epoch 10/10\n",
      "246/246 [==============================] - 56s 229ms/step - loss: 0.9808 - acc: 0.6657\n",
      "28/28 [==============================] - 1s 40ms/step\n",
      "\n",
      "CNN - fold 1 done\n",
      "\n",
      "Epoch 1/100\n",
      "246/246 [==============================] - 3s 7ms/step - loss: 2.5618 - accuracy: 0.1419 - val_loss: 2.1973 - val_accuracy: 0.1802\n",
      "Epoch 2/100\n",
      "246/246 [==============================] - 2s 8ms/step - loss: 2.3030 - accuracy: 0.1662 - val_loss: 2.1673 - val_accuracy: 0.1914\n",
      "Epoch 3/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 2.2088 - accuracy: 0.1808 - val_loss: 2.1609 - val_accuracy: 0.1802\n",
      "Epoch 4/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 2.1520 - accuracy: 0.1944 - val_loss: 2.1494 - val_accuracy: 0.1937\n",
      "Epoch 5/100\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 2.1254 - accuracy: 0.2037 - val_loss: 2.1538 - val_accuracy: 0.1858\n",
      "Epoch 6/100\n",
      "246/246 [==============================] - 2s 8ms/step - loss: 2.0787 - accuracy: 0.2274 - val_loss: 2.1442 - val_accuracy: 0.1914\n",
      "Epoch 7/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 2.0304 - accuracy: 0.2424 - val_loss: 2.1472 - val_accuracy: 0.1881\n",
      "Epoch 8/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 1.9852 - accuracy: 0.2631 - val_loss: 2.1405 - val_accuracy: 0.1858\n",
      "Epoch 9/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 1.9303 - accuracy: 0.2886 - val_loss: 2.1629 - val_accuracy: 0.1824\n",
      "Epoch 10/100\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 1.8610 - accuracy: 0.3186 - val_loss: 2.1607 - val_accuracy: 0.1813\n",
      "Epoch 11/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 1.7895 - accuracy: 0.3427 - val_loss: 2.1900 - val_accuracy: 0.1881\n",
      "Epoch 12/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 1.7311 - accuracy: 0.3631 - val_loss: 2.2058 - val_accuracy: 0.1847\n",
      "Epoch 13/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.6544 - accuracy: 0.4013 - val_loss: 2.2354 - val_accuracy: 0.1779\n",
      "Epoch 14/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.5857 - accuracy: 0.4213 - val_loss: 2.2854 - val_accuracy: 0.1802\n",
      "Epoch 15/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 1.5255 - accuracy: 0.4454 - val_loss: 2.2982 - val_accuracy: 0.1869\n",
      "Epoch 16/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 1.4653 - accuracy: 0.4745 - val_loss: 2.2939 - val_accuracy: 0.1903\n",
      "Epoch 17/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.4250 - accuracy: 0.4862 - val_loss: 2.3542 - val_accuracy: 0.1937\n",
      "Epoch 18/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.3674 - accuracy: 0.5126 - val_loss: 2.3789 - val_accuracy: 0.1937\n",
      "Epoch 19/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.3130 - accuracy: 0.5349 - val_loss: 2.3964 - val_accuracy: 0.1892\n",
      "Epoch 20/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2891 - accuracy: 0.5433 - val_loss: 2.3683 - val_accuracy: 0.1993\n",
      "Epoch 21/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.2337 - accuracy: 0.5616 - val_loss: 2.4485 - val_accuracy: 0.1903\n",
      "Epoch 22/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1985 - accuracy: 0.5821 - val_loss: 2.4361 - val_accuracy: 0.1914\n",
      "Epoch 23/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1751 - accuracy: 0.5864 - val_loss: 2.4580 - val_accuracy: 0.1903\n",
      "Epoch 24/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.1565 - accuracy: 0.5929 - val_loss: 2.4755 - val_accuracy: 0.1858\n",
      "Epoch 25/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0986 - accuracy: 0.6154 - val_loss: 2.5083 - val_accuracy: 0.1937\n",
      "Epoch 26/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 1.0983 - accuracy: 0.6140 - val_loss: 2.5349 - val_accuracy: 0.1948\n",
      "Epoch 27/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.0586 - accuracy: 0.6311 - val_loss: 2.5219 - val_accuracy: 0.1993\n",
      "Epoch 28/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.0310 - accuracy: 0.6354 - val_loss: 2.5618 - val_accuracy: 0.2027\n",
      "Epoch 29/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.0292 - accuracy: 0.6427 - val_loss: 2.4943 - val_accuracy: 0.1914\n",
      "Epoch 30/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.0081 - accuracy: 0.6508 - val_loss: 2.5539 - val_accuracy: 0.1926\n",
      "Epoch 31/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 1.0022 - accuracy: 0.6517 - val_loss: 2.5241 - val_accuracy: 0.1971\n",
      "Epoch 32/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9627 - accuracy: 0.6625 - val_loss: 2.6352 - val_accuracy: 0.1903\n",
      "Epoch 33/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.9655 - accuracy: 0.6689 - val_loss: 2.5934 - val_accuracy: 0.1914\n",
      "Epoch 34/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.9439 - accuracy: 0.6690 - val_loss: 2.5958 - val_accuracy: 0.1791\n",
      "Epoch 35/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9223 - accuracy: 0.6796 - val_loss: 2.5980 - val_accuracy: 0.1802\n",
      "Epoch 36/100\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.9110 - accuracy: 0.6849 - val_loss: 2.5841 - val_accuracy: 0.1869\n",
      "Epoch 37/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.9064 - accuracy: 0.6929 - val_loss: 2.6350 - val_accuracy: 0.1824\n",
      "Epoch 38/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8901 - accuracy: 0.6893 - val_loss: 2.5723 - val_accuracy: 0.1824\n",
      "Epoch 39/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8827 - accuracy: 0.7004 - val_loss: 2.6421 - val_accuracy: 0.1757\n",
      "Epoch 40/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8622 - accuracy: 0.7082 - val_loss: 2.6195 - val_accuracy: 0.1757\n",
      "Epoch 41/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8534 - accuracy: 0.7059 - val_loss: 2.6054 - val_accuracy: 0.1824\n",
      "Epoch 42/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8384 - accuracy: 0.7120 - val_loss: 2.6393 - val_accuracy: 0.1847\n",
      "Epoch 43/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8286 - accuracy: 0.7123 - val_loss: 2.6521 - val_accuracy: 0.1757\n",
      "Epoch 44/100\n",
      "246/246 [==============================] - 2s 8ms/step - loss: 0.8378 - accuracy: 0.7138 - val_loss: 2.5508 - val_accuracy: 0.1959\n",
      "Epoch 45/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8030 - accuracy: 0.7341 - val_loss: 2.7364 - val_accuracy: 0.1768\n",
      "Epoch 46/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.8215 - accuracy: 0.7221 - val_loss: 2.6131 - val_accuracy: 0.1791\n",
      "Epoch 47/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7855 - accuracy: 0.7257 - val_loss: 2.7613 - val_accuracy: 0.1723\n",
      "Epoch 48/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.7922 - accuracy: 0.7311 - val_loss: 2.7324 - val_accuracy: 0.1824\n",
      "Epoch 49/100\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.7912 - accuracy: 0.7290 - val_loss: 2.6571 - val_accuracy: 0.1813\n",
      "Epoch 50/100\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.7800 - accuracy: 0.7306 - val_loss: 2.6111 - val_accuracy: 0.1836\n",
      "Epoch 51/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7807 - accuracy: 0.7305 - val_loss: 2.5748 - val_accuracy: 0.1881\n",
      "Epoch 52/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7541 - accuracy: 0.7375 - val_loss: 2.6852 - val_accuracy: 0.1937\n",
      "Epoch 53/100\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.7481 - accuracy: 0.7367 - val_loss: 2.6076 - val_accuracy: 0.1948\n",
      "Epoch 54/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7688 - accuracy: 0.7361 - val_loss: 2.5321 - val_accuracy: 0.2005\n",
      "Epoch 55/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7505 - accuracy: 0.7436 - val_loss: 2.5491 - val_accuracy: 0.1881\n",
      "Epoch 56/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7198 - accuracy: 0.7541 - val_loss: 2.6319 - val_accuracy: 0.1858\n",
      "Epoch 57/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7321 - accuracy: 0.7473 - val_loss: 2.6032 - val_accuracy: 0.1903\n",
      "Epoch 58/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7451 - accuracy: 0.7513 - val_loss: 2.4837 - val_accuracy: 0.2027\n",
      "Epoch 59/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7294 - accuracy: 0.7528 - val_loss: 2.7623 - val_accuracy: 0.1768\n",
      "Epoch 60/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7027 - accuracy: 0.7626 - val_loss: 2.6745 - val_accuracy: 0.1937\n",
      "Epoch 61/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7038 - accuracy: 0.7658 - val_loss: 2.5693 - val_accuracy: 0.1959\n",
      "Epoch 62/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.7299 - accuracy: 0.7519 - val_loss: 2.6335 - val_accuracy: 0.1858\n",
      "Epoch 63/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6876 - accuracy: 0.7605 - val_loss: 2.5856 - val_accuracy: 0.1982\n",
      "Epoch 64/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6809 - accuracy: 0.7663 - val_loss: 2.6698 - val_accuracy: 0.1869\n",
      "Epoch 65/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6948 - accuracy: 0.7684 - val_loss: 2.5219 - val_accuracy: 0.1881\n",
      "Epoch 66/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6964 - accuracy: 0.7588 - val_loss: 2.5515 - val_accuracy: 0.1914\n",
      "Epoch 67/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6855 - accuracy: 0.7668 - val_loss: 2.5504 - val_accuracy: 0.1824\n",
      "Epoch 68/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6852 - accuracy: 0.7712 - val_loss: 2.7641 - val_accuracy: 0.1836\n",
      "Epoch 69/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6643 - accuracy: 0.7719 - val_loss: 2.6745 - val_accuracy: 0.1881\n",
      "Epoch 70/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6935 - accuracy: 0.7621 - val_loss: 2.6278 - val_accuracy: 0.1881\n",
      "Epoch 71/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6720 - accuracy: 0.7721 - val_loss: 2.5432 - val_accuracy: 0.1892\n",
      "Epoch 72/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6476 - accuracy: 0.7774 - val_loss: 2.5892 - val_accuracy: 0.1779\n",
      "Epoch 73/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6423 - accuracy: 0.7794 - val_loss: 2.6374 - val_accuracy: 0.1813\n",
      "Epoch 74/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6396 - accuracy: 0.7847 - val_loss: 2.5689 - val_accuracy: 0.1914\n",
      "Epoch 75/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6496 - accuracy: 0.7815 - val_loss: 2.6269 - val_accuracy: 0.1903\n",
      "Epoch 76/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.7798 - val_loss: 2.6806 - val_accuracy: 0.1813\n",
      "Epoch 77/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.7789 - val_loss: 2.6326 - val_accuracy: 0.1847\n",
      "Epoch 78/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6291 - accuracy: 0.7865 - val_loss: 2.5554 - val_accuracy: 0.1745\n",
      "Epoch 79/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6278 - accuracy: 0.7913 - val_loss: 2.5963 - val_accuracy: 0.1926\n",
      "Epoch 80/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6298 - accuracy: 0.7857 - val_loss: 2.6962 - val_accuracy: 0.1847\n",
      "Epoch 81/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6166 - accuracy: 0.7954 - val_loss: 2.6732 - val_accuracy: 0.1791\n",
      "Epoch 82/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6357 - accuracy: 0.7860 - val_loss: 2.5966 - val_accuracy: 0.1869\n",
      "Epoch 83/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6185 - accuracy: 0.7932 - val_loss: 2.7174 - val_accuracy: 0.1791\n",
      "Epoch 84/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5988 - accuracy: 0.7967 - val_loss: 2.6794 - val_accuracy: 0.1993\n",
      "Epoch 85/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6125 - accuracy: 0.7895 - val_loss: 2.6614 - val_accuracy: 0.1768\n",
      "Epoch 86/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6031 - accuracy: 0.7955 - val_loss: 2.5972 - val_accuracy: 0.1892\n",
      "Epoch 87/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6067 - accuracy: 0.7922 - val_loss: 2.7038 - val_accuracy: 0.1779\n",
      "Epoch 88/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6211 - accuracy: 0.7888 - val_loss: 2.6422 - val_accuracy: 0.1802\n",
      "Epoch 89/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6161 - accuracy: 0.7912 - val_loss: 2.6391 - val_accuracy: 0.1779\n",
      "Epoch 90/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6042 - accuracy: 0.8000 - val_loss: 2.7115 - val_accuracy: 0.1824\n",
      "Epoch 91/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5873 - accuracy: 0.8001 - val_loss: 2.6318 - val_accuracy: 0.1836\n",
      "Epoch 92/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5884 - accuracy: 0.8011 - val_loss: 2.6242 - val_accuracy: 0.1779\n",
      "Epoch 93/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5828 - accuracy: 0.8042 - val_loss: 2.6800 - val_accuracy: 0.1914\n",
      "Epoch 94/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5863 - accuracy: 0.8027 - val_loss: 2.6311 - val_accuracy: 0.1813\n",
      "Epoch 95/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5894 - accuracy: 0.7991 - val_loss: 2.7460 - val_accuracy: 0.1881\n",
      "Epoch 96/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5691 - accuracy: 0.8094 - val_loss: 2.6139 - val_accuracy: 0.1971\n",
      "Epoch 97/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5767 - accuracy: 0.8046 - val_loss: 2.7478 - val_accuracy: 0.1791\n",
      "Epoch 98/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5616 - accuracy: 0.8114 - val_loss: 2.7014 - val_accuracy: 0.1791\n",
      "Epoch 99/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.8049 - val_loss: 2.7066 - val_accuracy: 0.1903\n",
      "Epoch 100/100\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5786 - accuracy: 0.8044 - val_loss: 2.7435 - val_accuracy: 0.1745\n",
      "28/28 [==============================] - 0s 1ms/step\n",
      "\n",
      "MLP - fold 2 done\n",
      "\n",
      "Epoch 1/10\n",
      "246/246 [==============================] - 57s 229ms/step - loss: 1.9613 - acc: 0.3201\n",
      "Epoch 2/10\n",
      "246/246 [==============================] - 56s 227ms/step - loss: 1.6113 - acc: 0.4437\n",
      "Epoch 3/10\n",
      "246/246 [==============================] - 56s 227ms/step - loss: 1.4606 - acc: 0.4959\n",
      "Epoch 4/10\n",
      "246/246 [==============================] - 54s 218ms/step - loss: 1.3469 - acc: 0.5404\n",
      "Epoch 5/10\n",
      "246/246 [==============================] - 52s 210ms/step - loss: 1.2550 - acc: 0.5709\n",
      "Epoch 6/10\n",
      "246/246 [==============================] - 52s 211ms/step - loss: 1.1695 - acc: 0.6034\n",
      "Epoch 7/10\n",
      "246/246 [==============================] - 52s 210ms/step - loss: 1.1150 - acc: 0.6095\n",
      "Epoch 8/10\n",
      "246/246 [==============================] - 52s 212ms/step - loss: 1.0780 - acc: 0.6316\n",
      "Epoch 9/10\n",
      "246/246 [==============================] - 52s 211ms/step - loss: 1.0535 - acc: 0.6425\n",
      "Epoch 10/10\n",
      "246/246 [==============================] - 52s 212ms/step - loss: 0.9982 - acc: 0.6564\n",
      "28/28 [==============================] - 1s 39ms/step\n",
      "\n",
      "CNN - fold 2 done\n",
      "\n",
      "Epoch 1/100\n",
      "244/244 [==============================] - 2s 6ms/step - loss: 2.5584 - accuracy: 0.1482 - val_loss: 2.2144 - val_accuracy: 0.1319\n",
      "Epoch 2/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 2.2894 - accuracy: 0.1652 - val_loss: 2.1818 - val_accuracy: 0.1600\n",
      "Epoch 3/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 2.2018 - accuracy: 0.1834 - val_loss: 2.1816 - val_accuracy: 0.1697\n",
      "Epoch 4/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 2.1538 - accuracy: 0.1978 - val_loss: 2.1772 - val_accuracy: 0.1773\n",
      "Epoch 5/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 2.1231 - accuracy: 0.1979 - val_loss: 2.1741 - val_accuracy: 0.1665\n",
      "Epoch 6/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 2.0806 - accuracy: 0.2288 - val_loss: 2.1672 - val_accuracy: 0.2378\n",
      "Epoch 7/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 2.0289 - accuracy: 0.2562 - val_loss: 2.1806 - val_accuracy: 0.2076\n",
      "Epoch 8/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.9843 - accuracy: 0.2724 - val_loss: 2.1673 - val_accuracy: 0.1816\n",
      "Epoch 9/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.9285 - accuracy: 0.2947 - val_loss: 2.1767 - val_accuracy: 0.2270\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.8482 - accuracy: 0.3261 - val_loss: 2.1993 - val_accuracy: 0.1632\n",
      "Epoch 11/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.7802 - accuracy: 0.3602 - val_loss: 2.2301 - val_accuracy: 0.1697\n",
      "Epoch 12/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.7237 - accuracy: 0.3803 - val_loss: 2.2601 - val_accuracy: 0.1373\n",
      "Epoch 13/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.6362 - accuracy: 0.4169 - val_loss: 2.2849 - val_accuracy: 0.1546\n",
      "Epoch 14/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.5800 - accuracy: 0.4408 - val_loss: 2.2789 - val_accuracy: 0.1600\n",
      "Epoch 15/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.5059 - accuracy: 0.4591 - val_loss: 2.3076 - val_accuracy: 0.1741\n",
      "Epoch 16/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.4534 - accuracy: 0.4856 - val_loss: 2.3215 - val_accuracy: 0.1568\n",
      "Epoch 17/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.4174 - accuracy: 0.4951 - val_loss: 2.3169 - val_accuracy: 0.1816\n",
      "Epoch 18/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.3735 - accuracy: 0.5161 - val_loss: 2.3771 - val_accuracy: 0.1632\n",
      "Epoch 19/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.3131 - accuracy: 0.5370 - val_loss: 2.4055 - val_accuracy: 0.1665\n",
      "Epoch 20/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.2595 - accuracy: 0.5587 - val_loss: 2.4455 - val_accuracy: 0.1632\n",
      "Epoch 21/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.2475 - accuracy: 0.5589 - val_loss: 2.4473 - val_accuracy: 0.1643\n",
      "Epoch 22/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.2006 - accuracy: 0.5813 - val_loss: 2.5634 - val_accuracy: 0.1427\n",
      "Epoch 23/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.1426 - accuracy: 0.5870 - val_loss: 2.5004 - val_accuracy: 0.1643\n",
      "Epoch 24/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 1.1271 - accuracy: 0.6102 - val_loss: 2.5972 - val_accuracy: 0.1308\n",
      "Epoch 25/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 1.1200 - accuracy: 0.6137 - val_loss: 2.5077 - val_accuracy: 0.1784\n",
      "Epoch 26/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 1.0981 - accuracy: 0.6169 - val_loss: 2.5406 - val_accuracy: 0.1535\n",
      "Epoch 27/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 1.0463 - accuracy: 0.6407 - val_loss: 2.6256 - val_accuracy: 0.1654\n",
      "Epoch 28/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 1.0417 - accuracy: 0.6438 - val_loss: 2.5525 - val_accuracy: 0.1589\n",
      "Epoch 29/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 1.0080 - accuracy: 0.6476 - val_loss: 2.6069 - val_accuracy: 0.1578\n",
      "Epoch 30/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9950 - accuracy: 0.6566 - val_loss: 2.5821 - val_accuracy: 0.1632\n",
      "Epoch 31/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9910 - accuracy: 0.6589 - val_loss: 2.5445 - val_accuracy: 0.1524\n",
      "Epoch 32/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9548 - accuracy: 0.6661 - val_loss: 2.5854 - val_accuracy: 0.1708\n",
      "Epoch 33/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9393 - accuracy: 0.6725 - val_loss: 2.5740 - val_accuracy: 0.1611\n",
      "Epoch 34/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9494 - accuracy: 0.6743 - val_loss: 2.5017 - val_accuracy: 0.1654\n",
      "Epoch 35/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9181 - accuracy: 0.6854 - val_loss: 2.5817 - val_accuracy: 0.1632\n",
      "Epoch 36/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.9064 - accuracy: 0.6904 - val_loss: 2.5891 - val_accuracy: 0.1622\n",
      "Epoch 37/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.8883 - accuracy: 0.6940 - val_loss: 2.5556 - val_accuracy: 0.1632\n",
      "Epoch 38/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.8978 - accuracy: 0.6948 - val_loss: 2.6049 - val_accuracy: 0.1622\n",
      "Epoch 39/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.8811 - accuracy: 0.6950 - val_loss: 2.5614 - val_accuracy: 0.1622\n",
      "Epoch 40/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.8562 - accuracy: 0.7081 - val_loss: 2.6326 - val_accuracy: 0.1568\n",
      "Epoch 41/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.8612 - accuracy: 0.7114 - val_loss: 2.6721 - val_accuracy: 0.1589\n",
      "Epoch 42/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.8264 - accuracy: 0.7218 - val_loss: 2.7176 - val_accuracy: 0.1589\n",
      "Epoch 43/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.8314 - accuracy: 0.7106 - val_loss: 2.7054 - val_accuracy: 0.1600\n",
      "Epoch 44/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.8331 - accuracy: 0.7090 - val_loss: 2.5668 - val_accuracy: 0.1697\n",
      "Epoch 45/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7921 - accuracy: 0.7278 - val_loss: 2.6900 - val_accuracy: 0.1578\n",
      "Epoch 46/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7939 - accuracy: 0.7282 - val_loss: 2.6868 - val_accuracy: 0.1676\n",
      "Epoch 47/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7958 - accuracy: 0.7261 - val_loss: 2.6384 - val_accuracy: 0.1773\n",
      "Epoch 48/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7997 - accuracy: 0.7299 - val_loss: 2.6297 - val_accuracy: 0.1654\n",
      "Epoch 49/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7672 - accuracy: 0.7405 - val_loss: 2.8199 - val_accuracy: 0.1751\n",
      "Epoch 50/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7520 - accuracy: 0.7418 - val_loss: 2.6324 - val_accuracy: 0.1632\n",
      "Epoch 51/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7729 - accuracy: 0.7333 - val_loss: 2.5950 - val_accuracy: 0.1773\n",
      "Epoch 52/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7572 - accuracy: 0.7422 - val_loss: 2.6487 - val_accuracy: 0.1751\n",
      "Epoch 53/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7597 - accuracy: 0.7378 - val_loss: 2.5862 - val_accuracy: 0.1632\n",
      "Epoch 54/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7497 - accuracy: 0.7489 - val_loss: 2.6164 - val_accuracy: 0.1665\n",
      "Epoch 55/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7531 - accuracy: 0.7493 - val_loss: 2.6503 - val_accuracy: 0.1578\n",
      "Epoch 56/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7507 - accuracy: 0.7472 - val_loss: 2.6610 - val_accuracy: 0.1686\n",
      "Epoch 57/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7424 - accuracy: 0.7468 - val_loss: 2.6672 - val_accuracy: 0.1762\n",
      "Epoch 58/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7428 - accuracy: 0.7474 - val_loss: 2.7459 - val_accuracy: 0.1762\n",
      "Epoch 59/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6928 - accuracy: 0.7615 - val_loss: 2.7009 - val_accuracy: 0.1676\n",
      "Epoch 60/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7181 - accuracy: 0.7562 - val_loss: 2.7708 - val_accuracy: 0.1665\n",
      "Epoch 61/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6995 - accuracy: 0.7632 - val_loss: 2.6193 - val_accuracy: 0.1622\n",
      "Epoch 62/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.7080 - accuracy: 0.7618 - val_loss: 2.7229 - val_accuracy: 0.1568\n",
      "Epoch 63/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.7034 - accuracy: 0.7575 - val_loss: 2.6097 - val_accuracy: 0.1730\n",
      "Epoch 64/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6927 - accuracy: 0.7644 - val_loss: 2.8653 - val_accuracy: 0.1492\n",
      "Epoch 65/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6814 - accuracy: 0.7674 - val_loss: 2.6897 - val_accuracy: 0.1762\n",
      "Epoch 66/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6951 - accuracy: 0.7620 - val_loss: 2.6296 - val_accuracy: 0.1643\n",
      "Epoch 67/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6880 - accuracy: 0.7659 - val_loss: 2.6088 - val_accuracy: 0.1686\n",
      "Epoch 68/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6674 - accuracy: 0.7714 - val_loss: 2.6228 - val_accuracy: 0.1708\n",
      "Epoch 69/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6605 - accuracy: 0.7762 - val_loss: 2.6548 - val_accuracy: 0.1654\n",
      "Epoch 70/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6810 - accuracy: 0.7688 - val_loss: 2.7995 - val_accuracy: 0.1719\n",
      "Epoch 71/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6687 - accuracy: 0.7744 - val_loss: 2.6263 - val_accuracy: 0.1686\n",
      "Epoch 72/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.7812 - val_loss: 2.8050 - val_accuracy: 0.1665\n",
      "Epoch 73/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6614 - accuracy: 0.7801 - val_loss: 2.6943 - val_accuracy: 0.1578\n",
      "Epoch 74/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.7778 - val_loss: 2.7198 - val_accuracy: 0.1676\n",
      "Epoch 75/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6431 - accuracy: 0.7819 - val_loss: 2.7570 - val_accuracy: 0.1730\n",
      "Epoch 76/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6443 - accuracy: 0.7863 - val_loss: 2.7487 - val_accuracy: 0.1632\n",
      "Epoch 77/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6336 - accuracy: 0.7865 - val_loss: 2.8144 - val_accuracy: 0.1676\n",
      "Epoch 78/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6373 - accuracy: 0.7883 - val_loss: 2.6977 - val_accuracy: 0.1805\n",
      "Epoch 79/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6183 - accuracy: 0.7906 - val_loss: 2.6857 - val_accuracy: 0.1773\n",
      "Epoch 80/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6136 - accuracy: 0.7884 - val_loss: 2.7354 - val_accuracy: 0.1676\n",
      "Epoch 81/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.7828 - val_loss: 2.7131 - val_accuracy: 0.1654\n",
      "Epoch 82/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6224 - accuracy: 0.7887 - val_loss: 2.6079 - val_accuracy: 0.1622\n",
      "Epoch 83/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6213 - accuracy: 0.7907 - val_loss: 2.6804 - val_accuracy: 0.1708\n",
      "Epoch 84/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5999 - accuracy: 0.7977 - val_loss: 2.6414 - val_accuracy: 0.1643\n",
      "Epoch 85/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6222 - accuracy: 0.7938 - val_loss: 2.6394 - val_accuracy: 0.1643\n",
      "Epoch 86/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6210 - accuracy: 0.7915 - val_loss: 2.7441 - val_accuracy: 0.1611\n",
      "Epoch 87/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.6332 - accuracy: 0.7894 - val_loss: 2.5243 - val_accuracy: 0.1676\n",
      "Epoch 88/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6151 - accuracy: 0.7881 - val_loss: 2.7763 - val_accuracy: 0.1632\n",
      "Epoch 89/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.6022 - accuracy: 0.7966 - val_loss: 2.7882 - val_accuracy: 0.1665\n",
      "Epoch 90/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5792 - accuracy: 0.8054 - val_loss: 2.7972 - val_accuracy: 0.1611\n",
      "Epoch 91/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.5974 - accuracy: 0.7993 - val_loss: 2.6872 - val_accuracy: 0.1708\n",
      "Epoch 92/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5995 - accuracy: 0.7972 - val_loss: 2.7471 - val_accuracy: 0.1654\n",
      "Epoch 93/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5838 - accuracy: 0.8049 - val_loss: 2.6323 - val_accuracy: 0.1600\n",
      "Epoch 94/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5824 - accuracy: 0.8031 - val_loss: 2.6852 - val_accuracy: 0.1578\n",
      "Epoch 95/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5762 - accuracy: 0.8053 - val_loss: 2.8716 - val_accuracy: 0.1611\n",
      "Epoch 96/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.5942 - accuracy: 0.7980 - val_loss: 2.7161 - val_accuracy: 0.1611\n",
      "Epoch 97/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5810 - accuracy: 0.8034 - val_loss: 2.7502 - val_accuracy: 0.1568\n",
      "Epoch 98/100\n",
      "244/244 [==============================] - 1s 5ms/step - loss: 0.5811 - accuracy: 0.8026 - val_loss: 2.7102 - val_accuracy: 0.1600\n",
      "Epoch 99/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5746 - accuracy: 0.8054 - val_loss: 2.7534 - val_accuracy: 0.1654\n",
      "Epoch 100/100\n",
      "244/244 [==============================] - 1s 6ms/step - loss: 0.5861 - accuracy: 0.8061 - val_loss: 2.6549 - val_accuracy: 0.1535\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "\n",
      "MLP - fold 3 done\n",
      "\n",
      "Epoch 1/10\n",
      "244/244 [==============================] - 52s 209ms/step - loss: 1.9228 - acc: 0.3378\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 51s 209ms/step - loss: 1.5721 - acc: 0.4551\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 51s 209ms/step - loss: 1.4099 - acc: 0.5142\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 50s 206ms/step - loss: 1.3020 - acc: 0.5628\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 50s 205ms/step - loss: 1.2175 - acc: 0.5837\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 50s 205ms/step - loss: 1.1509 - acc: 0.6079\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 49s 202ms/step - loss: 1.0829 - acc: 0.6331\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 49s 203ms/step - loss: 1.0235 - acc: 0.6543\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 50s 205ms/step - loss: 1.0003 - acc: 0.6635\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 49s 202ms/step - loss: 0.9505 - acc: 0.6785\n",
      "29/29 [==============================] - 1s 40ms/step\n",
      "\n",
      "CNN - fold 3 done\n",
      "\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 2.5358 - accuracy: 0.1492 - val_loss: 2.2483 - val_accuracy: 0.1141\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.2955 - accuracy: 0.1644 - val_loss: 2.2046 - val_accuracy: 0.1313\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 2.2067 - accuracy: 0.1775 - val_loss: 2.1823 - val_accuracy: 0.1636\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.1331 - accuracy: 0.2024 - val_loss: 2.1904 - val_accuracy: 0.1535\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 2.1003 - accuracy: 0.2102 - val_loss: 2.2039 - val_accuracy: 0.1434\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.0597 - accuracy: 0.2251 - val_loss: 2.2185 - val_accuracy: 0.1455\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 2.0129 - accuracy: 0.2592 - val_loss: 2.2350 - val_accuracy: 0.1394\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.9439 - accuracy: 0.2815 - val_loss: 2.2708 - val_accuracy: 0.1515\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.8787 - accuracy: 0.3055 - val_loss: 2.2664 - val_accuracy: 0.1545\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.8112 - accuracy: 0.3326 - val_loss: 2.2925 - val_accuracy: 0.1515\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.7424 - accuracy: 0.3704 - val_loss: 2.3542 - val_accuracy: 0.1626\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.6868 - accuracy: 0.3878 - val_loss: 2.3926 - val_accuracy: 0.1364\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.6236 - accuracy: 0.4140 - val_loss: 2.4345 - val_accuracy: 0.1444\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.5428 - accuracy: 0.4401 - val_loss: 2.4249 - val_accuracy: 0.1606\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.5002 - accuracy: 0.4582 - val_loss: 2.5028 - val_accuracy: 0.1354\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.4400 - accuracy: 0.4853 - val_loss: 2.5203 - val_accuracy: 0.1657\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3847 - accuracy: 0.5035 - val_loss: 2.6299 - val_accuracy: 0.1455\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.3445 - accuracy: 0.5249 - val_loss: 2.5965 - val_accuracy: 0.1576\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2828 - accuracy: 0.5411 - val_loss: 2.7001 - val_accuracy: 0.1192\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2387 - accuracy: 0.5555 - val_loss: 2.6537 - val_accuracy: 0.1556\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.2074 - accuracy: 0.5672 - val_loss: 2.8063 - val_accuracy: 0.1505\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2052 - accuracy: 0.5747 - val_loss: 2.7232 - val_accuracy: 0.1545\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.1558 - accuracy: 0.5948 - val_loss: 2.7782 - val_accuracy: 0.1434\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.1161 - accuracy: 0.6125 - val_loss: 2.8571 - val_accuracy: 0.1505\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0863 - accuracy: 0.6141 - val_loss: 2.8459 - val_accuracy: 0.1465\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.0614 - accuracy: 0.6298 - val_loss: 2.9927 - val_accuracy: 0.1323\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.0335 - accuracy: 0.6346 - val_loss: 2.9298 - val_accuracy: 0.1364\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0385 - accuracy: 0.6348 - val_loss: 2.9442 - val_accuracy: 0.1535\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 1.0222 - accuracy: 0.6439 - val_loss: 2.8902 - val_accuracy: 0.1556\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9961 - accuracy: 0.6537 - val_loss: 2.9854 - val_accuracy: 0.1556\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.9743 - accuracy: 0.6568 - val_loss: 3.1060 - val_accuracy: 0.1404\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9460 - accuracy: 0.6626 - val_loss: 3.1169 - val_accuracy: 0.1525\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.9431 - accuracy: 0.6700 - val_loss: 3.0797 - val_accuracy: 0.1556\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8963 - accuracy: 0.6875 - val_loss: 2.9119 - val_accuracy: 0.1576\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9043 - accuracy: 0.6892 - val_loss: 3.0771 - val_accuracy: 0.1515\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8780 - accuracy: 0.6963 - val_loss: 2.9105 - val_accuracy: 0.1545\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8745 - accuracy: 0.6976 - val_loss: 3.1006 - val_accuracy: 0.1535\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8658 - accuracy: 0.6980 - val_loss: 2.9909 - val_accuracy: 0.1556\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8531 - accuracy: 0.7001 - val_loss: 2.8547 - val_accuracy: 0.1687\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8482 - accuracy: 0.7047 - val_loss: 3.1094 - val_accuracy: 0.1586\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8453 - accuracy: 0.7096 - val_loss: 2.9457 - val_accuracy: 0.1475\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8504 - accuracy: 0.7030 - val_loss: 2.9163 - val_accuracy: 0.1545\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8002 - accuracy: 0.7259 - val_loss: 2.9663 - val_accuracy: 0.1525\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.8045 - accuracy: 0.7249 - val_loss: 2.8452 - val_accuracy: 0.1636\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7946 - accuracy: 0.7241 - val_loss: 2.8184 - val_accuracy: 0.1485\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 0.7824 - accuracy: 0.7321 - val_loss: 2.7834 - val_accuracy: 0.1535\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.7911 - accuracy: 0.7245 - val_loss: 2.8610 - val_accuracy: 0.1485\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7772 - accuracy: 0.7346 - val_loss: 2.7832 - val_accuracy: 0.1545\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7541 - accuracy: 0.7352 - val_loss: 2.7437 - val_accuracy: 0.1616\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7735 - accuracy: 0.7339 - val_loss: 2.8226 - val_accuracy: 0.1586\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7620 - accuracy: 0.7328 - val_loss: 2.9097 - val_accuracy: 0.1606\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7455 - accuracy: 0.7427 - val_loss: 2.9236 - val_accuracy: 0.1566\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7415 - accuracy: 0.7450 - val_loss: 2.9207 - val_accuracy: 0.1586\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7396 - accuracy: 0.7417 - val_loss: 2.8634 - val_accuracy: 0.1646\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7294 - accuracy: 0.7450 - val_loss: 2.8333 - val_accuracy: 0.1505\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7295 - accuracy: 0.7521 - val_loss: 2.9264 - val_accuracy: 0.1596\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7256 - accuracy: 0.7511 - val_loss: 2.8313 - val_accuracy: 0.1545\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7347 - accuracy: 0.7499 - val_loss: 2.8617 - val_accuracy: 0.1646\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7088 - accuracy: 0.7590 - val_loss: 2.8917 - val_accuracy: 0.1677\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6993 - accuracy: 0.7618 - val_loss: 2.8462 - val_accuracy: 0.1677\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6963 - accuracy: 0.7591 - val_loss: 2.9793 - val_accuracy: 0.1596\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.7670 - val_loss: 3.0465 - val_accuracy: 0.1636\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6812 - accuracy: 0.7636 - val_loss: 2.9431 - val_accuracy: 0.1626\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6828 - accuracy: 0.7715 - val_loss: 2.9796 - val_accuracy: 0.1525\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6884 - accuracy: 0.7652 - val_loss: 2.9415 - val_accuracy: 0.1545\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6704 - accuracy: 0.7756 - val_loss: 3.0616 - val_accuracy: 0.1616\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6809 - accuracy: 0.7678 - val_loss: 2.9373 - val_accuracy: 0.1586\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6512 - accuracy: 0.7769 - val_loss: 2.9832 - val_accuracy: 0.1596\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6452 - accuracy: 0.7805 - val_loss: 2.9254 - val_accuracy: 0.1606\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6554 - accuracy: 0.7753 - val_loss: 2.9060 - val_accuracy: 0.1586\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6745 - accuracy: 0.7728 - val_loss: 2.9451 - val_accuracy: 0.1636\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6479 - accuracy: 0.7836 - val_loss: 2.9057 - val_accuracy: 0.1606\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6565 - accuracy: 0.7799 - val_loss: 2.8175 - val_accuracy: 0.1586\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6333 - accuracy: 0.7843 - val_loss: 2.8678 - val_accuracy: 0.1626\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6183 - accuracy: 0.7900 - val_loss: 2.9915 - val_accuracy: 0.1657\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.7898 - val_loss: 2.9077 - val_accuracy: 0.1525\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.7873 - val_loss: 2.8156 - val_accuracy: 0.1646\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6312 - accuracy: 0.7905 - val_loss: 2.8494 - val_accuracy: 0.1545\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6297 - accuracy: 0.7895 - val_loss: 2.6921 - val_accuracy: 0.1667\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.7906 - val_loss: 2.9072 - val_accuracy: 0.1636\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.7862 - val_loss: 2.7839 - val_accuracy: 0.1525\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6173 - accuracy: 0.7967 - val_loss: 2.8764 - val_accuracy: 0.1606\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5963 - accuracy: 0.7986 - val_loss: 2.7384 - val_accuracy: 0.1556\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6217 - accuracy: 0.7919 - val_loss: 2.9386 - val_accuracy: 0.1566\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6019 - accuracy: 0.8025 - val_loss: 2.8448 - val_accuracy: 0.1566\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6183 - accuracy: 0.7951 - val_loss: 2.8039 - val_accuracy: 0.1616\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5922 - accuracy: 0.7960 - val_loss: 2.8481 - val_accuracy: 0.1747\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5983 - accuracy: 0.7994 - val_loss: 2.7805 - val_accuracy: 0.1596\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6058 - accuracy: 0.7982 - val_loss: 2.7412 - val_accuracy: 0.1758\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5948 - accuracy: 0.7958 - val_loss: 2.6728 - val_accuracy: 0.1687\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5810 - accuracy: 0.8038 - val_loss: 2.7748 - val_accuracy: 0.1515\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5867 - accuracy: 0.8026 - val_loss: 2.7793 - val_accuracy: 0.1394\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.8061 - val_loss: 2.6739 - val_accuracy: 0.1677\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5729 - accuracy: 0.8114 - val_loss: 2.7662 - val_accuracy: 0.1566\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5911 - accuracy: 0.8006 - val_loss: 2.7673 - val_accuracy: 0.1515\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5758 - accuracy: 0.8095 - val_loss: 2.7412 - val_accuracy: 0.1596\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5723 - accuracy: 0.8092 - val_loss: 2.7986 - val_accuracy: 0.1677\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5626 - accuracy: 0.8083 - val_loss: 2.7925 - val_accuracy: 0.1525\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5728 - accuracy: 0.8090 - val_loss: 2.8783 - val_accuracy: 0.1556\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5641 - accuracy: 0.8143 - val_loss: 2.8909 - val_accuracy: 0.1657\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      "\n",
      "MLP - fold 4 done\n",
      "\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 62s 252ms/step - loss: 1.9115 - acc: 0.3378\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 58s 239ms/step - loss: 1.5503 - acc: 0.4602\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 58s 240ms/step - loss: 1.4110 - acc: 0.5160\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 56s 232ms/step - loss: 1.2785 - acc: 0.5673\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 56s 230ms/step - loss: 1.2069 - acc: 0.5849\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 55s 229ms/step - loss: 1.1343 - acc: 0.6137\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 55s 229ms/step - loss: 1.0749 - acc: 0.6443\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 55s 228ms/step - loss: 1.0502 - acc: 0.6447\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 55s 227ms/step - loss: 1.0033 - acc: 0.6609\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 55s 227ms/step - loss: 0.9733 - acc: 0.6717\n",
      "31/31 [==============================] - 1s 42ms/step\n",
      "\n",
      "CNN - fold 4 done\n",
      "\n",
      "Epoch 1/100\n",
      "244/244 [==============================] - 3s 8ms/step - loss: 2.5609 - accuracy: 0.1449 - val_loss: 2.2584 - val_accuracy: 0.1239\n",
      "Epoch 2/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 2.2792 - accuracy: 0.1730 - val_loss: 2.2432 - val_accuracy: 0.1474\n",
      "Epoch 3/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 2.1836 - accuracy: 0.1815 - val_loss: 2.2471 - val_accuracy: 0.1806\n",
      "Epoch 4/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 2.1327 - accuracy: 0.1963 - val_loss: 2.2336 - val_accuracy: 0.1827\n",
      "Epoch 5/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 2.0852 - accuracy: 0.2174 - val_loss: 2.2475 - val_accuracy: 0.1838\n",
      "Epoch 6/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 2.0568 - accuracy: 0.2326 - val_loss: 2.2433 - val_accuracy: 0.1667\n",
      "Epoch 7/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 2.0103 - accuracy: 0.2531 - val_loss: 2.2684 - val_accuracy: 0.1667\n",
      "Epoch 8/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.9526 - accuracy: 0.2726 - val_loss: 2.2752 - val_accuracy: 0.1752\n",
      "Epoch 9/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.8991 - accuracy: 0.2972 - val_loss: 2.2702 - val_accuracy: 0.1709\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.8407 - accuracy: 0.3220 - val_loss: 2.3017 - val_accuracy: 0.1613\n",
      "Epoch 11/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.7650 - accuracy: 0.3479 - val_loss: 2.3062 - val_accuracy: 0.1699\n",
      "Epoch 12/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.7019 - accuracy: 0.3813 - val_loss: 2.3404 - val_accuracy: 0.1592\n",
      "Epoch 13/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.6375 - accuracy: 0.4052 - val_loss: 2.3939 - val_accuracy: 0.1549\n",
      "Epoch 14/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.5645 - accuracy: 0.4366 - val_loss: 2.4249 - val_accuracy: 0.1538\n",
      "Epoch 15/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.5133 - accuracy: 0.4559 - val_loss: 2.4828 - val_accuracy: 0.1592\n",
      "Epoch 16/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.4612 - accuracy: 0.4756 - val_loss: 2.5717 - val_accuracy: 0.1528\n",
      "Epoch 17/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.3970 - accuracy: 0.5031 - val_loss: 2.5500 - val_accuracy: 0.1581\n",
      "Epoch 18/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.3709 - accuracy: 0.5128 - val_loss: 2.5692 - val_accuracy: 0.1474\n",
      "Epoch 19/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.3060 - accuracy: 0.5384 - val_loss: 2.5478 - val_accuracy: 0.1592\n",
      "Epoch 20/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.2511 - accuracy: 0.5579 - val_loss: 2.5716 - val_accuracy: 0.1603\n",
      "Epoch 21/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.2185 - accuracy: 0.5622 - val_loss: 2.6655 - val_accuracy: 0.1538\n",
      "Epoch 22/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.1803 - accuracy: 0.5839 - val_loss: 2.6465 - val_accuracy: 0.1699\n",
      "Epoch 23/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.1314 - accuracy: 0.6016 - val_loss: 2.6236 - val_accuracy: 0.1592\n",
      "Epoch 24/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.1341 - accuracy: 0.6008 - val_loss: 2.6285 - val_accuracy: 0.1581\n",
      "Epoch 25/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 1.0922 - accuracy: 0.6219 - val_loss: 2.7023 - val_accuracy: 0.1549\n",
      "Epoch 26/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.0643 - accuracy: 0.6221 - val_loss: 2.6837 - val_accuracy: 0.1549\n",
      "Epoch 27/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.0430 - accuracy: 0.6333 - val_loss: 2.7149 - val_accuracy: 0.1560\n",
      "Epoch 28/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 1.0264 - accuracy: 0.6407 - val_loss: 2.7777 - val_accuracy: 0.1560\n",
      "Epoch 29/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.9939 - accuracy: 0.6516 - val_loss: 2.7842 - val_accuracy: 0.1474\n",
      "Epoch 30/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.9908 - accuracy: 0.6498 - val_loss: 2.8211 - val_accuracy: 0.1581\n",
      "Epoch 31/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.9778 - accuracy: 0.6567 - val_loss: 2.7759 - val_accuracy: 0.1517\n",
      "Epoch 32/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.9639 - accuracy: 0.6650 - val_loss: 2.9354 - val_accuracy: 0.1528\n",
      "Epoch 33/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.9417 - accuracy: 0.6719 - val_loss: 2.9050 - val_accuracy: 0.1581\n",
      "Epoch 34/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.9236 - accuracy: 0.6803 - val_loss: 2.9294 - val_accuracy: 0.1528\n",
      "Epoch 35/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.8999 - accuracy: 0.6879 - val_loss: 2.9576 - val_accuracy: 0.1613\n",
      "Epoch 36/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.9075 - accuracy: 0.6874 - val_loss: 2.8567 - val_accuracy: 0.1496\n",
      "Epoch 37/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.8940 - accuracy: 0.6910 - val_loss: 2.8693 - val_accuracy: 0.1645\n",
      "Epoch 38/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.8797 - accuracy: 0.6896 - val_loss: 2.8624 - val_accuracy: 0.1581\n",
      "Epoch 39/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.8667 - accuracy: 0.7004 - val_loss: 2.7713 - val_accuracy: 0.1592\n",
      "Epoch 40/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.8541 - accuracy: 0.7029 - val_loss: 2.6936 - val_accuracy: 0.1549\n",
      "Epoch 41/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.8421 - accuracy: 0.7045 - val_loss: 2.8246 - val_accuracy: 0.1571\n",
      "Epoch 42/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.8287 - accuracy: 0.7131 - val_loss: 2.7682 - val_accuracy: 0.1613\n",
      "Epoch 43/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.8352 - accuracy: 0.7155 - val_loss: 2.8858 - val_accuracy: 0.1592\n",
      "Epoch 44/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.8181 - accuracy: 0.7158 - val_loss: 2.7948 - val_accuracy: 0.1603\n",
      "Epoch 45/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.8082 - accuracy: 0.7228 - val_loss: 2.7028 - val_accuracy: 0.1571\n",
      "Epoch 46/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7988 - accuracy: 0.7234 - val_loss: 2.9159 - val_accuracy: 0.1485\n",
      "Epoch 47/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7848 - accuracy: 0.7263 - val_loss: 2.7963 - val_accuracy: 0.1517\n",
      "Epoch 48/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.7514 - accuracy: 0.7372 - val_loss: 2.9503 - val_accuracy: 0.1571\n",
      "Epoch 49/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7625 - accuracy: 0.7318 - val_loss: 2.8349 - val_accuracy: 0.1688\n",
      "Epoch 50/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7733 - accuracy: 0.7282 - val_loss: 2.8172 - val_accuracy: 0.1645\n",
      "Epoch 51/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7630 - accuracy: 0.7408 - val_loss: 2.7694 - val_accuracy: 0.1603\n",
      "Epoch 52/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7661 - accuracy: 0.7367 - val_loss: 2.8130 - val_accuracy: 0.1581\n",
      "Epoch 53/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.7489 - accuracy: 0.7420 - val_loss: 2.7467 - val_accuracy: 0.1677\n",
      "Epoch 54/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7553 - accuracy: 0.7368 - val_loss: 2.8078 - val_accuracy: 0.1645\n",
      "Epoch 55/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.7295 - accuracy: 0.7496 - val_loss: 2.9668 - val_accuracy: 0.1581\n",
      "Epoch 56/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7439 - accuracy: 0.7456 - val_loss: 2.8173 - val_accuracy: 0.1592\n",
      "Epoch 57/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7157 - accuracy: 0.7544 - val_loss: 2.8430 - val_accuracy: 0.1603\n",
      "Epoch 58/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7348 - accuracy: 0.7473 - val_loss: 2.8897 - val_accuracy: 0.1667\n",
      "Epoch 59/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7110 - accuracy: 0.7500 - val_loss: 2.8046 - val_accuracy: 0.1538\n",
      "Epoch 60/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.7118 - accuracy: 0.7524 - val_loss: 2.7458 - val_accuracy: 0.1581\n",
      "Epoch 61/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7148 - accuracy: 0.7549 - val_loss: 2.9622 - val_accuracy: 0.1560\n",
      "Epoch 62/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6820 - accuracy: 0.7676 - val_loss: 3.1461 - val_accuracy: 0.1528\n",
      "Epoch 63/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6951 - accuracy: 0.7672 - val_loss: 2.8355 - val_accuracy: 0.1571\n",
      "Epoch 64/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.7057 - accuracy: 0.7544 - val_loss: 2.8691 - val_accuracy: 0.1549\n",
      "Epoch 65/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6808 - accuracy: 0.7605 - val_loss: 2.9882 - val_accuracy: 0.1485\n",
      "Epoch 66/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6802 - accuracy: 0.7630 - val_loss: 2.9645 - val_accuracy: 0.1538\n",
      "Epoch 67/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6787 - accuracy: 0.7715 - val_loss: 2.8602 - val_accuracy: 0.1581\n",
      "Epoch 68/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6689 - accuracy: 0.7681 - val_loss: 2.9707 - val_accuracy: 0.1549\n",
      "Epoch 69/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6765 - accuracy: 0.7678 - val_loss: 2.8301 - val_accuracy: 0.1528\n",
      "Epoch 70/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6718 - accuracy: 0.7645 - val_loss: 2.8704 - val_accuracy: 0.1592\n",
      "Epoch 71/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6709 - accuracy: 0.7736 - val_loss: 2.8369 - val_accuracy: 0.1571\n",
      "Epoch 72/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6513 - accuracy: 0.7778 - val_loss: 2.8412 - val_accuracy: 0.1699\n",
      "Epoch 73/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6583 - accuracy: 0.7762 - val_loss: 2.8608 - val_accuracy: 0.1549\n",
      "Epoch 74/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6476 - accuracy: 0.7796 - val_loss: 3.0268 - val_accuracy: 0.1656\n",
      "Epoch 75/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6478 - accuracy: 0.7742 - val_loss: 2.8895 - val_accuracy: 0.1656\n",
      "Epoch 76/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6290 - accuracy: 0.7834 - val_loss: 2.9481 - val_accuracy: 0.1560\n",
      "Epoch 77/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6365 - accuracy: 0.7805 - val_loss: 2.7920 - val_accuracy: 0.1496\n",
      "Epoch 78/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6238 - accuracy: 0.7862 - val_loss: 2.9698 - val_accuracy: 0.1656\n",
      "Epoch 79/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6153 - accuracy: 0.7905 - val_loss: 2.8515 - val_accuracy: 0.1688\n",
      "Epoch 80/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6227 - accuracy: 0.7836 - val_loss: 2.8691 - val_accuracy: 0.1613\n",
      "Epoch 81/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6247 - accuracy: 0.7853 - val_loss: 2.9794 - val_accuracy: 0.1571\n",
      "Epoch 82/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6276 - accuracy: 0.7855 - val_loss: 2.9320 - val_accuracy: 0.1677\n",
      "Epoch 83/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6360 - accuracy: 0.7842 - val_loss: 2.8102 - val_accuracy: 0.1624\n",
      "Epoch 84/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6024 - accuracy: 0.7953 - val_loss: 2.8677 - val_accuracy: 0.1613\n",
      "Epoch 85/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6021 - accuracy: 0.7899 - val_loss: 2.8949 - val_accuracy: 0.1624\n",
      "Epoch 86/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.6028 - accuracy: 0.7903 - val_loss: 2.7936 - val_accuracy: 0.1581\n",
      "Epoch 87/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5922 - accuracy: 0.7946 - val_loss: 2.7833 - val_accuracy: 0.1624\n",
      "Epoch 88/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.6098 - accuracy: 0.7932 - val_loss: 2.7899 - val_accuracy: 0.1677\n",
      "Epoch 89/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5923 - accuracy: 0.8007 - val_loss: 2.8255 - val_accuracy: 0.1677\n",
      "Epoch 90/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.5943 - accuracy: 0.7989 - val_loss: 2.8851 - val_accuracy: 0.1677\n",
      "Epoch 91/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5983 - accuracy: 0.7941 - val_loss: 2.8064 - val_accuracy: 0.1528\n",
      "Epoch 92/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5763 - accuracy: 0.8027 - val_loss: 2.8250 - val_accuracy: 0.1592\n",
      "Epoch 93/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5825 - accuracy: 0.8034 - val_loss: 2.8641 - val_accuracy: 0.1677\n",
      "Epoch 94/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5742 - accuracy: 0.8108 - val_loss: 2.7672 - val_accuracy: 0.1592\n",
      "Epoch 95/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.5760 - accuracy: 0.8072 - val_loss: 2.8508 - val_accuracy: 0.1613\n",
      "Epoch 96/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5863 - accuracy: 0.7994 - val_loss: 2.8264 - val_accuracy: 0.1613\n",
      "Epoch 97/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.5730 - accuracy: 0.7995 - val_loss: 3.0770 - val_accuracy: 0.1549\n",
      "Epoch 98/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5858 - accuracy: 0.8062 - val_loss: 2.7974 - val_accuracy: 0.1538\n",
      "Epoch 99/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5636 - accuracy: 0.8090 - val_loss: 2.7553 - val_accuracy: 0.1624\n",
      "Epoch 100/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.5546 - accuracy: 0.8105 - val_loss: 2.7795 - val_accuracy: 0.1496\n",
      "30/30 [==============================] - 0s 2ms/step\n",
      "\n",
      "MLP - fold 5 done\n",
      "\n",
      "Epoch 1/10\n",
      "244/244 [==============================] - 53s 215ms/step - loss: 1.9265 - acc: 0.3279\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 52s 213ms/step - loss: 1.5564 - acc: 0.4554\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 52s 212ms/step - loss: 1.4292 - acc: 0.5072\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 52s 212ms/step - loss: 1.3224 - acc: 0.5466\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 51s 211ms/step - loss: 1.2515 - acc: 0.5700\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 52s 211ms/step - loss: 1.1884 - acc: 0.5963\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 52s 211ms/step - loss: 1.1262 - acc: 0.6140\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 51s 210ms/step - loss: 1.0728 - acc: 0.6281\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 51s 209ms/step - loss: 1.0320 - acc: 0.6496\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 51s 210ms/step - loss: 0.9965 - acc: 0.6670\n",
      "30/30 [==============================] - 1s 40ms/step\n",
      "\n",
      "CNN - fold 5 done\n",
      "\n",
      "Epoch 1/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.5651 - accuracy: 0.1425 - val_loss: 2.2553 - val_accuracy: 0.1531\n",
      "Epoch 2/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 2.2893 - accuracy: 0.1680 - val_loss: 2.2185 - val_accuracy: 0.1580\n",
      "Epoch 3/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 2.1960 - accuracy: 0.1847 - val_loss: 2.2061 - val_accuracy: 0.1422\n",
      "Epoch 4/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 2.1484 - accuracy: 0.2007 - val_loss: 2.2119 - val_accuracy: 0.1580\n",
      "Epoch 5/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 2.1063 - accuracy: 0.2143 - val_loss: 2.2106 - val_accuracy: 0.1519\n",
      "Epoch 6/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.0701 - accuracy: 0.2256 - val_loss: 2.2117 - val_accuracy: 0.1446\n",
      "Epoch 7/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 2.0055 - accuracy: 0.2565 - val_loss: 2.2140 - val_accuracy: 0.1567\n",
      "Epoch 8/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.9709 - accuracy: 0.2761 - val_loss: 2.1990 - val_accuracy: 0.1555\n",
      "Epoch 9/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.9105 - accuracy: 0.3040 - val_loss: 2.2213 - val_accuracy: 0.1738\n",
      "Epoch 10/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.8370 - accuracy: 0.3368 - val_loss: 2.2186 - val_accuracy: 0.1555\n",
      "Epoch 11/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.7669 - accuracy: 0.3601 - val_loss: 2.2269 - val_accuracy: 0.1507\n",
      "Epoch 12/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.7223 - accuracy: 0.3788 - val_loss: 2.2329 - val_accuracy: 0.1750\n",
      "Epoch 13/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.6493 - accuracy: 0.4085 - val_loss: 2.2841 - val_accuracy: 0.1774\n",
      "Epoch 14/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.6005 - accuracy: 0.4275 - val_loss: 2.2791 - val_accuracy: 0.1701\n",
      "Epoch 15/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.5446 - accuracy: 0.4533 - val_loss: 2.3377 - val_accuracy: 0.1652\n",
      "Epoch 16/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.4702 - accuracy: 0.4760 - val_loss: 2.3640 - val_accuracy: 0.1531\n",
      "Epoch 17/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.4328 - accuracy: 0.4869 - val_loss: 2.3722 - val_accuracy: 0.1616\n",
      "Epoch 18/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.3907 - accuracy: 0.5074 - val_loss: 2.4039 - val_accuracy: 0.1640\n",
      "Epoch 19/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.3196 - accuracy: 0.5283 - val_loss: 2.3996 - val_accuracy: 0.1689\n",
      "Epoch 20/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.2770 - accuracy: 0.5504 - val_loss: 2.5116 - val_accuracy: 0.1652\n",
      "Epoch 21/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.2433 - accuracy: 0.5649 - val_loss: 2.4787 - val_accuracy: 0.1665\n",
      "Epoch 22/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 1.2101 - accuracy: 0.5730 - val_loss: 2.4211 - val_accuracy: 0.1677\n",
      "Epoch 23/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.1799 - accuracy: 0.5910 - val_loss: 2.5258 - val_accuracy: 0.1677\n",
      "Epoch 24/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.1591 - accuracy: 0.5979 - val_loss: 2.4586 - val_accuracy: 0.1835\n",
      "Epoch 25/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.1321 - accuracy: 0.6087 - val_loss: 2.4950 - val_accuracy: 0.1713\n",
      "Epoch 26/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.0931 - accuracy: 0.6227 - val_loss: 2.5396 - val_accuracy: 0.1640\n",
      "Epoch 27/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.0695 - accuracy: 0.6281 - val_loss: 2.6178 - val_accuracy: 0.1665\n",
      "Epoch 28/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.0550 - accuracy: 0.6365 - val_loss: 2.5412 - val_accuracy: 0.1738\n",
      "Epoch 29/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.0298 - accuracy: 0.6472 - val_loss: 2.5835 - val_accuracy: 0.1786\n",
      "Epoch 30/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.0034 - accuracy: 0.6504 - val_loss: 2.5877 - val_accuracy: 0.1689\n",
      "Epoch 31/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 1.0021 - accuracy: 0.6481 - val_loss: 2.6581 - val_accuracy: 0.1628\n",
      "Epoch 32/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9735 - accuracy: 0.6662 - val_loss: 2.5686 - val_accuracy: 0.1665\n",
      "Epoch 33/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.9759 - accuracy: 0.6642 - val_loss: 2.6428 - val_accuracy: 0.1677\n",
      "Epoch 34/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.9327 - accuracy: 0.6770 - val_loss: 2.6289 - val_accuracy: 0.1616\n",
      "Epoch 35/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.9292 - accuracy: 0.6800 - val_loss: 2.6991 - val_accuracy: 0.1580\n",
      "Epoch 36/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8986 - accuracy: 0.6896 - val_loss: 2.6358 - val_accuracy: 0.1616\n",
      "Epoch 37/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8968 - accuracy: 0.6950 - val_loss: 2.6622 - val_accuracy: 0.1543\n",
      "Epoch 38/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9045 - accuracy: 0.6904 - val_loss: 2.6176 - val_accuracy: 0.1543\n",
      "Epoch 39/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8869 - accuracy: 0.6964 - val_loss: 2.7138 - val_accuracy: 0.1640\n",
      "Epoch 40/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8727 - accuracy: 0.6996 - val_loss: 2.6621 - val_accuracy: 0.1652\n",
      "Epoch 41/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8886 - accuracy: 0.7027 - val_loss: 2.5887 - val_accuracy: 0.1738\n",
      "Epoch 42/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8309 - accuracy: 0.7146 - val_loss: 2.6591 - val_accuracy: 0.1701\n",
      "Epoch 43/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8312 - accuracy: 0.7105 - val_loss: 2.6774 - val_accuracy: 0.1677\n",
      "Epoch 44/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8115 - accuracy: 0.7239 - val_loss: 2.6600 - val_accuracy: 0.1604\n",
      "Epoch 45/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8308 - accuracy: 0.7167 - val_loss: 2.6968 - val_accuracy: 0.1652\n",
      "Epoch 46/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8042 - accuracy: 0.7230 - val_loss: 2.6481 - val_accuracy: 0.1604\n",
      "Epoch 47/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7943 - accuracy: 0.7266 - val_loss: 2.6815 - val_accuracy: 0.1628\n",
      "Epoch 48/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8116 - accuracy: 0.7208 - val_loss: 2.6692 - val_accuracy: 0.1580\n",
      "Epoch 49/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.8024 - accuracy: 0.7236 - val_loss: 2.7906 - val_accuracy: 0.1531\n",
      "Epoch 50/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7785 - accuracy: 0.7312 - val_loss: 2.6600 - val_accuracy: 0.1750\n",
      "Epoch 51/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7907 - accuracy: 0.7285 - val_loss: 2.6623 - val_accuracy: 0.1592\n",
      "Epoch 52/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7518 - accuracy: 0.7421 - val_loss: 2.6351 - val_accuracy: 0.1677\n",
      "Epoch 53/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7520 - accuracy: 0.7451 - val_loss: 2.6007 - val_accuracy: 0.1665\n",
      "Epoch 54/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7487 - accuracy: 0.7443 - val_loss: 2.7145 - val_accuracy: 0.1677\n",
      "Epoch 55/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7644 - accuracy: 0.7371 - val_loss: 2.6261 - val_accuracy: 0.1531\n",
      "Epoch 56/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7599 - accuracy: 0.7393 - val_loss: 2.6405 - val_accuracy: 0.1640\n",
      "Epoch 57/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7259 - accuracy: 0.7546 - val_loss: 2.6913 - val_accuracy: 0.1604\n",
      "Epoch 58/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7377 - accuracy: 0.7486 - val_loss: 2.6300 - val_accuracy: 0.1701\n",
      "Epoch 59/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7153 - accuracy: 0.7567 - val_loss: 2.6611 - val_accuracy: 0.1786\n",
      "Epoch 60/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7182 - accuracy: 0.7580 - val_loss: 2.6541 - val_accuracy: 0.1677\n",
      "Epoch 61/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7120 - accuracy: 0.7585 - val_loss: 2.5981 - val_accuracy: 0.1567\n",
      "Epoch 62/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 0.7195 - accuracy: 0.7552 - val_loss: 2.6177 - val_accuracy: 0.1580\n",
      "Epoch 63/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.7113 - accuracy: 0.7585 - val_loss: 2.6723 - val_accuracy: 0.1555\n",
      "Epoch 64/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 0.7089 - accuracy: 0.7575 - val_loss: 2.6798 - val_accuracy: 0.1677\n",
      "Epoch 65/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6920 - accuracy: 0.7631 - val_loss: 2.7809 - val_accuracy: 0.1652\n",
      "Epoch 66/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6911 - accuracy: 0.7618 - val_loss: 2.7557 - val_accuracy: 0.1750\n",
      "Epoch 67/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6953 - accuracy: 0.7680 - val_loss: 2.7763 - val_accuracy: 0.1701\n",
      "Epoch 68/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6870 - accuracy: 0.7701 - val_loss: 2.7112 - val_accuracy: 0.1774\n",
      "Epoch 69/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6842 - accuracy: 0.7661 - val_loss: 2.7666 - val_accuracy: 0.1689\n",
      "Epoch 70/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6667 - accuracy: 0.7757 - val_loss: 2.6560 - val_accuracy: 0.1725\n",
      "Epoch 71/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6604 - accuracy: 0.7770 - val_loss: 2.7157 - val_accuracy: 0.1738\n",
      "Epoch 72/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6566 - accuracy: 0.7751 - val_loss: 2.6432 - val_accuracy: 0.1762\n",
      "Epoch 73/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 0.6789 - accuracy: 0.7765 - val_loss: 2.6210 - val_accuracy: 0.1713\n",
      "Epoch 74/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6622 - accuracy: 0.7724 - val_loss: 2.7033 - val_accuracy: 0.1640\n",
      "Epoch 75/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6418 - accuracy: 0.7820 - val_loss: 2.7069 - val_accuracy: 0.1738\n",
      "Epoch 76/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6391 - accuracy: 0.7828 - val_loss: 2.8652 - val_accuracy: 0.1750\n",
      "Epoch 77/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6484 - accuracy: 0.7813 - val_loss: 2.6662 - val_accuracy: 0.1689\n",
      "Epoch 78/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6311 - accuracy: 0.7848 - val_loss: 2.7849 - val_accuracy: 0.1689\n",
      "Epoch 79/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6280 - accuracy: 0.7827 - val_loss: 2.6876 - val_accuracy: 0.1701\n",
      "Epoch 80/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6329 - accuracy: 0.7866 - val_loss: 2.6434 - val_accuracy: 0.1738\n",
      "Epoch 81/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6350 - accuracy: 0.7861 - val_loss: 2.7036 - val_accuracy: 0.1652\n",
      "Epoch 82/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6403 - accuracy: 0.7863 - val_loss: 2.8068 - val_accuracy: 0.1665\n",
      "Epoch 83/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6343 - accuracy: 0.7835 - val_loss: 2.7276 - val_accuracy: 0.1652\n",
      "Epoch 84/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6329 - accuracy: 0.7862 - val_loss: 2.7280 - val_accuracy: 0.1677\n",
      "Epoch 85/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6345 - accuracy: 0.7852 - val_loss: 2.7292 - val_accuracy: 0.1713\n",
      "Epoch 86/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6168 - accuracy: 0.7962 - val_loss: 2.7134 - val_accuracy: 0.1640\n",
      "Epoch 87/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6203 - accuracy: 0.7920 - val_loss: 2.7988 - val_accuracy: 0.1701\n",
      "Epoch 88/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6163 - accuracy: 0.7919 - val_loss: 2.7294 - val_accuracy: 0.1665\n",
      "Epoch 89/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6064 - accuracy: 0.7993 - val_loss: 2.6800 - val_accuracy: 0.2041\n",
      "Epoch 90/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 0.5995 - accuracy: 0.7988 - val_loss: 2.8016 - val_accuracy: 0.1701\n",
      "Epoch 91/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5918 - accuracy: 0.7988 - val_loss: 2.7288 - val_accuracy: 0.1774\n",
      "Epoch 92/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5922 - accuracy: 0.8006 - val_loss: 2.7984 - val_accuracy: 0.1896\n",
      "Epoch 93/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5937 - accuracy: 0.8021 - val_loss: 2.8157 - val_accuracy: 0.1786\n",
      "Epoch 94/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.6083 - accuracy: 0.7920 - val_loss: 2.7131 - val_accuracy: 0.1665\n",
      "Epoch 95/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 0.5998 - accuracy: 0.7978 - val_loss: 2.6867 - val_accuracy: 0.1896\n",
      "Epoch 96/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5897 - accuracy: 0.7997 - val_loss: 2.8167 - val_accuracy: 0.1762\n",
      "Epoch 97/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5986 - accuracy: 0.7972 - val_loss: 2.7093 - val_accuracy: 0.1823\n",
      "Epoch 98/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5781 - accuracy: 0.8087 - val_loss: 2.8058 - val_accuracy: 0.1677\n",
      "Epoch 99/100\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 0.5807 - accuracy: 0.8040 - val_loss: 2.7276 - val_accuracy: 0.1689\n",
      "Epoch 100/100\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 0.5684 - accuracy: 0.8076 - val_loss: 2.8121 - val_accuracy: 0.1689\n",
      "26/26 [==============================] - 0s 2ms/step\n",
      "\n",
      "MLP - fold 6 done\n",
      "\n",
      "Epoch 1/10\n",
      "248/248 [==============================] - 54s 215ms/step - loss: 1.9020 - acc: 0.3487\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 54s 219ms/step - loss: 1.5477 - acc: 0.4671\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 53s 215ms/step - loss: 1.3891 - acc: 0.5284\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 58s 232ms/step - loss: 1.3026 - acc: 0.5541\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 58s 232ms/step - loss: 1.2223 - acc: 0.5869\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 59s 238ms/step - loss: 1.1455 - acc: 0.6108\n",
      "Epoch 7/10\n",
      "248/248 [==============================] - 62s 252ms/step - loss: 1.1034 - acc: 0.6244\n",
      "Epoch 8/10\n",
      "248/248 [==============================] - 61s 247ms/step - loss: 1.0573 - acc: 0.6424\n",
      "Epoch 9/10\n",
      "248/248 [==============================] - 61s 245ms/step - loss: 1.0105 - acc: 0.6638\n",
      "Epoch 10/10\n",
      "248/248 [==============================] - 61s 248ms/step - loss: 0.9796 - acc: 0.6704\n",
      "26/26 [==============================] - 1s 42ms/step\n",
      "\n",
      "CNN - fold 6 done\n",
      "\n",
      "Epoch 1/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 2.5458 - accuracy: 0.1466 - val_loss: 2.2184 - val_accuracy: 0.2064\n",
      "Epoch 2/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 2.2915 - accuracy: 0.1602 - val_loss: 2.1597 - val_accuracy: 0.1969\n",
      "Epoch 3/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 2.2020 - accuracy: 0.1722 - val_loss: 2.1654 - val_accuracy: 0.1695\n",
      "Epoch 4/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 2.1465 - accuracy: 0.1958 - val_loss: 2.1625 - val_accuracy: 0.2017\n",
      "Epoch 5/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 2.1107 - accuracy: 0.2131 - val_loss: 2.1587 - val_accuracy: 0.2232\n",
      "Epoch 6/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 2.0757 - accuracy: 0.2284 - val_loss: 2.1534 - val_accuracy: 0.2220\n",
      "Epoch 7/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 2.0356 - accuracy: 0.2409 - val_loss: 2.1598 - val_accuracy: 0.2255\n",
      "Epoch 8/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.9840 - accuracy: 0.2620 - val_loss: 2.1599 - val_accuracy: 0.2243\n",
      "Epoch 9/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.9374 - accuracy: 0.2769 - val_loss: 2.1505 - val_accuracy: 0.2184\n",
      "Epoch 10/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.8763 - accuracy: 0.3097 - val_loss: 2.1983 - val_accuracy: 0.1862\n",
      "Epoch 11/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.8149 - accuracy: 0.3400 - val_loss: 2.1815 - val_accuracy: 0.2064\n",
      "Epoch 12/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.7519 - accuracy: 0.3599 - val_loss: 2.2158 - val_accuracy: 0.1921\n",
      "Epoch 13/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.6667 - accuracy: 0.3974 - val_loss: 2.2311 - val_accuracy: 0.1909\n",
      "Epoch 14/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.6233 - accuracy: 0.4160 - val_loss: 2.2916 - val_accuracy: 0.1778\n",
      "Epoch 15/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.5435 - accuracy: 0.4465 - val_loss: 2.3260 - val_accuracy: 0.1862\n",
      "Epoch 16/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.4806 - accuracy: 0.4695 - val_loss: 2.3817 - val_accuracy: 0.1838\n",
      "Epoch 17/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 1.4327 - accuracy: 0.4897 - val_loss: 2.4027 - val_accuracy: 0.1909\n",
      "Epoch 18/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 1.4021 - accuracy: 0.4977 - val_loss: 2.3545 - val_accuracy: 0.2005\n",
      "Epoch 19/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.3410 - accuracy: 0.5247 - val_loss: 2.4317 - val_accuracy: 0.1909\n",
      "Epoch 20/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.3037 - accuracy: 0.5353 - val_loss: 2.4253 - val_accuracy: 0.1909\n",
      "Epoch 21/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.2551 - accuracy: 0.5541 - val_loss: 2.4484 - val_accuracy: 0.1957\n",
      "Epoch 22/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.2208 - accuracy: 0.5599 - val_loss: 2.4995 - val_accuracy: 0.1874\n",
      "Epoch 23/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.2029 - accuracy: 0.5675 - val_loss: 2.5483 - val_accuracy: 0.1957\n",
      "Epoch 24/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 1.1685 - accuracy: 0.5816 - val_loss: 2.5123 - val_accuracy: 0.1933\n",
      "Epoch 25/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 1.1140 - accuracy: 0.6088 - val_loss: 2.6672 - val_accuracy: 0.1862\n",
      "Epoch 26/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.1072 - accuracy: 0.6108 - val_loss: 2.4741 - val_accuracy: 0.2041\n",
      "Epoch 27/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.0767 - accuracy: 0.6174 - val_loss: 2.5585 - val_accuracy: 0.1885\n",
      "Epoch 28/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.0422 - accuracy: 0.6352 - val_loss: 2.5190 - val_accuracy: 0.2005\n",
      "Epoch 29/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.0501 - accuracy: 0.6328 - val_loss: 2.6049 - val_accuracy: 0.1993\n",
      "Epoch 30/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 1.0190 - accuracy: 0.6478 - val_loss: 2.5917 - val_accuracy: 0.1921\n",
      "Epoch 31/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.9915 - accuracy: 0.6557 - val_loss: 2.6171 - val_accuracy: 0.1933\n",
      "Epoch 32/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.9990 - accuracy: 0.6461 - val_loss: 2.5753 - val_accuracy: 0.2112\n",
      "Epoch 33/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.9641 - accuracy: 0.6627 - val_loss: 2.6529 - val_accuracy: 0.1957\n",
      "Epoch 34/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.9479 - accuracy: 0.6768 - val_loss: 2.6291 - val_accuracy: 0.2029\n",
      "Epoch 35/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.9322 - accuracy: 0.6748 - val_loss: 2.5645 - val_accuracy: 0.1969\n",
      "Epoch 36/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.9399 - accuracy: 0.6767 - val_loss: 2.5579 - val_accuracy: 0.2017\n",
      "Epoch 37/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.8977 - accuracy: 0.6869 - val_loss: 2.7057 - val_accuracy: 0.2041\n",
      "Epoch 38/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.9091 - accuracy: 0.6818 - val_loss: 2.6287 - val_accuracy: 0.2029\n",
      "Epoch 39/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.8754 - accuracy: 0.6966 - val_loss: 2.7675 - val_accuracy: 0.1909\n",
      "Epoch 40/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.8560 - accuracy: 0.7040 - val_loss: 2.6851 - val_accuracy: 0.2017\n",
      "Epoch 41/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.8685 - accuracy: 0.7026 - val_loss: 2.6477 - val_accuracy: 0.1993\n",
      "Epoch 42/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.8596 - accuracy: 0.7074 - val_loss: 2.6635 - val_accuracy: 0.2053\n",
      "Epoch 43/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.8611 - accuracy: 0.7132 - val_loss: 2.6882 - val_accuracy: 0.2076\n",
      "Epoch 44/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.8431 - accuracy: 0.7090 - val_loss: 2.5670 - val_accuracy: 0.2112\n",
      "Epoch 45/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.8067 - accuracy: 0.7241 - val_loss: 2.6108 - val_accuracy: 0.2053\n",
      "Epoch 46/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.8259 - accuracy: 0.7136 - val_loss: 2.6285 - val_accuracy: 0.1993\n",
      "Epoch 47/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.8112 - accuracy: 0.7245 - val_loss: 2.5873 - val_accuracy: 0.1969\n",
      "Epoch 48/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7962 - accuracy: 0.7274 - val_loss: 2.6345 - val_accuracy: 0.1957\n",
      "Epoch 49/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.8009 - accuracy: 0.7246 - val_loss: 2.6000 - val_accuracy: 0.1957\n",
      "Epoch 50/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7754 - accuracy: 0.7373 - val_loss: 2.6406 - val_accuracy: 0.2076\n",
      "Epoch 51/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7684 - accuracy: 0.7430 - val_loss: 2.6201 - val_accuracy: 0.2160\n",
      "Epoch 52/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7754 - accuracy: 0.7387 - val_loss: 2.5223 - val_accuracy: 0.2112\n",
      "Epoch 53/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7480 - accuracy: 0.7473 - val_loss: 2.6855 - val_accuracy: 0.2208\n",
      "Epoch 54/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7462 - accuracy: 0.7459 - val_loss: 2.6089 - val_accuracy: 0.1981\n",
      "Epoch 55/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7490 - accuracy: 0.7439 - val_loss: 2.7275 - val_accuracy: 0.2064\n",
      "Epoch 56/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7411 - accuracy: 0.7483 - val_loss: 2.7500 - val_accuracy: 0.1993\n",
      "Epoch 57/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.7223 - accuracy: 0.7597 - val_loss: 2.7129 - val_accuracy: 0.1981\n",
      "Epoch 58/100\n",
      "247/247 [==============================] - 1s 5ms/step - loss: 0.7362 - accuracy: 0.7506 - val_loss: 2.6607 - val_accuracy: 0.1945\n",
      "Epoch 59/100\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.7399 - accuracy: 0.7563 - val_loss: 2.6988 - val_accuracy: 0.1957\n",
      "Epoch 60/100\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.7212 - accuracy: 0.7520 - val_loss: 2.6079 - val_accuracy: 0.2005\n",
      "Epoch 61/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.7041 - accuracy: 0.7607 - val_loss: 2.7326 - val_accuracy: 0.1957\n",
      "Epoch 62/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.7014 - accuracy: 0.7636 - val_loss: 2.6771 - val_accuracy: 0.1993\n",
      "Epoch 63/100\n",
      "247/247 [==============================] - 2s 8ms/step - loss: 0.7025 - accuracy: 0.7629 - val_loss: 2.6514 - val_accuracy: 0.1933\n",
      "Epoch 64/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6749 - accuracy: 0.7740 - val_loss: 2.6922 - val_accuracy: 0.1993\n",
      "Epoch 65/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6926 - accuracy: 0.7664 - val_loss: 2.6915 - val_accuracy: 0.1993\n",
      "Epoch 66/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6997 - accuracy: 0.7660 - val_loss: 2.6161 - val_accuracy: 0.1981\n",
      "Epoch 67/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6720 - accuracy: 0.7707 - val_loss: 2.6615 - val_accuracy: 0.1742\n",
      "Epoch 68/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.6699 - accuracy: 0.7722 - val_loss: 2.7314 - val_accuracy: 0.1874\n",
      "Epoch 69/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6894 - accuracy: 0.7698 - val_loss: 2.6537 - val_accuracy: 0.1945\n",
      "Epoch 70/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6655 - accuracy: 0.7796 - val_loss: 2.6446 - val_accuracy: 0.1993\n",
      "Epoch 71/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6570 - accuracy: 0.7777 - val_loss: 2.6872 - val_accuracy: 0.1993\n",
      "Epoch 72/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6512 - accuracy: 0.7854 - val_loss: 2.7436 - val_accuracy: 0.1993\n",
      "Epoch 73/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6550 - accuracy: 0.7782 - val_loss: 2.7277 - val_accuracy: 0.1993\n",
      "Epoch 74/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6413 - accuracy: 0.7872 - val_loss: 2.6780 - val_accuracy: 0.2064\n",
      "Epoch 75/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.7848 - val_loss: 2.7465 - val_accuracy: 0.1921\n",
      "Epoch 76/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6465 - accuracy: 0.7817 - val_loss: 2.6634 - val_accuracy: 0.1909\n",
      "Epoch 77/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6382 - accuracy: 0.7884 - val_loss: 2.7356 - val_accuracy: 0.2017\n",
      "Epoch 78/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6203 - accuracy: 0.7872 - val_loss: 2.7723 - val_accuracy: 0.1981\n",
      "Epoch 79/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6536 - accuracy: 0.7853 - val_loss: 2.6384 - val_accuracy: 0.2112\n",
      "Epoch 80/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.6177 - accuracy: 0.7905 - val_loss: 2.6926 - val_accuracy: 0.1909\n",
      "Epoch 81/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.7922 - val_loss: 2.7639 - val_accuracy: 0.2017\n",
      "Epoch 82/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6033 - accuracy: 0.7982 - val_loss: 2.6660 - val_accuracy: 0.1850\n",
      "Epoch 83/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6257 - accuracy: 0.7982 - val_loss: 2.7867 - val_accuracy: 0.2005\n",
      "Epoch 84/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6091 - accuracy: 0.7926 - val_loss: 2.7093 - val_accuracy: 0.1802\n",
      "Epoch 85/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5994 - accuracy: 0.7974 - val_loss: 2.6577 - val_accuracy: 0.2029\n",
      "Epoch 86/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6024 - accuracy: 0.7982 - val_loss: 2.6706 - val_accuracy: 0.1957\n",
      "Epoch 87/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6301 - accuracy: 0.7873 - val_loss: 2.6715 - val_accuracy: 0.1945\n",
      "Epoch 88/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6163 - accuracy: 0.7938 - val_loss: 2.7591 - val_accuracy: 0.1814\n",
      "Epoch 89/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6097 - accuracy: 0.7939 - val_loss: 2.6638 - val_accuracy: 0.1874\n",
      "Epoch 90/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5976 - accuracy: 0.8006 - val_loss: 2.7745 - val_accuracy: 0.2005\n",
      "Epoch 91/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5893 - accuracy: 0.8025 - val_loss: 2.6267 - val_accuracy: 0.2017\n",
      "Epoch 92/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6082 - accuracy: 0.7976 - val_loss: 2.6420 - val_accuracy: 0.2017\n",
      "Epoch 93/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.6065 - accuracy: 0.7987 - val_loss: 2.7504 - val_accuracy: 0.1921\n",
      "Epoch 94/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5858 - accuracy: 0.8011 - val_loss: 2.8087 - val_accuracy: 0.1802\n",
      "Epoch 95/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5802 - accuracy: 0.8058 - val_loss: 2.6691 - val_accuracy: 0.2005\n",
      "Epoch 96/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5740 - accuracy: 0.8081 - val_loss: 2.7260 - val_accuracy: 0.2076\n",
      "Epoch 97/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5661 - accuracy: 0.8067 - val_loss: 2.6516 - val_accuracy: 0.1897\n",
      "Epoch 98/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5692 - accuracy: 0.8078 - val_loss: 2.7198 - val_accuracy: 0.2005\n",
      "Epoch 99/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5498 - accuracy: 0.8166 - val_loss: 2.8013 - val_accuracy: 0.1969\n",
      "Epoch 100/100\n",
      "247/247 [==============================] - 1s 6ms/step - loss: 0.5603 - accuracy: 0.8139 - val_loss: 2.7400 - val_accuracy: 0.1981\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "\n",
      "MLP - fold 7 done\n",
      "\n",
      "Epoch 1/10\n",
      "247/247 [==============================] - 61s 240ms/step - loss: 1.9169 - acc: 0.3372\n",
      "Epoch 2/10\n",
      "247/247 [==============================] - 59s 238ms/step - loss: 1.6199 - acc: 0.4396\n",
      "Epoch 3/10\n",
      "247/247 [==============================] - 59s 238ms/step - loss: 1.4387 - acc: 0.5065\n",
      "Epoch 4/10\n",
      "247/247 [==============================] - 59s 239ms/step - loss: 1.3328 - acc: 0.5431\n",
      "Epoch 5/10\n",
      "247/247 [==============================] - 59s 238ms/step - loss: 1.2451 - acc: 0.5821\n",
      "Epoch 6/10\n",
      "247/247 [==============================] - 59s 238ms/step - loss: 1.1678 - acc: 0.6010\n",
      "Epoch 7/10\n",
      "247/247 [==============================] - 59s 239ms/step - loss: 1.1114 - acc: 0.6245\n",
      "Epoch 8/10\n",
      "247/247 [==============================] - 59s 239ms/step - loss: 1.0643 - acc: 0.6393\n",
      "Epoch 9/10\n",
      "247/247 [==============================] - 59s 238ms/step - loss: 1.0237 - acc: 0.6563\n",
      "Epoch 10/10\n",
      "247/247 [==============================] - 59s 239ms/step - loss: 0.9781 - acc: 0.6708\n",
      "27/27 [==============================] - 1s 45ms/step\n",
      "\n",
      "CNN - fold 7 done\n",
      "\n",
      "Epoch 1/100\n",
      "248/248 [==============================] - 3s 9ms/step - loss: 2.5573 - accuracy: 0.1471 - val_loss: 2.2594 - val_accuracy: 0.1154\n",
      "Epoch 2/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 2.3040 - accuracy: 0.1673 - val_loss: 2.2704 - val_accuracy: 0.1228\n",
      "Epoch 3/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 2.2119 - accuracy: 0.1799 - val_loss: 2.2721 - val_accuracy: 0.1340\n",
      "Epoch 4/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 2.1472 - accuracy: 0.1887 - val_loss: 2.2620 - val_accuracy: 0.1328\n",
      "Epoch 5/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 2.1127 - accuracy: 0.2022 - val_loss: 2.2632 - val_accuracy: 0.1377\n",
      "Epoch 6/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 2.0724 - accuracy: 0.2266 - val_loss: 2.2609 - val_accuracy: 0.1452\n",
      "Epoch 7/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 2.0468 - accuracy: 0.2434 - val_loss: 2.2765 - val_accuracy: 0.1687\n",
      "Epoch 8/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.9920 - accuracy: 0.2566 - val_loss: 2.3232 - val_accuracy: 0.1700\n",
      "Epoch 9/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.9315 - accuracy: 0.2839 - val_loss: 2.3208 - val_accuracy: 0.1712\n",
      "Epoch 10/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.8680 - accuracy: 0.3154 - val_loss: 2.3818 - val_accuracy: 0.1712\n",
      "Epoch 11/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.7956 - accuracy: 0.3514 - val_loss: 2.3802 - val_accuracy: 0.1476\n",
      "Epoch 12/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.7509 - accuracy: 0.3647 - val_loss: 2.3719 - val_accuracy: 0.1402\n",
      "Epoch 13/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.6692 - accuracy: 0.3999 - val_loss: 2.3969 - val_accuracy: 0.1489\n",
      "Epoch 14/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.6122 - accuracy: 0.4208 - val_loss: 2.4362 - val_accuracy: 0.1328\n",
      "Epoch 15/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.5676 - accuracy: 0.4391 - val_loss: 2.4166 - val_accuracy: 0.1563\n",
      "Epoch 16/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.4886 - accuracy: 0.4709 - val_loss: 2.4221 - val_accuracy: 0.1799\n",
      "Epoch 17/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.4164 - accuracy: 0.4958 - val_loss: 2.5036 - val_accuracy: 0.1588\n",
      "Epoch 18/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.3739 - accuracy: 0.5140 - val_loss: 2.5435 - val_accuracy: 0.1588\n",
      "Epoch 19/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.3278 - accuracy: 0.5293 - val_loss: 2.4957 - val_accuracy: 0.1489\n",
      "Epoch 20/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.2703 - accuracy: 0.5565 - val_loss: 2.6074 - val_accuracy: 0.1613\n",
      "Epoch 21/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.2430 - accuracy: 0.5637 - val_loss: 2.6477 - val_accuracy: 0.1638\n",
      "Epoch 22/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.2043 - accuracy: 0.5834 - val_loss: 2.5984 - val_accuracy: 0.1712\n",
      "Epoch 23/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.1780 - accuracy: 0.5883 - val_loss: 2.6801 - val_accuracy: 0.1687\n",
      "Epoch 24/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.1379 - accuracy: 0.5997 - val_loss: 2.7500 - val_accuracy: 0.1725\n",
      "Epoch 25/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.1163 - accuracy: 0.6083 - val_loss: 2.6678 - val_accuracy: 0.1824\n",
      "Epoch 26/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.0777 - accuracy: 0.6233 - val_loss: 2.6926 - val_accuracy: 0.1911\n",
      "Epoch 27/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.0592 - accuracy: 0.6310 - val_loss: 2.7858 - val_accuracy: 0.1650\n",
      "Epoch 28/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.0327 - accuracy: 0.6412 - val_loss: 2.6835 - val_accuracy: 0.1799\n",
      "Epoch 29/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.0096 - accuracy: 0.6499 - val_loss: 2.7248 - val_accuracy: 0.1749\n",
      "Epoch 30/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 1.0003 - accuracy: 0.6515 - val_loss: 2.8772 - val_accuracy: 0.1749\n",
      "Epoch 31/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.9849 - accuracy: 0.6570 - val_loss: 2.8093 - val_accuracy: 0.1762\n",
      "Epoch 32/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.9685 - accuracy: 0.6634 - val_loss: 2.7925 - val_accuracy: 0.1811\n",
      "Epoch 33/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.9420 - accuracy: 0.6679 - val_loss: 2.8110 - val_accuracy: 0.1787\n",
      "Epoch 34/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.9380 - accuracy: 0.6776 - val_loss: 2.7614 - val_accuracy: 0.1700\n",
      "Epoch 35/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.9114 - accuracy: 0.6870 - val_loss: 2.7974 - val_accuracy: 0.1725\n",
      "Epoch 36/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8912 - accuracy: 0.6928 - val_loss: 2.6940 - val_accuracy: 0.1774\n",
      "Epoch 37/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8797 - accuracy: 0.6968 - val_loss: 2.7308 - val_accuracy: 0.1749\n",
      "Epoch 38/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8800 - accuracy: 0.6956 - val_loss: 2.6829 - val_accuracy: 0.1700\n",
      "Epoch 39/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8756 - accuracy: 0.6967 - val_loss: 2.6716 - val_accuracy: 0.1749\n",
      "Epoch 40/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8488 - accuracy: 0.7101 - val_loss: 2.6977 - val_accuracy: 0.1700\n",
      "Epoch 41/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8459 - accuracy: 0.7144 - val_loss: 2.7096 - val_accuracy: 0.1787\n",
      "Epoch 42/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8426 - accuracy: 0.7088 - val_loss: 2.7187 - val_accuracy: 0.1675\n",
      "Epoch 43/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8152 - accuracy: 0.7184 - val_loss: 2.6877 - val_accuracy: 0.1774\n",
      "Epoch 44/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8077 - accuracy: 0.7194 - val_loss: 2.7505 - val_accuracy: 0.1811\n",
      "Epoch 45/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7973 - accuracy: 0.7304 - val_loss: 2.7453 - val_accuracy: 0.1774\n",
      "Epoch 46/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.8000 - accuracy: 0.7320 - val_loss: 2.6821 - val_accuracy: 0.1749\n",
      "Epoch 47/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7974 - accuracy: 0.7262 - val_loss: 2.8772 - val_accuracy: 0.1675\n",
      "Epoch 48/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7838 - accuracy: 0.7272 - val_loss: 2.9479 - val_accuracy: 0.1625\n",
      "Epoch 49/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7729 - accuracy: 0.7403 - val_loss: 2.8839 - val_accuracy: 0.1675\n",
      "Epoch 50/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7701 - accuracy: 0.7319 - val_loss: 2.7211 - val_accuracy: 0.1799\n",
      "Epoch 51/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7776 - accuracy: 0.7272 - val_loss: 2.8156 - val_accuracy: 0.1663\n",
      "Epoch 52/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7495 - accuracy: 0.7402 - val_loss: 2.8752 - val_accuracy: 0.1700\n",
      "Epoch 53/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7502 - accuracy: 0.7409 - val_loss: 2.8484 - val_accuracy: 0.1663\n",
      "Epoch 54/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7613 - accuracy: 0.7440 - val_loss: 2.8598 - val_accuracy: 0.1538\n",
      "Epoch 55/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7318 - accuracy: 0.7499 - val_loss: 2.8317 - val_accuracy: 0.1625\n",
      "Epoch 56/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7268 - accuracy: 0.7477 - val_loss: 2.8070 - val_accuracy: 0.1712\n",
      "Epoch 57/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7222 - accuracy: 0.7533 - val_loss: 2.8549 - val_accuracy: 0.1849\n",
      "Epoch 58/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7153 - accuracy: 0.7482 - val_loss: 2.7822 - val_accuracy: 0.1687\n",
      "Epoch 59/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7295 - accuracy: 0.7468 - val_loss: 2.9441 - val_accuracy: 0.1675\n",
      "Epoch 60/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7190 - accuracy: 0.7588 - val_loss: 2.9952 - val_accuracy: 0.1638\n",
      "Epoch 61/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7278 - accuracy: 0.7538 - val_loss: 2.9482 - val_accuracy: 0.1650\n",
      "Epoch 62/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6933 - accuracy: 0.7636 - val_loss: 2.9687 - val_accuracy: 0.1600\n",
      "Epoch 63/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.7039 - accuracy: 0.7552 - val_loss: 3.0142 - val_accuracy: 0.1638\n",
      "Epoch 64/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6879 - accuracy: 0.7700 - val_loss: 2.9877 - val_accuracy: 0.1638\n",
      "Epoch 65/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6823 - accuracy: 0.7565 - val_loss: 2.8930 - val_accuracy: 0.1737\n",
      "Epoch 66/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6857 - accuracy: 0.7700 - val_loss: 2.9778 - val_accuracy: 0.1650\n",
      "Epoch 67/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6852 - accuracy: 0.7661 - val_loss: 2.7691 - val_accuracy: 0.1824\n",
      "Epoch 68/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6906 - accuracy: 0.7677 - val_loss: 2.7949 - val_accuracy: 0.1663\n",
      "Epoch 69/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6573 - accuracy: 0.7786 - val_loss: 2.8799 - val_accuracy: 0.1712\n",
      "Epoch 70/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6499 - accuracy: 0.7792 - val_loss: 3.0702 - val_accuracy: 0.1551\n",
      "Epoch 71/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6767 - accuracy: 0.7742 - val_loss: 2.7708 - val_accuracy: 0.1638\n",
      "Epoch 72/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6520 - accuracy: 0.7811 - val_loss: 2.7681 - val_accuracy: 0.1663\n",
      "Epoch 73/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6593 - accuracy: 0.7744 - val_loss: 2.8393 - val_accuracy: 0.1588\n",
      "Epoch 74/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6397 - accuracy: 0.7843 - val_loss: 2.7870 - val_accuracy: 0.1725\n",
      "Epoch 75/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6409 - accuracy: 0.7835 - val_loss: 2.8049 - val_accuracy: 0.1576\n",
      "Epoch 76/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6403 - accuracy: 0.7837 - val_loss: 2.7934 - val_accuracy: 0.1687\n",
      "Epoch 77/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6488 - accuracy: 0.7824 - val_loss: 2.7995 - val_accuracy: 0.1588\n",
      "Epoch 78/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6206 - accuracy: 0.7867 - val_loss: 2.8172 - val_accuracy: 0.1650\n",
      "Epoch 79/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6262 - accuracy: 0.7834 - val_loss: 2.8013 - val_accuracy: 0.1538\n",
      "Epoch 80/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6217 - accuracy: 0.7879 - val_loss: 2.9370 - val_accuracy: 0.1687\n",
      "Epoch 81/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6082 - accuracy: 0.7960 - val_loss: 2.9221 - val_accuracy: 0.1625\n",
      "Epoch 82/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6048 - accuracy: 0.7908 - val_loss: 2.8470 - val_accuracy: 0.1613\n",
      "Epoch 83/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6070 - accuracy: 0.7941 - val_loss: 2.7872 - val_accuracy: 0.1675\n",
      "Epoch 84/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6006 - accuracy: 0.7993 - val_loss: 2.7590 - val_accuracy: 0.1762\n",
      "Epoch 85/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6003 - accuracy: 0.7947 - val_loss: 2.9072 - val_accuracy: 0.1687\n",
      "Epoch 86/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5908 - accuracy: 0.7950 - val_loss: 2.8450 - val_accuracy: 0.1700\n",
      "Epoch 87/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5819 - accuracy: 0.8007 - val_loss: 2.8444 - val_accuracy: 0.1737\n",
      "Epoch 88/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5756 - accuracy: 0.8036 - val_loss: 2.8852 - val_accuracy: 0.1700\n",
      "Epoch 89/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.6074 - accuracy: 0.7927 - val_loss: 2.8369 - val_accuracy: 0.1787\n",
      "Epoch 90/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5887 - accuracy: 0.8009 - val_loss: 2.8732 - val_accuracy: 0.1675\n",
      "Epoch 91/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5949 - accuracy: 0.8053 - val_loss: 2.8733 - val_accuracy: 0.1749\n",
      "Epoch 92/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5658 - accuracy: 0.8041 - val_loss: 2.8822 - val_accuracy: 0.1774\n",
      "Epoch 93/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5936 - accuracy: 0.7964 - val_loss: 2.7951 - val_accuracy: 0.1762\n",
      "Epoch 94/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5746 - accuracy: 0.8015 - val_loss: 2.8250 - val_accuracy: 0.1712\n",
      "Epoch 95/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5835 - accuracy: 0.8032 - val_loss: 2.7762 - val_accuracy: 0.1749\n",
      "Epoch 96/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5813 - accuracy: 0.8004 - val_loss: 2.8274 - val_accuracy: 0.1762\n",
      "Epoch 97/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5634 - accuracy: 0.8063 - val_loss: 3.0277 - val_accuracy: 0.1675\n",
      "Epoch 98/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5679 - accuracy: 0.8099 - val_loss: 2.9865 - val_accuracy: 0.1725\n",
      "Epoch 99/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5728 - accuracy: 0.8130 - val_loss: 2.8949 - val_accuracy: 0.1663\n",
      "Epoch 100/100\n",
      "248/248 [==============================] - 2s 9ms/step - loss: 0.5803 - accuracy: 0.8043 - val_loss: 2.7632 - val_accuracy: 0.1774\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "\n",
      "MLP - fold 8 done\n",
      "\n",
      "Epoch 1/10\n",
      "248/248 [==============================] - 60s 238ms/step - loss: 1.9325 - acc: 0.3282\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 59s 236ms/step - loss: 1.5505 - acc: 0.4710\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 58s 232ms/step - loss: 1.3963 - acc: 0.5241\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 57s 231ms/step - loss: 1.3003 - acc: 0.5482\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 57s 230ms/step - loss: 1.2024 - acc: 0.5881\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 57s 231ms/step - loss: 1.1281 - acc: 0.6162\n",
      "Epoch 7/10\n",
      "248/248 [==============================] - 57s 229ms/step - loss: 1.0896 - acc: 0.6238\n",
      "Epoch 8/10\n",
      "248/248 [==============================] - 57s 229ms/step - loss: 1.0382 - acc: 0.6480\n",
      "Epoch 9/10\n",
      "248/248 [==============================] - 57s 229ms/step - loss: 0.9859 - acc: 0.6684\n",
      "Epoch 10/10\n",
      "248/248 [==============================] - 57s 229ms/step - loss: 0.9589 - acc: 0.6788\n",
      "26/26 [==============================] - 1s 43ms/step\n",
      "\n",
      "CNN - fold 8 done\n",
      "\n",
      "Epoch 1/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 2.5468 - accuracy: 0.1426 - val_loss: 2.2037 - val_accuracy: 0.1569\n",
      "Epoch 2/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.2860 - accuracy: 0.1683 - val_loss: 2.2229 - val_accuracy: 0.1593\n",
      "Epoch 3/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.2005 - accuracy: 0.1737 - val_loss: 2.1901 - val_accuracy: 0.1838\n",
      "Epoch 4/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.1511 - accuracy: 0.1945 - val_loss: 2.1880 - val_accuracy: 0.1875\n",
      "Epoch 5/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.1117 - accuracy: 0.2125 - val_loss: 2.1757 - val_accuracy: 0.2230\n",
      "Epoch 6/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.0635 - accuracy: 0.2294 - val_loss: 2.1832 - val_accuracy: 0.1949\n",
      "Epoch 7/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 2.0199 - accuracy: 0.2525 - val_loss: 2.2112 - val_accuracy: 0.2010\n",
      "Epoch 8/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.9734 - accuracy: 0.2789 - val_loss: 2.2021 - val_accuracy: 0.1850\n",
      "Epoch 9/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.9027 - accuracy: 0.3087 - val_loss: 2.2118 - val_accuracy: 0.1716\n",
      "Epoch 10/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.8349 - accuracy: 0.3394 - val_loss: 2.2082 - val_accuracy: 0.1998\n",
      "Epoch 11/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.7696 - accuracy: 0.3675 - val_loss: 2.2469 - val_accuracy: 0.1777\n",
      "Epoch 12/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.6763 - accuracy: 0.3981 - val_loss: 2.3084 - val_accuracy: 0.1593\n",
      "Epoch 13/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.6285 - accuracy: 0.4203 - val_loss: 2.3180 - val_accuracy: 0.1483\n",
      "Epoch 14/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.5467 - accuracy: 0.4541 - val_loss: 2.3190 - val_accuracy: 0.1716\n",
      "Epoch 15/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.4877 - accuracy: 0.4765 - val_loss: 2.3826 - val_accuracy: 0.1912\n",
      "Epoch 16/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.4377 - accuracy: 0.4986 - val_loss: 2.4542 - val_accuracy: 0.1569\n",
      "Epoch 17/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.3832 - accuracy: 0.5130 - val_loss: 2.3710 - val_accuracy: 0.2194\n",
      "Epoch 18/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.3204 - accuracy: 0.5406 - val_loss: 2.4763 - val_accuracy: 0.1740\n",
      "Epoch 19/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.2722 - accuracy: 0.5528 - val_loss: 2.5211 - val_accuracy: 0.2047\n",
      "Epoch 20/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.2292 - accuracy: 0.5822 - val_loss: 2.5356 - val_accuracy: 0.1936\n",
      "Epoch 21/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.2178 - accuracy: 0.5778 - val_loss: 2.5107 - val_accuracy: 0.1850\n",
      "Epoch 22/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.1829 - accuracy: 0.5870 - val_loss: 2.5460 - val_accuracy: 0.1973\n",
      "Epoch 23/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.1391 - accuracy: 0.6065 - val_loss: 2.5955 - val_accuracy: 0.1924\n",
      "Epoch 24/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.1326 - accuracy: 0.6022 - val_loss: 2.6045 - val_accuracy: 0.1642\n",
      "Epoch 25/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.0778 - accuracy: 0.6224 - val_loss: 2.5702 - val_accuracy: 0.1985\n",
      "Epoch 26/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.0716 - accuracy: 0.6286 - val_loss: 2.5440 - val_accuracy: 0.1973\n",
      "Epoch 27/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.0306 - accuracy: 0.6483 - val_loss: 2.5523 - val_accuracy: 0.1605\n",
      "Epoch 28/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.0216 - accuracy: 0.6508 - val_loss: 2.5690 - val_accuracy: 0.1961\n",
      "Epoch 29/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 1.0145 - accuracy: 0.6534 - val_loss: 2.6810 - val_accuracy: 0.1703\n",
      "Epoch 30/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9794 - accuracy: 0.6606 - val_loss: 2.5779 - val_accuracy: 0.2108\n",
      "Epoch 31/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9708 - accuracy: 0.6679 - val_loss: 2.5904 - val_accuracy: 0.1973\n",
      "Epoch 32/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9356 - accuracy: 0.6849 - val_loss: 2.6731 - val_accuracy: 0.1838\n",
      "Epoch 33/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9285 - accuracy: 0.6828 - val_loss: 2.5847 - val_accuracy: 0.1973\n",
      "Epoch 34/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9285 - accuracy: 0.6862 - val_loss: 2.5873 - val_accuracy: 0.1900\n",
      "Epoch 35/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9247 - accuracy: 0.6890 - val_loss: 2.6009 - val_accuracy: 0.1777\n",
      "Epoch 36/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.9041 - accuracy: 0.6935 - val_loss: 2.6267 - val_accuracy: 0.1961\n",
      "Epoch 37/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8835 - accuracy: 0.7006 - val_loss: 2.6171 - val_accuracy: 0.1949\n",
      "Epoch 38/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8743 - accuracy: 0.6978 - val_loss: 2.5788 - val_accuracy: 0.2071\n",
      "Epoch 39/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8719 - accuracy: 0.6968 - val_loss: 2.6431 - val_accuracy: 0.2108\n",
      "Epoch 40/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8448 - accuracy: 0.7129 - val_loss: 2.6175 - val_accuracy: 0.2034\n",
      "Epoch 41/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8272 - accuracy: 0.7170 - val_loss: 2.6865 - val_accuracy: 0.1777\n",
      "Epoch 42/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8295 - accuracy: 0.7127 - val_loss: 2.5596 - val_accuracy: 0.1789\n",
      "Epoch 43/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8086 - accuracy: 0.7204 - val_loss: 2.7256 - val_accuracy: 0.1900\n",
      "Epoch 44/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8274 - accuracy: 0.7198 - val_loss: 2.6765 - val_accuracy: 0.1814\n",
      "Epoch 45/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.8006 - accuracy: 0.7241 - val_loss: 2.6574 - val_accuracy: 0.1998\n",
      "Epoch 46/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7984 - accuracy: 0.7294 - val_loss: 2.6680 - val_accuracy: 0.2022\n",
      "Epoch 47/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7772 - accuracy: 0.7294 - val_loss: 2.6656 - val_accuracy: 0.1887\n",
      "Epoch 48/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7900 - accuracy: 0.7280 - val_loss: 2.6271 - val_accuracy: 0.2034\n",
      "Epoch 49/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7840 - accuracy: 0.7376 - val_loss: 2.7213 - val_accuracy: 0.1324\n",
      "Epoch 50/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7633 - accuracy: 0.7384 - val_loss: 2.8326 - val_accuracy: 0.1949\n",
      "Epoch 51/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7600 - accuracy: 0.7432 - val_loss: 2.6965 - val_accuracy: 0.2047\n",
      "Epoch 52/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7624 - accuracy: 0.7341 - val_loss: 2.7517 - val_accuracy: 0.1863\n",
      "Epoch 53/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7159 - accuracy: 0.7528 - val_loss: 2.6562 - val_accuracy: 0.2010\n",
      "Epoch 54/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7394 - accuracy: 0.7549 - val_loss: 2.7106 - val_accuracy: 0.1752\n",
      "Epoch 55/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7489 - accuracy: 0.7442 - val_loss: 2.6864 - val_accuracy: 0.1814\n",
      "Epoch 56/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7507 - accuracy: 0.7499 - val_loss: 2.6104 - val_accuracy: 0.1863\n",
      "Epoch 57/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7223 - accuracy: 0.7551 - val_loss: 2.6838 - val_accuracy: 0.1863\n",
      "Epoch 58/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7226 - accuracy: 0.7586 - val_loss: 2.6961 - val_accuracy: 0.1985\n",
      "Epoch 59/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6999 - accuracy: 0.7628 - val_loss: 2.8665 - val_accuracy: 0.1850\n",
      "Epoch 60/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.7063 - accuracy: 0.7628 - val_loss: 2.6956 - val_accuracy: 0.1936\n",
      "Epoch 61/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6960 - accuracy: 0.7601 - val_loss: 2.6470 - val_accuracy: 0.2022\n",
      "Epoch 62/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6984 - accuracy: 0.7587 - val_loss: 2.6845 - val_accuracy: 0.1887\n",
      "Epoch 63/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6985 - accuracy: 0.7615 - val_loss: 2.6967 - val_accuracy: 0.1863\n",
      "Epoch 64/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6808 - accuracy: 0.7725 - val_loss: 2.7914 - val_accuracy: 0.1801\n",
      "Epoch 65/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6765 - accuracy: 0.7698 - val_loss: 2.6954 - val_accuracy: 0.1973\n",
      "Epoch 66/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6923 - accuracy: 0.7612 - val_loss: 2.7791 - val_accuracy: 0.1863\n",
      "Epoch 67/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6995 - accuracy: 0.7654 - val_loss: 2.7144 - val_accuracy: 0.1924\n",
      "Epoch 68/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6663 - accuracy: 0.7754 - val_loss: 2.6881 - val_accuracy: 0.2096\n",
      "Epoch 69/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6591 - accuracy: 0.7756 - val_loss: 2.7070 - val_accuracy: 0.1985\n",
      "Epoch 70/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6756 - accuracy: 0.7729 - val_loss: 2.6699 - val_accuracy: 0.1949\n",
      "Epoch 71/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6789 - accuracy: 0.7701 - val_loss: 2.6602 - val_accuracy: 0.2071\n",
      "Epoch 72/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6500 - accuracy: 0.7759 - val_loss: 2.6892 - val_accuracy: 0.2022\n",
      "Epoch 73/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6524 - accuracy: 0.7792 - val_loss: 2.7694 - val_accuracy: 0.1863\n",
      "Epoch 74/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6456 - accuracy: 0.7793 - val_loss: 2.7201 - val_accuracy: 0.1912\n",
      "Epoch 75/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6397 - accuracy: 0.7811 - val_loss: 2.8081 - val_accuracy: 0.1900\n",
      "Epoch 76/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 0.6387 - accuracy: 0.7851 - val_loss: 2.7268 - val_accuracy: 0.1961\n",
      "Epoch 77/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6274 - accuracy: 0.7817 - val_loss: 2.6982 - val_accuracy: 0.2145\n",
      "Epoch 78/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6547 - accuracy: 0.7769 - val_loss: 2.7906 - val_accuracy: 0.1900\n",
      "Epoch 79/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6259 - accuracy: 0.7927 - val_loss: 2.7018 - val_accuracy: 0.1985\n",
      "Epoch 80/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6364 - accuracy: 0.7806 - val_loss: 2.7523 - val_accuracy: 0.2157\n",
      "Epoch 81/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6191 - accuracy: 0.7900 - val_loss: 2.7491 - val_accuracy: 0.2022\n",
      "Epoch 82/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6210 - accuracy: 0.7868 - val_loss: 2.6845 - val_accuracy: 0.1949\n",
      "Epoch 83/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6384 - accuracy: 0.7918 - val_loss: 2.6755 - val_accuracy: 0.2108\n",
      "Epoch 84/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6025 - accuracy: 0.7951 - val_loss: 2.6469 - val_accuracy: 0.2096\n",
      "Epoch 85/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 0.5945 - accuracy: 0.7961 - val_loss: 2.6889 - val_accuracy: 0.2047\n",
      "Epoch 86/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6163 - accuracy: 0.7964 - val_loss: 2.7851 - val_accuracy: 0.1936\n",
      "Epoch 87/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6173 - accuracy: 0.7892 - val_loss: 2.6756 - val_accuracy: 0.1973\n",
      "Epoch 88/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6097 - accuracy: 0.7961 - val_loss: 2.7735 - val_accuracy: 0.1654\n",
      "Epoch 89/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.6108 - accuracy: 0.7898 - val_loss: 2.7169 - val_accuracy: 0.1961\n",
      "Epoch 90/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5888 - accuracy: 0.8031 - val_loss: 2.6926 - val_accuracy: 0.1924\n",
      "Epoch 91/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 0.6072 - accuracy: 0.8023 - val_loss: 2.6953 - val_accuracy: 0.1887\n",
      "Epoch 92/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5925 - accuracy: 0.8014 - val_loss: 2.7815 - val_accuracy: 0.1789\n",
      "Epoch 93/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5980 - accuracy: 0.8008 - val_loss: 2.7516 - val_accuracy: 0.1949\n",
      "Epoch 94/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5797 - accuracy: 0.8053 - val_loss: 2.7453 - val_accuracy: 0.1936\n",
      "Epoch 95/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 0.5671 - accuracy: 0.8084 - val_loss: 2.7482 - val_accuracy: 0.1863\n",
      "Epoch 96/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5932 - accuracy: 0.7972 - val_loss: 2.6492 - val_accuracy: 0.2206\n",
      "Epoch 97/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 0.5776 - accuracy: 0.8019 - val_loss: 2.7415 - val_accuracy: 0.1949\n",
      "Epoch 98/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5759 - accuracy: 0.8123 - val_loss: 2.6869 - val_accuracy: 0.2096\n",
      "Epoch 99/100\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 0.5635 - accuracy: 0.8128 - val_loss: 2.6847 - val_accuracy: 0.1924\n",
      "Epoch 100/100\n",
      "248/248 [==============================] - 2s 8ms/step - loss: 0.5757 - accuracy: 0.8062 - val_loss: 2.8030 - val_accuracy: 0.2010\n",
      "26/26 [==============================] - 0s 2ms/step\n",
      "\n",
      "MLP - fold 9 done\n",
      "\n",
      "Epoch 1/10\n",
      "248/248 [==============================] - 60s 240ms/step - loss: 1.9323 - acc: 0.3306\n",
      "Epoch 2/10\n",
      "248/248 [==============================] - 60s 241ms/step - loss: 1.5743 - acc: 0.4583\n",
      "Epoch 3/10\n",
      "248/248 [==============================] - 59s 240ms/step - loss: 1.4485 - acc: 0.5024\n",
      "Epoch 4/10\n",
      "248/248 [==============================] - 59s 238ms/step - loss: 1.3452 - acc: 0.5331\n",
      "Epoch 5/10\n",
      "248/248 [==============================] - 59s 238ms/step - loss: 1.2587 - acc: 0.5690\n",
      "Epoch 6/10\n",
      "248/248 [==============================] - 59s 238ms/step - loss: 1.1876 - acc: 0.5970\n",
      "Epoch 7/10\n",
      "248/248 [==============================] - 59s 237ms/step - loss: 1.1282 - acc: 0.6243\n",
      "Epoch 8/10\n",
      "248/248 [==============================] - 58s 235ms/step - loss: 1.0901 - acc: 0.6267\n",
      "Epoch 9/10\n",
      "248/248 [==============================] - 57s 231ms/step - loss: 1.0204 - acc: 0.6535\n",
      "Epoch 10/10\n",
      "248/248 [==============================] - 57s 231ms/step - loss: 1.0019 - acc: 0.6633\n",
      "26/26 [==============================] - 1s 45ms/step\n",
      "\n",
      "CNN - fold 9 done\n",
      "\n",
      "Epoch 1/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 2.5561 - accuracy: 0.1426 - val_loss: 2.1921 - val_accuracy: 0.2007\n",
      "Epoch 2/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 2.2985 - accuracy: 0.1578 - val_loss: 2.1394 - val_accuracy: 0.2115\n",
      "Epoch 3/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 2.1967 - accuracy: 0.1761 - val_loss: 2.1366 - val_accuracy: 0.2103\n",
      "Epoch 4/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 2.1543 - accuracy: 0.1949 - val_loss: 2.1298 - val_accuracy: 0.2186\n",
      "Epoch 5/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 2.1121 - accuracy: 0.2039 - val_loss: 2.1436 - val_accuracy: 0.2127\n",
      "Epoch 6/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 2.0684 - accuracy: 0.2244 - val_loss: 2.1143 - val_accuracy: 0.2270\n",
      "Epoch 7/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 2.0317 - accuracy: 0.2462 - val_loss: 2.1134 - val_accuracy: 0.2210\n",
      "Epoch 8/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.9864 - accuracy: 0.2617 - val_loss: 2.1227 - val_accuracy: 0.2139\n",
      "Epoch 9/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.9256 - accuracy: 0.2964 - val_loss: 2.1018 - val_accuracy: 0.2139\n",
      "Epoch 10/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.8525 - accuracy: 0.3268 - val_loss: 2.1468 - val_accuracy: 0.1983\n",
      "Epoch 11/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.7901 - accuracy: 0.3519 - val_loss: 2.1368 - val_accuracy: 0.1971\n",
      "Epoch 12/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.7336 - accuracy: 0.3663 - val_loss: 2.1647 - val_accuracy: 0.1768\n",
      "Epoch 13/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.6426 - accuracy: 0.4066 - val_loss: 2.1573 - val_accuracy: 0.2031\n",
      "Epoch 14/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.5895 - accuracy: 0.4365 - val_loss: 2.1788 - val_accuracy: 0.2103\n",
      "Epoch 15/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.5241 - accuracy: 0.4597 - val_loss: 2.2373 - val_accuracy: 0.1983\n",
      "Epoch 16/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.4923 - accuracy: 0.4655 - val_loss: 2.1832 - val_accuracy: 0.2162\n",
      "Epoch 17/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.4130 - accuracy: 0.4942 - val_loss: 2.2889 - val_accuracy: 0.2055\n",
      "Epoch 18/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.3598 - accuracy: 0.5177 - val_loss: 2.2581 - val_accuracy: 0.2162\n",
      "Epoch 19/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.3156 - accuracy: 0.5322 - val_loss: 2.3736 - val_accuracy: 0.2079\n",
      "Epoch 20/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.2995 - accuracy: 0.5388 - val_loss: 2.3262 - val_accuracy: 0.2127\n",
      "Epoch 21/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.2606 - accuracy: 0.5541 - val_loss: 2.4237 - val_accuracy: 0.2127\n",
      "Epoch 22/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 1.2213 - accuracy: 0.5695 - val_loss: 2.4157 - val_accuracy: 0.1971\n",
      "Epoch 23/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.1903 - accuracy: 0.5790 - val_loss: 2.3858 - val_accuracy: 0.2043\n",
      "Epoch 24/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.1423 - accuracy: 0.5975 - val_loss: 2.4380 - val_accuracy: 0.1995\n",
      "Epoch 25/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.1149 - accuracy: 0.6076 - val_loss: 2.5288 - val_accuracy: 0.2031\n",
      "Epoch 26/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.0933 - accuracy: 0.6208 - val_loss: 2.4531 - val_accuracy: 0.2139\n",
      "Epoch 27/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.0673 - accuracy: 0.6270 - val_loss: 2.5058 - val_accuracy: 0.2151\n",
      "Epoch 28/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.0422 - accuracy: 0.6286 - val_loss: 2.5556 - val_accuracy: 0.2198\n",
      "Epoch 29/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.0194 - accuracy: 0.6472 - val_loss: 2.5722 - val_accuracy: 0.2162\n",
      "Epoch 30/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.9848 - accuracy: 0.6550 - val_loss: 2.6989 - val_accuracy: 0.2162\n",
      "Epoch 31/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 1.0054 - accuracy: 0.6502 - val_loss: 2.6900 - val_accuracy: 0.2139\n",
      "Epoch 32/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.9615 - accuracy: 0.6670 - val_loss: 2.6368 - val_accuracy: 0.2198\n",
      "Epoch 33/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.9645 - accuracy: 0.6659 - val_loss: 2.6172 - val_accuracy: 0.2139\n",
      "Epoch 34/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.9441 - accuracy: 0.6694 - val_loss: 2.7464 - val_accuracy: 0.2139\n",
      "Epoch 35/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.9199 - accuracy: 0.6768 - val_loss: 2.6568 - val_accuracy: 0.2139\n",
      "Epoch 36/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.9303 - accuracy: 0.6785 - val_loss: 2.7764 - val_accuracy: 0.2198\n",
      "Epoch 37/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8885 - accuracy: 0.6920 - val_loss: 2.7269 - val_accuracy: 0.2162\n",
      "Epoch 38/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.9209 - accuracy: 0.6832 - val_loss: 2.7103 - val_accuracy: 0.2043\n",
      "Epoch 39/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8785 - accuracy: 0.6975 - val_loss: 2.6670 - val_accuracy: 0.2151\n",
      "Epoch 40/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8823 - accuracy: 0.6977 - val_loss: 2.6657 - val_accuracy: 0.2234\n",
      "Epoch 41/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8472 - accuracy: 0.7148 - val_loss: 2.6550 - val_accuracy: 0.2246\n",
      "Epoch 42/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8153 - accuracy: 0.7136 - val_loss: 2.7657 - val_accuracy: 0.2151\n",
      "Epoch 43/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8451 - accuracy: 0.7041 - val_loss: 2.5411 - val_accuracy: 0.2270\n",
      "Epoch 44/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8320 - accuracy: 0.7158 - val_loss: 2.6590 - val_accuracy: 0.2186\n",
      "Epoch 45/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8253 - accuracy: 0.7084 - val_loss: 2.7309 - val_accuracy: 0.2258\n",
      "Epoch 46/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.8184 - accuracy: 0.7158 - val_loss: 2.6792 - val_accuracy: 0.2162\n",
      "Epoch 47/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7969 - accuracy: 0.7283 - val_loss: 2.6674 - val_accuracy: 0.2246\n",
      "Epoch 48/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7934 - accuracy: 0.7316 - val_loss: 2.5758 - val_accuracy: 0.2246\n",
      "Epoch 49/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7896 - accuracy: 0.7272 - val_loss: 2.6816 - val_accuracy: 0.2270\n",
      "Epoch 50/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7772 - accuracy: 0.7343 - val_loss: 2.6286 - val_accuracy: 0.2294\n",
      "Epoch 51/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7602 - accuracy: 0.7358 - val_loss: 2.6694 - val_accuracy: 0.2330\n",
      "Epoch 52/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7630 - accuracy: 0.7350 - val_loss: 2.5284 - val_accuracy: 0.2294\n",
      "Epoch 53/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7464 - accuracy: 0.7416 - val_loss: 2.7686 - val_accuracy: 0.2103\n",
      "Epoch 54/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7415 - accuracy: 0.7478 - val_loss: 2.6875 - val_accuracy: 0.2127\n",
      "Epoch 55/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7455 - accuracy: 0.7506 - val_loss: 2.5541 - val_accuracy: 0.2139\n",
      "Epoch 56/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7393 - accuracy: 0.7459 - val_loss: 2.5692 - val_accuracy: 0.2162\n",
      "Epoch 57/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7421 - accuracy: 0.7458 - val_loss: 2.6744 - val_accuracy: 0.2103\n",
      "Epoch 58/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7156 - accuracy: 0.7526 - val_loss: 2.6879 - val_accuracy: 0.2294\n",
      "Epoch 59/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7123 - accuracy: 0.7536 - val_loss: 2.7320 - val_accuracy: 0.2270\n",
      "Epoch 60/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6985 - accuracy: 0.7574 - val_loss: 2.6870 - val_accuracy: 0.2282\n",
      "Epoch 61/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7034 - accuracy: 0.7564 - val_loss: 2.6222 - val_accuracy: 0.2222\n",
      "Epoch 62/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7038 - accuracy: 0.7581 - val_loss: 2.6208 - val_accuracy: 0.2151\n",
      "Epoch 63/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7094 - accuracy: 0.7586 - val_loss: 2.6637 - val_accuracy: 0.2127\n",
      "Epoch 64/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.7167 - accuracy: 0.7566 - val_loss: 2.5481 - val_accuracy: 0.2055\n",
      "Epoch 65/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6817 - accuracy: 0.7663 - val_loss: 2.6535 - val_accuracy: 0.2151\n",
      "Epoch 66/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6841 - accuracy: 0.7642 - val_loss: 2.6672 - val_accuracy: 0.2186\n",
      "Epoch 67/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6703 - accuracy: 0.7730 - val_loss: 2.5733 - val_accuracy: 0.2246\n",
      "Epoch 68/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6679 - accuracy: 0.7725 - val_loss: 2.7145 - val_accuracy: 0.2282\n",
      "Epoch 69/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6712 - accuracy: 0.7691 - val_loss: 2.5575 - val_accuracy: 0.2282\n",
      "Epoch 70/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6648 - accuracy: 0.7701 - val_loss: 2.5111 - val_accuracy: 0.2306\n",
      "Epoch 71/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6801 - accuracy: 0.7687 - val_loss: 2.8138 - val_accuracy: 0.2198\n",
      "Epoch 72/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6590 - accuracy: 0.7726 - val_loss: 2.5686 - val_accuracy: 0.2342\n",
      "Epoch 73/100\n",
      "247/247 [==============================] - 2s 6ms/step - loss: 0.6489 - accuracy: 0.7762 - val_loss: 2.5650 - val_accuracy: 0.2306\n",
      "Epoch 74/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6401 - accuracy: 0.7738 - val_loss: 2.5535 - val_accuracy: 0.2234\n",
      "Epoch 75/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6357 - accuracy: 0.7761 - val_loss: 2.8189 - val_accuracy: 0.2103\n",
      "Epoch 76/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6464 - accuracy: 0.7781 - val_loss: 2.6275 - val_accuracy: 0.2270\n",
      "Epoch 77/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6330 - accuracy: 0.7830 - val_loss: 2.7243 - val_accuracy: 0.2234\n",
      "Epoch 78/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6380 - accuracy: 0.7802 - val_loss: 2.6964 - val_accuracy: 0.2282\n",
      "Epoch 79/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6435 - accuracy: 0.7799 - val_loss: 2.4952 - val_accuracy: 0.2294\n",
      "Epoch 80/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6270 - accuracy: 0.7858 - val_loss: 2.6114 - val_accuracy: 0.2234\n",
      "Epoch 81/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6059 - accuracy: 0.7953 - val_loss: 2.7734 - val_accuracy: 0.2174\n",
      "Epoch 82/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6106 - accuracy: 0.7919 - val_loss: 2.5699 - val_accuracy: 0.2258\n",
      "Epoch 83/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6148 - accuracy: 0.7913 - val_loss: 2.7283 - val_accuracy: 0.2270\n",
      "Epoch 84/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5974 - accuracy: 0.7973 - val_loss: 2.6870 - val_accuracy: 0.2222\n",
      "Epoch 85/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6115 - accuracy: 0.7938 - val_loss: 2.5412 - val_accuracy: 0.2234\n",
      "Epoch 86/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5943 - accuracy: 0.7947 - val_loss: 2.6720 - val_accuracy: 0.2246\n",
      "Epoch 87/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6254 - accuracy: 0.7852 - val_loss: 2.6050 - val_accuracy: 0.2258\n",
      "Epoch 88/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6051 - accuracy: 0.7934 - val_loss: 2.5349 - val_accuracy: 0.2258\n",
      "Epoch 89/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5864 - accuracy: 0.8004 - val_loss: 2.4629 - val_accuracy: 0.2306\n",
      "Epoch 90/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5655 - accuracy: 0.8077 - val_loss: 2.5672 - val_accuracy: 0.2378\n",
      "Epoch 91/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5993 - accuracy: 0.7984 - val_loss: 2.4485 - val_accuracy: 0.2318\n",
      "Epoch 92/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.6005 - accuracy: 0.7937 - val_loss: 2.4475 - val_accuracy: 0.2306\n",
      "Epoch 93/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5750 - accuracy: 0.7984 - val_loss: 2.5105 - val_accuracy: 0.2270\n",
      "Epoch 94/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5859 - accuracy: 0.8053 - val_loss: 2.6488 - val_accuracy: 0.2294\n",
      "Epoch 95/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5841 - accuracy: 0.7963 - val_loss: 2.6929 - val_accuracy: 0.2270\n",
      "Epoch 96/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5967 - accuracy: 0.7963 - val_loss: 2.4768 - val_accuracy: 0.2306\n",
      "Epoch 97/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5657 - accuracy: 0.8061 - val_loss: 2.6377 - val_accuracy: 0.2258\n",
      "Epoch 98/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5822 - accuracy: 0.8046 - val_loss: 2.6239 - val_accuracy: 0.2258\n",
      "Epoch 99/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5821 - accuracy: 0.7986 - val_loss: 2.7527 - val_accuracy: 0.2198\n",
      "Epoch 100/100\n",
      "247/247 [==============================] - 2s 7ms/step - loss: 0.5788 - accuracy: 0.8024 - val_loss: 2.7583 - val_accuracy: 0.2210\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "\n",
      "MLP - fold 10 done\n",
      "\n",
      "Epoch 1/10\n",
      "247/247 [==============================] - 76s 303ms/step - loss: 1.9288 - acc: 0.3341\n",
      "Epoch 2/10\n",
      "247/247 [==============================] - 75s 305ms/step - loss: 1.5624 - acc: 0.4647\n",
      "Epoch 3/10\n",
      "247/247 [==============================] - 75s 304ms/step - loss: 1.4085 - acc: 0.5230\n",
      "Epoch 4/10\n",
      "247/247 [==============================] - 78s 316ms/step - loss: 1.3008 - acc: 0.5595\n",
      "Epoch 5/10\n",
      "247/247 [==============================] - 84s 339ms/step - loss: 1.2164 - acc: 0.5843\n",
      "Epoch 6/10\n",
      "247/247 [==============================] - 82s 331ms/step - loss: 1.1687 - acc: 0.6098\n",
      "Epoch 7/10\n",
      "247/247 [==============================] - 80s 325ms/step - loss: 1.1030 - acc: 0.6272\n",
      "Epoch 8/10\n",
      "247/247 [==============================] - 82s 332ms/step - loss: 1.0588 - acc: 0.6403\n",
      "Epoch 9/10\n",
      "247/247 [==============================] - 81s 328ms/step - loss: 1.0041 - acc: 0.6614\n",
      "Epoch 10/10\n",
      "247/247 [==============================] - 84s 341ms/step - loss: 0.9867 - acc: 0.6721\n",
      "27/27 [==============================] - 2s 57ms/step\n",
      "\n",
      "CNN - fold 10 done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp_accuracies = []\n",
    "cnn_accuracies = []\n",
    "mlp_conf_matrices = []\n",
    "cnn_conf_matrices = []\n",
    "for i in range(10):\n",
    "    # getting training and test sets containing the audio signals\n",
    "    train_df = audio_df.drop(audio_df[audio_df['fold'] == i+1].index)\n",
    "    test_df = audio_df.drop(audio_df[audio_df['fold'] != i+1].index)\n",
    "\n",
    "    X_train = np.array(train_df['audio'].tolist())\n",
    "    X_test = np.array(test_df['audio'].tolist())\n",
    "    y_train = np.array(train_df['label'].tolist())\n",
    "    y_test = np.array(test_df['label'].tolist())\n",
    "    \n",
    "    # reloading the MLP\n",
    "    mlp = get_mlp()\n",
    "    \n",
    "    # training the MLP\n",
    "    n_epochs_mlp = 100\n",
    "    n_batch_size_mlp = 32\n",
    "    mlp.fit(X_train, y_train, batch_size=n_batch_size_mlp, epochs=n_epochs_mlp, validation_data=(X_test, y_test))\n",
    "    y_pred = mlp.predict(X_test,n_batch_size_mlp)\n",
    "    \n",
    "    # performance metrics for MLP\n",
    "    accuracy = mlp.evaluate(X_test,y_test,verbose=0)[1]\n",
    "    mlp_accuracies.append(accuracy)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    mlp_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    mlp_conf_matrices.append(mlp_conf_matrix)\n",
    "    \n",
    "    print(f\"\\nMLP - fold {i+1} done\\n\")\n",
    "    \n",
    "    # getting training and test sets containing the MFCCs\n",
    "    train_df = mfcc_df.drop(mfcc_df[mfcc_df['fold'] == i+1].index)\n",
    "    test_df = mfcc_df.drop(mfcc_df[mfcc_df['fold'] != i+1].index)\n",
    "    \n",
    "    X_train = np.array(train_df['mfcc'].tolist())\n",
    "    X_test = np.array(test_df['mfcc'].tolist())\n",
    "    y_train = np.array(train_df['label'].tolist())\n",
    "    y_test = np.array(test_df['label'].tolist())\n",
    "    \n",
    "    # reloading the CNN\n",
    "    cnn = get_cnn()\n",
    "    \n",
    "    # training the CNN\n",
    "    n_epochs_cnn = 10\n",
    "    n_batch_size_cnn = 32\n",
    "    cnn.fit(X_train,y_train,epochs=n_epochs_cnn,batch_size=n_batch_size_cnn)\n",
    "    y_pred = cnn.predict(X_test,n_batch_size_cnn)\n",
    "    \n",
    "    # performance metrics for CNN\n",
    "    accuracy = cnn.evaluate(X_test,y_test,verbose=0)[1]\n",
    "    cnn_accuracies.append(accuracy)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    cnn_conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cnn_conf_matrices.append(cnn_conf_matrix)\n",
    "    \n",
    "    print(f\"\\nCNN - fold {i+1} done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "Average accuracy: 0.17689255774021148\n",
      "Accuracy's standard deviation: 0.02186023844372127\n",
      "\n",
      "CNN\n",
      "Average accuracy: 0.4413173794746399\n",
      "Accuracy's standard deviation: 0.050849187797625144\n"
     ]
    }
   ],
   "source": [
    "mlp_avg_accuracy = np.mean(mlp_accuracies)\n",
    "mlp_std_dev_accuracy = np.std(mlp_accuracies)\n",
    "mlp_avg_conf_matrix = np.mean(mlp_conf_matrices,axis=0)\n",
    "\n",
    "cnn_avg_accuracy = np.mean(cnn_accuracies)\n",
    "cnn_std_dev_accuracy = np.std(cnn_accuracies)\n",
    "cnn_avg_conf_matrix = np.mean(cnn_conf_matrices,axis=0)\n",
    "\n",
    "print(f'''MLP\n",
    "Average accuracy: {mlp_avg_accuracy}\n",
    "Accuracy's standard deviation: {mlp_std_dev_accuracy}\n",
    "\n",
    "CNN\n",
    "Average accuracy: {cnn_avg_accuracy}\n",
    "Accuracy's standard deviation: {cnn_std_dev_accuracy}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing accuracy per fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(mlp_accuracies, labels=[1,2,3,4,5,6,7,8,9,10], vert=True, patch_artist=True)\n",
    "plt.title('Box Plot of Accuracy Across Folds - MLP')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convlutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(mlp_accuracies, labels=[1,2,3,4,5,6,7,8,9,10], vert=True, patch_artist=True)\n",
    "plt.title('Box Plot of Accuracy Across Folds - CNN')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing confusion matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Label')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(mlp_avg_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "classes = labels\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convlutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cnn_avg_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "classes = labels\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UrbanSound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
