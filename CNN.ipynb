{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Input, InputLayer, Dropout, BatchNormalization, Convolution2D, MaxPooling2D, GlobalMaxPool2D\n",
    "from keras import activations, models, optimizers, losses\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, multilabel_confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.500000</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>99812-1-2-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>159.522205</td>\n",
       "      <td>163.522205</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>99812-1-3-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>181.142431</td>\n",
       "      <td>183.284976</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>99812-1-4-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>242.691902</td>\n",
       "      <td>246.197885</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>99812-1-5-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>253.209850</td>\n",
       "      <td>255.741948</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>99812-1-6-0.wav</td>\n",
       "      <td>99812</td>\n",
       "      <td>332.289233</td>\n",
       "      <td>334.821332</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>car_horn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8732 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         slice_file_name    fsID       start         end  salience  fold  \\\n",
       "0       100032-3-0-0.wav  100032    0.000000    0.317551         1     5   \n",
       "1     100263-2-0-117.wav  100263   58.500000   62.500000         1     5   \n",
       "2     100263-2-0-121.wav  100263   60.500000   64.500000         1     5   \n",
       "3     100263-2-0-126.wav  100263   63.000000   67.000000         1     5   \n",
       "4     100263-2-0-137.wav  100263   68.500000   72.500000         1     5   \n",
       "...                  ...     ...         ...         ...       ...   ...   \n",
       "8727     99812-1-2-0.wav   99812  159.522205  163.522205         2     7   \n",
       "8728     99812-1-3-0.wav   99812  181.142431  183.284976         2     7   \n",
       "8729     99812-1-4-0.wav   99812  242.691902  246.197885         2     7   \n",
       "8730     99812-1-5-0.wav   99812  253.209850  255.741948         2     7   \n",
       "8731     99812-1-6-0.wav   99812  332.289233  334.821332         2     7   \n",
       "\n",
       "      classID             class  \n",
       "0           3          dog_bark  \n",
       "1           2  children_playing  \n",
       "2           2  children_playing  \n",
       "3           2  children_playing  \n",
       "4           2  children_playing  \n",
       "...       ...               ...  \n",
       "8727        1          car_horn  \n",
       "8728        1          car_horn  \n",
       "8729        1          car_horn  \n",
       "8730        1          car_horn  \n",
       "8731        1          car_horn  \n",
       "\n",
       "[8732 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv('../UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata\n",
    "# (x_train, y_train) = ((Xaudios, 4000), (Xaudios,))\n",
    "# (x_test, y_test) = ((Yaudios, 4000), (Yaudios,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog_bark' 'children_playing' 'car_horn' 'air_conditioner' 'street_music'\n",
      " 'gun_shot' 'siren' 'engine_idling' 'jackhammer' 'drilling']\n"
     ]
    }
   ],
   "source": [
    "labels = metadata['class'].unique()    # obtaining the class labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label_id):\n",
    "    onehot = [0]*10   # length of labels array\n",
    "    onehot[label_id-1]=1\n",
    "    return onehot\n",
    "\n",
    "def reflective_padding(signal, target_duration, target_rate):\n",
    "    target_duration = target_duration*target_rate\n",
    "    current_duration = len(signal)\n",
    "    \n",
    "    # Calculate the required padding on each side\n",
    "    padding_needed = target_duration - current_duration\n",
    "    left_padding = padding_needed // 2\n",
    "    right_padding = padding_needed - left_padding\n",
    "    \n",
    "    # Reflective padding on both sides\n",
    "    padded_signal = np.pad(signal, (left_padding, right_padding), 'reflect')\n",
    "    \n",
    "    return padded_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_time = 4    # each signal will have 4 seconds of duration\n",
    "target_rate = 1000    # resampling frequence\n",
    "\n",
    "# MFCC parameters\n",
    "n_mfcc=40\n",
    "hop_length=round(target_rate*0.0125)\n",
    "win_length=round(target_rate*0.023)\n",
    "n_fft=2**14\n",
    "mfcc_time_size = 4*target_rate//hop_length+1\n",
    "\n",
    "dataset = []        # [audio, label, fold]\n",
    "dataset_mfcc = []   # [MFCCs, label, fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=16384 is too large for input signal of length=4000\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 done\n",
      "Index 1 done\n",
      "Index 2 done\n",
      "Index 3 done\n",
      "Index 4 done\n",
      "Index 5 done\n",
      "Index 6 done\n",
      "Index 7 done\n",
      "Index 8 done\n",
      "Index 9 done\n",
      "Index 10 done\n",
      "Index 11 done\n",
      "Index 12 done\n",
      "Index 13 done\n",
      "Index 14 done\n",
      "Index 15 done\n",
      "Index 16 done\n",
      "Index 17 done\n",
      "Index 18 done\n",
      "Index 19 done\n",
      "Index 20 done\n",
      "Index 21 done\n",
      "Index 22 done\n",
      "Index 23 done\n",
      "Index 24 done\n",
      "Index 25 done\n",
      "Index 26 done\n",
      "Index 27 done\n",
      "Index 28 done\n",
      "Index 29 done\n",
      "Index 30 done\n",
      "Index 31 done\n",
      "Index 32 done\n",
      "Index 33 done\n",
      "Index 34 done\n",
      "Index 35 done\n",
      "Index 36 done\n",
      "Index 37 done\n",
      "Index 38 done\n",
      "Index 39 done\n",
      "Index 40 done\n",
      "Index 41 done\n",
      "Index 42 done\n",
      "Index 43 done\n",
      "Index 44 done\n",
      "Index 45 done\n",
      "Index 46 done\n",
      "Index 47 done\n",
      "Index 48 done\n",
      "Index 49 done\n",
      "Index 50 done\n",
      "Index 51 done\n",
      "Index 52 done\n",
      "Index 53 done\n",
      "Index 54 done\n",
      "Index 55 done\n",
      "Index 56 done\n",
      "Index 57 done\n",
      "Index 58 done\n",
      "Index 59 done\n",
      "Index 60 done\n",
      "Index 61 done\n",
      "Index 62 done\n",
      "Index 63 done\n",
      "Index 64 done\n",
      "Index 65 done\n",
      "Index 66 done\n",
      "Index 67 done\n",
      "Index 68 done\n",
      "Index 69 done\n",
      "Index 70 done\n",
      "Index 71 done\n",
      "Index 72 done\n",
      "Index 73 done\n",
      "Index 74 done\n",
      "Index 75 done\n",
      "Index 76 done\n",
      "Index 77 done\n",
      "Index 78 done\n",
      "Index 79 done\n",
      "Index 80 done\n",
      "Index 81 done\n",
      "Index 82 done\n",
      "Index 83 done\n",
      "Index 84 done\n",
      "Index 85 done\n",
      "Index 86 done\n",
      "Index 87 done\n",
      "Index 88 done\n",
      "Index 89 done\n",
      "Index 90 done\n",
      "Index 91 done\n",
      "Index 92 done\n",
      "Index 93 done\n",
      "Index 94 done\n",
      "Index 95 done\n",
      "Index 96 done\n",
      "Index 97 done\n",
      "Index 98 done\n",
      "Index 99 done\n",
      "Index 100 done\n",
      "Index 101 done\n",
      "Index 102 done\n",
      "Index 103 done\n",
      "Index 104 done\n",
      "Index 105 done\n",
      "Index 106 done\n",
      "Index 107 done\n",
      "Index 108 done\n",
      "Index 109 done\n",
      "Index 110 done\n",
      "Index 111 done\n",
      "Index 112 done\n",
      "Index 113 done\n",
      "Index 114 done\n",
      "Index 115 done\n",
      "Index 116 done\n",
      "Index 117 done\n",
      "Index 118 done\n",
      "Index 119 done\n",
      "Index 120 done\n",
      "Index 121 done\n",
      "Index 122 done\n",
      "Index 123 done\n",
      "Index 124 done\n",
      "Index 125 done\n",
      "Index 126 done\n",
      "Index 127 done\n",
      "Index 128 done\n",
      "Index 129 done\n",
      "Index 130 done\n",
      "Index 131 done\n",
      "Index 132 done\n",
      "Index 133 done\n",
      "Index 134 done\n",
      "Index 135 done\n",
      "Index 136 done\n",
      "Index 137 done\n",
      "Index 138 done\n",
      "Index 139 done\n",
      "Index 140 done\n",
      "Index 141 done\n",
      "Index 142 done\n",
      "Index 143 done\n",
      "Index 144 done\n",
      "Index 145 done\n",
      "Index 146 done\n",
      "Index 147 done\n",
      "Index 148 done\n",
      "Index 149 done\n",
      "Index 150 done\n",
      "Index 151 done\n",
      "Index 152 done\n",
      "Index 153 done\n",
      "Index 154 done\n",
      "Index 155 done\n",
      "Index 156 done\n",
      "Index 157 done\n",
      "Index 158 done\n",
      "Index 159 done\n",
      "Index 160 done\n",
      "Index 161 done\n",
      "Index 162 done\n",
      "Index 163 done\n",
      "Index 164 done\n",
      "Index 165 done\n",
      "Index 166 done\n",
      "Index 167 done\n",
      "Index 168 done\n",
      "Index 169 done\n",
      "Index 170 done\n",
      "Index 171 done\n",
      "Index 172 done\n",
      "Index 173 done\n",
      "Index 174 done\n",
      "Index 175 done\n",
      "Index 176 done\n",
      "Index 177 done\n",
      "Index 178 done\n",
      "Index 179 done\n",
      "Index 180 done\n",
      "Index 181 done\n",
      "Index 182 done\n",
      "Index 183 done\n",
      "Index 184 done\n",
      "Index 185 done\n",
      "Index 186 done\n",
      "Index 187 done\n",
      "Index 188 done\n",
      "Index 189 done\n",
      "Index 190 done\n",
      "Index 191 done\n",
      "Index 192 done\n",
      "Index 193 done\n",
      "Index 194 done\n",
      "Index 195 done\n",
      "Index 196 done\n",
      "Index 197 done\n",
      "Index 198 done\n",
      "Index 199 done\n",
      "Index 200 done\n",
      "Index 201 done\n",
      "Index 202 done\n",
      "Index 203 done\n",
      "Index 204 done\n",
      "Index 205 done\n",
      "Index 206 done\n",
      "Index 207 done\n",
      "Index 208 done\n",
      "Index 209 done\n",
      "Index 210 done\n",
      "Index 211 done\n",
      "Index 212 done\n",
      "Index 213 done\n",
      "Index 214 done\n",
      "Index 215 done\n",
      "Index 216 done\n",
      "Index 217 done\n",
      "Index 218 done\n",
      "Index 219 done\n",
      "Index 220 done\n",
      "Index 221 done\n",
      "Index 222 done\n",
      "Index 223 done\n",
      "Index 224 done\n",
      "Index 225 done\n",
      "Index 226 done\n",
      "Index 227 done\n",
      "Index 228 done\n",
      "Index 229 done\n",
      "Index 230 done\n",
      "Index 231 done\n",
      "Index 232 done\n",
      "Index 233 done\n",
      "Index 234 done\n",
      "Index 235 done\n",
      "Index 236 done\n",
      "Index 237 done\n",
      "Index 238 done\n",
      "Index 239 done\n",
      "Index 240 done\n",
      "Index 241 done\n",
      "Index 242 done\n",
      "Index 243 done\n",
      "Index 244 done\n",
      "Index 245 done\n",
      "Index 246 done\n",
      "Index 247 done\n",
      "Index 248 done\n",
      "Index 249 done\n",
      "Index 250 done\n",
      "Index 251 done\n",
      "Index 252 done\n",
      "Index 253 done\n",
      "Index 254 done\n",
      "Index 255 done\n",
      "Index 256 done\n",
      "Index 257 done\n",
      "Index 258 done\n",
      "Index 259 done\n",
      "Index 260 done\n",
      "Index 261 done\n",
      "Index 262 done\n",
      "Index 263 done\n",
      "Index 264 done\n",
      "Index 265 done\n",
      "Index 266 done\n",
      "Index 267 done\n",
      "Index 268 done\n",
      "Index 269 done\n",
      "Index 270 done\n",
      "Index 271 done\n",
      "Index 272 done\n",
      "Index 273 done\n",
      "Index 274 done\n",
      "Index 275 done\n",
      "Index 276 done\n",
      "Index 277 done\n",
      "Index 278 done\n",
      "Index 279 done\n",
      "Index 280 done\n",
      "Index 281 done\n",
      "Index 282 done\n",
      "Index 283 done\n",
      "Index 284 done\n",
      "Index 285 done\n",
      "Index 286 done\n",
      "Index 287 done\n",
      "Index 288 done\n",
      "Index 289 done\n",
      "Index 290 done\n",
      "Index 291 done\n",
      "Index 292 done\n",
      "Index 293 done\n",
      "Index 294 done\n",
      "Index 295 done\n",
      "Index 296 done\n",
      "Index 297 done\n",
      "Index 298 done\n",
      "Index 299 done\n",
      "Index 300 done\n",
      "Index 301 done\n",
      "Index 302 done\n",
      "Index 303 done\n",
      "Index 304 done\n",
      "Index 305 done\n",
      "Index 306 done\n",
      "Index 307 done\n",
      "Index 308 done\n",
      "Index 309 done\n",
      "Index 310 done\n",
      "Index 311 done\n",
      "Index 312 done\n",
      "Index 313 done\n",
      "Index 314 done\n",
      "Index 315 done\n",
      "Index 316 done\n",
      "Index 317 done\n",
      "Index 318 done\n",
      "Index 319 done\n",
      "Index 320 done\n",
      "Index 321 done\n",
      "Index 322 done\n",
      "Index 323 done\n",
      "Index 324 done\n",
      "Index 325 done\n",
      "Index 326 done\n",
      "Index 327 done\n",
      "Index 328 done\n",
      "Index 329 done\n",
      "Index 330 done\n",
      "Index 331 done\n",
      "Index 332 done\n",
      "Index 333 done\n",
      "Index 334 done\n",
      "Index 335 done\n",
      "Index 336 done\n",
      "Index 337 done\n",
      "Index 338 done\n",
      "Index 339 done\n",
      "Index 340 done\n",
      "Index 341 done\n",
      "Index 342 done\n",
      "Index 343 done\n",
      "Index 344 done\n",
      "Index 345 done\n",
      "Index 346 done\n",
      "Index 347 done\n",
      "Index 348 done\n",
      "Index 349 done\n",
      "Index 350 done\n",
      "Index 351 done\n",
      "Index 352 done\n",
      "Index 353 done\n",
      "Index 354 done\n",
      "Index 355 done\n",
      "Index 356 done\n",
      "Index 357 done\n",
      "Index 358 done\n",
      "Index 359 done\n",
      "Index 360 done\n",
      "Index 361 done\n",
      "Index 362 done\n",
      "Index 363 done\n",
      "Index 364 done\n",
      "Index 365 done\n",
      "Index 366 done\n",
      "Index 367 done\n",
      "Index 368 done\n",
      "Index 369 done\n",
      "Index 370 done\n",
      "Index 371 done\n",
      "Index 372 done\n",
      "Index 373 done\n",
      "Index 374 done\n",
      "Index 375 done\n",
      "Index 376 done\n",
      "Index 377 done\n",
      "Index 378 done\n",
      "Index 379 done\n",
      "Index 380 done\n",
      "Index 381 done\n",
      "Index 382 done\n",
      "Index 383 done\n",
      "Index 384 done\n",
      "Index 385 done\n",
      "Index 386 done\n",
      "Index 387 done\n",
      "Index 388 done\n",
      "Index 389 done\n",
      "Index 390 done\n",
      "Index 391 done\n",
      "Index 392 done\n",
      "Index 393 done\n",
      "Index 394 done\n",
      "Index 395 done\n",
      "Index 396 done\n",
      "Index 397 done\n",
      "Index 398 done\n",
      "Index 399 done\n",
      "Index 400 done\n",
      "Index 401 done\n",
      "Index 402 done\n",
      "Index 403 done\n",
      "Index 404 done\n",
      "Index 405 done\n",
      "Index 406 done\n",
      "Index 407 done\n",
      "Index 408 done\n",
      "Index 409 done\n",
      "Index 410 done\n",
      "Index 411 done\n",
      "Index 412 done\n",
      "Index 413 done\n",
      "Index 414 done\n",
      "Index 415 done\n",
      "Index 416 done\n",
      "Index 417 done\n",
      "Index 418 done\n",
      "Index 419 done\n",
      "Index 420 done\n",
      "Index 421 done\n",
      "Index 422 done\n",
      "Index 423 done\n",
      "Index 424 done\n",
      "Index 425 done\n",
      "Index 426 done\n",
      "Index 427 done\n",
      "Index 428 done\n",
      "Index 429 done\n",
      "Index 430 done\n",
      "Index 431 done\n",
      "Index 432 done\n",
      "Index 433 done\n",
      "Index 434 done\n",
      "Index 435 done\n",
      "Index 436 done\n",
      "Index 437 done\n",
      "Index 438 done\n",
      "Index 439 done\n",
      "Index 440 done\n",
      "Index 441 done\n",
      "Index 442 done\n",
      "Index 443 done\n",
      "Index 444 done\n",
      "Index 445 done\n",
      "Index 446 done\n",
      "Index 447 done\n",
      "Index 448 done\n",
      "Index 449 done\n",
      "Index 450 done\n",
      "Index 451 done\n",
      "Index 452 done\n",
      "Index 453 done\n",
      "Index 454 done\n",
      "Index 455 done\n",
      "Index 456 done\n",
      "Index 457 done\n",
      "Index 458 done\n",
      "Index 459 done\n",
      "Index 460 done\n",
      "Index 461 done\n",
      "Index 462 done\n",
      "Index 463 done\n",
      "Index 464 done\n",
      "Index 465 done\n",
      "Index 466 done\n",
      "Index 467 done\n",
      "Index 468 done\n",
      "Index 469 done\n",
      "Index 470 done\n",
      "Index 471 done\n",
      "Index 472 done\n",
      "Index 473 done\n",
      "Index 474 done\n",
      "Index 475 done\n",
      "Index 476 done\n",
      "Index 477 done\n",
      "Index 478 done\n",
      "Index 479 done\n",
      "Index 480 done\n",
      "Index 481 done\n",
      "Index 482 done\n",
      "Index 483 done\n",
      "Index 484 done\n",
      "Index 485 done\n",
      "Index 486 done\n",
      "Index 487 done\n",
      "Index 488 done\n",
      "Index 489 done\n",
      "Index 490 done\n",
      "Index 491 done\n",
      "Index 492 done\n",
      "Index 493 done\n",
      "Index 494 done\n",
      "Index 495 done\n",
      "Index 496 done\n",
      "Index 497 done\n",
      "Index 498 done\n",
      "Index 499 done\n",
      "Index 500 done\n",
      "Index 501 done\n",
      "Index 502 done\n",
      "Index 503 done\n",
      "Index 504 done\n",
      "Index 505 done\n",
      "Index 506 done\n",
      "Index 507 done\n",
      "Index 508 done\n",
      "Index 509 done\n",
      "Index 510 done\n",
      "Index 511 done\n",
      "Index 512 done\n",
      "Index 513 done\n",
      "Index 514 done\n",
      "Index 515 done\n",
      "Index 516 done\n",
      "Index 517 done\n",
      "Index 518 done\n",
      "Index 519 done\n",
      "Index 520 done\n",
      "Index 521 done\n",
      "Index 522 done\n",
      "Index 523 done\n",
      "Index 524 done\n",
      "Index 525 done\n",
      "Index 526 done\n",
      "Index 527 done\n",
      "Index 528 done\n",
      "Index 529 done\n",
      "Index 530 done\n",
      "Index 531 done\n",
      "Index 532 done\n",
      "Index 533 done\n",
      "Index 534 done\n",
      "Index 535 done\n",
      "Index 536 done\n",
      "Index 537 done\n",
      "Index 538 done\n",
      "Index 539 done\n",
      "Index 540 done\n",
      "Index 541 done\n",
      "Index 542 done\n",
      "Index 543 done\n",
      "Index 544 done\n",
      "Index 545 done\n",
      "Index 546 done\n",
      "Index 547 done\n",
      "Index 548 done\n",
      "Index 549 done\n",
      "Index 550 done\n",
      "Index 551 done\n",
      "Index 552 done\n",
      "Index 553 done\n",
      "Index 554 done\n",
      "Index 555 done\n",
      "Index 556 done\n",
      "Index 557 done\n",
      "Index 558 done\n",
      "Index 559 done\n",
      "Index 560 done\n",
      "Index 561 done\n",
      "Index 562 done\n",
      "Index 563 done\n",
      "Index 564 done\n",
      "Index 565 done\n",
      "Index 566 done\n",
      "Index 567 done\n",
      "Index 568 done\n",
      "Index 569 done\n",
      "Index 570 done\n",
      "Index 571 done\n",
      "Index 572 done\n",
      "Index 573 done\n",
      "Index 574 done\n",
      "Index 575 done\n",
      "Index 576 done\n",
      "Index 577 done\n",
      "Index 578 done\n",
      "Index 579 done\n",
      "Index 580 done\n",
      "Index 581 done\n",
      "Index 582 done\n",
      "Index 583 done\n",
      "Index 584 done\n",
      "Index 585 done\n",
      "Index 586 done\n",
      "Index 587 done\n",
      "Index 588 done\n",
      "Index 589 done\n",
      "Index 590 done\n",
      "Index 591 done\n",
      "Index 592 done\n",
      "Index 593 done\n",
      "Index 594 done\n",
      "Index 595 done\n",
      "Index 596 done\n",
      "Index 597 done\n",
      "Index 598 done\n",
      "Index 599 done\n",
      "Index 600 done\n",
      "Index 601 done\n",
      "Index 602 done\n",
      "Index 603 done\n",
      "Index 604 done\n",
      "Index 605 done\n",
      "Index 606 done\n",
      "Index 607 done\n",
      "Index 608 done\n",
      "Index 609 done\n",
      "Index 610 done\n",
      "Index 611 done\n",
      "Index 612 done\n",
      "Index 613 done\n",
      "Index 614 done\n",
      "Index 615 done\n",
      "Index 616 done\n",
      "Index 617 done\n",
      "Index 618 done\n",
      "Index 619 done\n",
      "Index 620 done\n",
      "Index 621 done\n",
      "Index 622 done\n",
      "Index 623 done\n",
      "Index 624 done\n",
      "Index 625 done\n",
      "Index 626 done\n",
      "Index 627 done\n",
      "Index 628 done\n",
      "Index 629 done\n",
      "Index 630 done\n",
      "Index 631 done\n",
      "Index 632 done\n",
      "Index 633 done\n",
      "Index 634 done\n",
      "Index 635 done\n",
      "Index 636 done\n",
      "Index 637 done\n",
      "Index 638 done\n",
      "Index 639 done\n",
      "Index 640 done\n",
      "Index 641 done\n",
      "Index 642 done\n",
      "Index 643 done\n",
      "Index 644 done\n",
      "Index 645 done\n",
      "Index 646 done\n",
      "Index 647 done\n",
      "Index 648 done\n",
      "Index 649 done\n",
      "Index 650 done\n",
      "Index 651 done\n",
      "Index 652 done\n",
      "Index 653 done\n",
      "Index 654 done\n",
      "Index 655 done\n",
      "Index 656 done\n",
      "Index 657 done\n",
      "Index 658 done\n",
      "Index 659 done\n",
      "Index 660 done\n",
      "Index 661 done\n",
      "Index 662 done\n",
      "Index 663 done\n",
      "Index 664 done\n",
      "Index 665 done\n",
      "Index 666 done\n",
      "Index 667 done\n",
      "Index 668 done\n",
      "Index 669 done\n",
      "Index 670 done\n",
      "Index 671 done\n",
      "Index 672 done\n",
      "Index 673 done\n",
      "Index 674 done\n",
      "Index 675 done\n",
      "Index 676 done\n",
      "Index 677 done\n",
      "Index 678 done\n",
      "Index 679 done\n",
      "Index 680 done\n",
      "Index 681 done\n",
      "Index 682 done\n",
      "Index 683 done\n",
      "Index 684 done\n",
      "Index 685 done\n",
      "Index 686 done\n",
      "Index 687 done\n",
      "Index 688 done\n",
      "Index 689 done\n",
      "Index 690 done\n",
      "Index 691 done\n",
      "Index 692 done\n",
      "Index 693 done\n",
      "Index 694 done\n",
      "Index 695 done\n",
      "Index 696 done\n",
      "Index 697 done\n",
      "Index 698 done\n",
      "Index 699 done\n",
      "Index 700 done\n",
      "Index 701 done\n",
      "Index 702 done\n",
      "Index 703 done\n",
      "Index 704 done\n",
      "Index 705 done\n",
      "Index 706 done\n",
      "Index 707 done\n",
      "Index 708 done\n",
      "Index 709 done\n",
      "Index 710 done\n",
      "Index 711 done\n",
      "Index 712 done\n",
      "Index 713 done\n",
      "Index 714 done\n",
      "Index 715 done\n",
      "Index 716 done\n",
      "Index 717 done\n",
      "Index 718 done\n",
      "Index 719 done\n",
      "Index 720 done\n",
      "Index 721 done\n",
      "Index 722 done\n",
      "Index 723 done\n",
      "Index 724 done\n",
      "Index 725 done\n",
      "Index 726 done\n",
      "Index 727 done\n",
      "Index 728 done\n",
      "Index 729 done\n",
      "Index 730 done\n",
      "Index 731 done\n",
      "Index 732 done\n",
      "Index 733 done\n",
      "Index 734 done\n",
      "Index 735 done\n",
      "Index 736 done\n",
      "Index 737 done\n",
      "Index 738 done\n",
      "Index 739 done\n",
      "Index 740 done\n",
      "Index 741 done\n",
      "Index 742 done\n",
      "Index 743 done\n",
      "Index 744 done\n",
      "Index 745 done\n",
      "Index 746 done\n",
      "Index 747 done\n",
      "Index 748 done\n",
      "Index 749 done\n",
      "Index 750 done\n",
      "Index 751 done\n",
      "Index 752 done\n",
      "Index 753 done\n",
      "Index 754 done\n",
      "Index 755 done\n",
      "Index 756 done\n",
      "Index 757 done\n",
      "Index 758 done\n",
      "Index 759 done\n",
      "Index 760 done\n",
      "Index 761 done\n",
      "Index 762 done\n",
      "Index 763 done\n",
      "Index 764 done\n",
      "Index 765 done\n",
      "Index 766 done\n",
      "Index 767 done\n",
      "Index 768 done\n",
      "Index 769 done\n",
      "Index 770 done\n",
      "Index 771 done\n",
      "Index 772 done\n",
      "Index 773 done\n",
      "Index 774 done\n",
      "Index 775 done\n",
      "Index 776 done\n",
      "Index 777 done\n",
      "Index 778 done\n",
      "Index 779 done\n",
      "Index 780 done\n",
      "Index 781 done\n",
      "Index 782 done\n",
      "Index 783 done\n"
     ]
    }
   ],
   "source": [
    "for index, row in metadata.iterrows():\n",
    "    #for fold\n",
    "    fold = row[\"fold\"]\n",
    "\n",
    "    # for audio\n",
    "    signal, rate = librosa.load(f\"../UrbanSound8K/audio/fold{fold}/\"+row[\"slice_file_name\"], sr=None)\n",
    "    new_signal = librosa.resample(signal, orig_sr=rate, target_sr=target_rate)\n",
    "    if len(new_signal) < 4*target_rate:\n",
    "        new_signal = reflective_padding(new_signal, 4, target_rate)\n",
    "    audio = new_signal[:4000]\n",
    "    \n",
    "    # MFCCs\n",
    "    sig_mfcc = librosa.feature.mfcc(y=new_signal,sr=target_rate,n_fft=n_fft,hop_length=hop_length,win_length=win_length,n_mfcc=n_mfcc)\n",
    "    sig_mfcc = sig_mfcc[:,:334]\n",
    "\n",
    "    #for label\n",
    "    label = one_hot_encode(row[\"classID\"])\n",
    "\n",
    "    dataset.append([audio, label, fold])\n",
    "    dataset_mfcc.append([sig_mfcc, label, fold])\n",
    "    print(f\"Index {index} done\")\n",
    "\n",
    "audio_df = pd.DataFrame(dataset, columns=[\"audio\",\"label\",\"fold\"])\n",
    "mfcc_df = pd.DataFrame(dataset_mfcc, columns=[\"mfcc\",\"label\",\"fold\"])\n",
    "print(audio_df.head())\n",
    "print(mfcc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/audio_df.pkl\", 'wb') as f:\n",
    "    pickle.dump(audio_df, f)\n",
    "\n",
    "\n",
    "with open(\"datasets/mfcc_df.pkl\", 'wb') as f:\n",
    "    pickle.dump(mfcc_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/audio_df.pkl\", 'rb') as f:\n",
    "    audio_df = pickle.load(f)\n",
    "    \n",
    "with open(\"datasets/mfcc_df.pkl\", 'rb') as f:\n",
    "    mfcc_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0051111234, 0.00027401396, 0.0015376861, 7...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0010924106, 0.0020332793, 0.0022091647, 0.0...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.0011693948, 0.0005625988, -0.00020165322, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.0010711739, -0.004255988, -0.002138806, -0...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0018479167, 0.005483534, 0.0029024398, -0.0...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8727</th>\n",
       "      <td>[-0.0019209236, -0.0020613694, -0.00032925355,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8728</th>\n",
       "      <td>[-0.001814239, 9.417883e-06, 0.0049034064, -0....</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729</th>\n",
       "      <td>[0.011096266, -0.0060437755, -0.009053946, -0....</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>[-0.00019298907, 0.0027881288, -0.0014333489, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8731</th>\n",
       "      <td>[0.0008923183, 0.0017864045, 0.0011411511, 0.0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8732 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  audio  \\\n",
       "0     [-0.0051111234, 0.00027401396, 0.0015376861, 7...   \n",
       "1     [0.0010924106, 0.0020332793, 0.0022091647, 0.0...   \n",
       "2     [-0.0011693948, 0.0005625988, -0.00020165322, ...   \n",
       "3     [-0.0010711739, -0.004255988, -0.002138806, -0...   \n",
       "4     [0.0018479167, 0.005483534, 0.0029024398, -0.0...   \n",
       "...                                                 ...   \n",
       "8727  [-0.0019209236, -0.0020613694, -0.00032925355,...   \n",
       "8728  [-0.001814239, 9.417883e-06, 0.0049034064, -0....   \n",
       "8729  [0.011096266, -0.0060437755, -0.009053946, -0....   \n",
       "8730  [-0.00019298907, 0.0027881288, -0.0014333489, ...   \n",
       "8731  [0.0008923183, 0.0017864045, 0.0011411511, 0.0...   \n",
       "\n",
       "                               label  fold  \n",
       "0     [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]     5  \n",
       "1     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]     5  \n",
       "2     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]     5  \n",
       "3     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]     5  \n",
       "4     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]     5  \n",
       "...                              ...   ...  \n",
       "8727  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]     7  \n",
       "8728  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]     7  \n",
       "8729  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]     7  \n",
       "8730  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]     7  \n",
       "8731  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]     7  \n",
       "\n",
       "[8732 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/mfcc_df.pkl\", 'rb') as f:\n",
    "    mfcc_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCCs matrix shape for a sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40, 334)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"MFCCs matrix shape for a sample\")\n",
    "mfcc_df['mfcc'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 200)               800200    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 200)              800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 904,110\n",
      "Trainable params: 902,910\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(200, activation='relu',input_shape=(4000, ))) # input layer  #4000 = sample rate 1000 * 4sec audio\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(200,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(200,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(BatchNormalization())\n",
    "mlp.add(Dense(100,activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(10, activation='softmax')) # output layer  #10 = n_class\n",
    "\n",
    "mlp.compile(loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            optimizer='adam')\n",
    "            \n",
    "\n",
    "# summary\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 157, 320, 1)]     0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 157, 320, 1)      4         \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 155, 314, 16)      352       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 153, 308, 16)      5392      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 51, 44, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 51, 44, 16)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 49, 42, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 47, 40, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 13, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15, 13, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 11, 128)       36992     \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 128)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 91,966\n",
      "Trainable params: 91,452\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nclass = 10\n",
    "inp = Input(shape=(157, 320, 1))        # MFCCs\n",
    "norm_inp = BatchNormalization()(inp)\n",
    "audio = Convolution2D(16, kernel_size=(3, 7), activation=activations.relu)(norm_inp)\n",
    "audio = Convolution2D(16, kernel_size=(3, 7), activation=activations.relu)(audio)\n",
    "audio = MaxPooling2D(pool_size=(3, 7))(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "audio = Convolution2D(32, kernel_size=3, activation=activations.relu)(audio)\n",
    "audio = Convolution2D(32, kernel_size=3, activation=activations.relu)(audio)\n",
    "audio = MaxPooling2D(pool_size=(3, 3))(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "audio = Convolution2D(128, kernel_size=3, activation=activations.relu)(audio)\n",
    "audio = GlobalMaxPool2D()(audio)\n",
    "audio = Dropout(rate=0.1)(audio)\n",
    "\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(audio))\n",
    "dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "cnn = models.Model(inputs=inp, outputs=dense_1)\n",
    "opt = optimizers.Adam()\n",
    "\n",
    "cnn.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4475 - accuracy: 0.8521 - val_loss: 1.3495 - val_accuracy: 0.5459\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4422 - accuracy: 0.8559 - val_loss: 1.2969 - val_accuracy: 0.5319\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4520 - accuracy: 0.8472 - val_loss: 1.2632 - val_accuracy: 0.5503\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4532 - accuracy: 0.8526 - val_loss: 1.2936 - val_accuracy: 0.5611\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 3s 13ms/step - loss: 0.4424 - accuracy: 0.8559 - val_loss: 1.2834 - val_accuracy: 0.5643\n",
      "Epoch 6/10\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4513 - accuracy: 0.8485 - val_loss: 1.5081 - val_accuracy: 0.5243\n",
      "Epoch 7/10\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4598 - accuracy: 0.8530 - val_loss: 1.3988 - val_accuracy: 0.5481\n",
      "Epoch 8/10\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4383 - accuracy: 0.8554 - val_loss: 1.2856 - val_accuracy: 0.5719\n",
      "Epoch 9/10\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4359 - accuracy: 0.8564 - val_loss: 1.5099 - val_accuracy: 0.5038\n",
      "Epoch 10/10\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4483 - accuracy: 0.8519 - val_loss: 1.5876 - val_accuracy: 0.5103\n",
      "Epoch 1/20\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4519 - accuracy: 0.8524 - val_loss: 1.4472 - val_accuracy: 0.5189\n",
      "Epoch 2/20\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4368 - accuracy: 0.8572 - val_loss: 1.5036 - val_accuracy: 0.5168\n",
      "Epoch 3/20\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4410 - accuracy: 0.8595 - val_loss: 1.3727 - val_accuracy: 0.5459\n",
      "Epoch 4/20\n",
      "244/244 [==============================] - 4s 15ms/step - loss: 0.4541 - accuracy: 0.8532 - val_loss: 1.4000 - val_accuracy: 0.5589\n",
      "Epoch 5/20\n",
      "244/244 [==============================] - 3s 10ms/step - loss: 0.4436 - accuracy: 0.8622 - val_loss: 1.4411 - val_accuracy: 0.5470\n",
      "Epoch 6/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4439 - accuracy: 0.8559 - val_loss: 1.4272 - val_accuracy: 0.5276\n",
      "Epoch 7/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4512 - accuracy: 0.8496 - val_loss: 1.5455 - val_accuracy: 0.5059\n",
      "Epoch 8/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4492 - accuracy: 0.8514 - val_loss: 1.7750 - val_accuracy: 0.4768\n",
      "Epoch 9/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4500 - accuracy: 0.8524 - val_loss: 1.9435 - val_accuracy: 0.4541\n",
      "Epoch 10/20\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4420 - accuracy: 0.8542 - val_loss: 1.6273 - val_accuracy: 0.5189\n",
      "Epoch 11/20\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4293 - accuracy: 0.8568 - val_loss: 1.6183 - val_accuracy: 0.4854\n",
      "Epoch 12/20\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4353 - accuracy: 0.8582 - val_loss: 1.4955 - val_accuracy: 0.5427\n",
      "Epoch 13/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4110 - accuracy: 0.8632 - val_loss: 1.4494 - val_accuracy: 0.5449\n",
      "Epoch 14/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4414 - accuracy: 0.8578 - val_loss: 1.4439 - val_accuracy: 0.5341\n",
      "Epoch 15/20\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4298 - accuracy: 0.8590 - val_loss: 1.6952 - val_accuracy: 0.4984\n",
      "Epoch 16/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4264 - accuracy: 0.8612 - val_loss: 1.7090 - val_accuracy: 0.5059\n",
      "Epoch 17/20\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4400 - accuracy: 0.8559 - val_loss: 1.5288 - val_accuracy: 0.5016\n",
      "Epoch 18/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4516 - accuracy: 0.8562 - val_loss: 1.3618 - val_accuracy: 0.5449\n",
      "Epoch 19/20\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4497 - accuracy: 0.8542 - val_loss: 1.4488 - val_accuracy: 0.5524\n",
      "Epoch 20/20\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4448 - accuracy: 0.8531 - val_loss: 1.6164 - val_accuracy: 0.4984\n",
      "Epoch 1/30\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4324 - accuracy: 0.8562 - val_loss: 1.5576 - val_accuracy: 0.5178\n",
      "Epoch 2/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4587 - accuracy: 0.8490 - val_loss: 1.7416 - val_accuracy: 0.4951\n",
      "Epoch 3/30\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4399 - accuracy: 0.8563 - val_loss: 1.5283 - val_accuracy: 0.5200\n",
      "Epoch 4/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4290 - accuracy: 0.8585 - val_loss: 1.7756 - val_accuracy: 0.4595\n",
      "Epoch 5/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4389 - accuracy: 0.8569 - val_loss: 1.5836 - val_accuracy: 0.4886\n",
      "Epoch 6/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4295 - accuracy: 0.8600 - val_loss: 1.5406 - val_accuracy: 0.5092\n",
      "Epoch 7/30\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4463 - accuracy: 0.8578 - val_loss: 1.6452 - val_accuracy: 0.4746\n",
      "Epoch 8/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4354 - accuracy: 0.8567 - val_loss: 1.8109 - val_accuracy: 0.4497\n",
      "Epoch 9/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4163 - accuracy: 0.8642 - val_loss: 1.6467 - val_accuracy: 0.4919\n",
      "Epoch 10/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4407 - accuracy: 0.8559 - val_loss: 1.5648 - val_accuracy: 0.5211\n",
      "Epoch 11/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4495 - accuracy: 0.8540 - val_loss: 1.5480 - val_accuracy: 0.5114\n",
      "Epoch 12/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4403 - accuracy: 0.8568 - val_loss: 1.6467 - val_accuracy: 0.5059\n",
      "Epoch 13/30\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4278 - accuracy: 0.8562 - val_loss: 1.7734 - val_accuracy: 0.4530\n",
      "Epoch 14/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4342 - accuracy: 0.8587 - val_loss: 1.4388 - val_accuracy: 0.5124\n",
      "Epoch 15/30\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4403 - accuracy: 0.8556 - val_loss: 1.6524 - val_accuracy: 0.4627\n",
      "Epoch 16/30\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4512 - accuracy: 0.8550 - val_loss: 1.6975 - val_accuracy: 0.4562\n",
      "Epoch 17/30\n",
      "244/244 [==============================] - 4s 16ms/step - loss: 0.4608 - accuracy: 0.8532 - val_loss: 1.3417 - val_accuracy: 0.5492\n",
      "Epoch 18/30\n",
      "244/244 [==============================] - 3s 14ms/step - loss: 0.4482 - accuracy: 0.8563 - val_loss: 1.3888 - val_accuracy: 0.5319\n",
      "Epoch 19/30\n",
      "244/244 [==============================] - 3s 13ms/step - loss: 0.4443 - accuracy: 0.8546 - val_loss: 1.3715 - val_accuracy: 0.5589\n",
      "Epoch 20/30\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4503 - accuracy: 0.8524 - val_loss: 1.4642 - val_accuracy: 0.5168\n",
      "Epoch 21/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4143 - accuracy: 0.8632 - val_loss: 1.8933 - val_accuracy: 0.4378\n",
      "Epoch 22/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4532 - accuracy: 0.8523 - val_loss: 1.4742 - val_accuracy: 0.5049\n",
      "Epoch 23/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4230 - accuracy: 0.8617 - val_loss: 1.4647 - val_accuracy: 0.5449\n",
      "Epoch 24/30\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4352 - accuracy: 0.8588 - val_loss: 1.3831 - val_accuracy: 0.5222\n",
      "Epoch 25/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4326 - accuracy: 0.8615 - val_loss: 1.5486 - val_accuracy: 0.4973\n",
      "Epoch 26/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4397 - accuracy: 0.8609 - val_loss: 1.7627 - val_accuracy: 0.4551\n",
      "Epoch 27/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4259 - accuracy: 0.8546 - val_loss: 1.6437 - val_accuracy: 0.4865\n",
      "Epoch 28/30\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4169 - accuracy: 0.8652 - val_loss: 1.5596 - val_accuracy: 0.4854\n",
      "Epoch 29/30\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4212 - accuracy: 0.8618 - val_loss: 1.4412 - val_accuracy: 0.5178\n",
      "Epoch 30/30\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4295 - accuracy: 0.8595 - val_loss: 1.5611 - val_accuracy: 0.5103\n",
      "Epoch 1/50\n",
      "244/244 [==============================] - 3s 9ms/step - loss: 0.4290 - accuracy: 0.8620 - val_loss: 1.5908 - val_accuracy: 0.4768\n",
      "Epoch 2/50\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4310 - accuracy: 0.8637 - val_loss: 1.5289 - val_accuracy: 0.5146\n",
      "Epoch 3/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4288 - accuracy: 0.8661 - val_loss: 2.0620 - val_accuracy: 0.4119\n",
      "Epoch 4/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4339 - accuracy: 0.8595 - val_loss: 1.7604 - val_accuracy: 0.4130\n",
      "Epoch 5/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4209 - accuracy: 0.8614 - val_loss: 1.6618 - val_accuracy: 0.4389\n",
      "Epoch 6/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4421 - accuracy: 0.8563 - val_loss: 1.6561 - val_accuracy: 0.4854\n",
      "Epoch 7/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4312 - accuracy: 0.8642 - val_loss: 1.5469 - val_accuracy: 0.5005\n",
      "Epoch 8/50\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4324 - accuracy: 0.8597 - val_loss: 1.5103 - val_accuracy: 0.5222\n",
      "Epoch 9/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4336 - accuracy: 0.8614 - val_loss: 1.4812 - val_accuracy: 0.5005\n",
      "Epoch 10/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4379 - accuracy: 0.8583 - val_loss: 1.4860 - val_accuracy: 0.5049\n",
      "Epoch 11/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4429 - accuracy: 0.8579 - val_loss: 1.6173 - val_accuracy: 0.4789\n",
      "Epoch 12/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4369 - accuracy: 0.8617 - val_loss: 1.8757 - val_accuracy: 0.4562\n",
      "Epoch 13/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4249 - accuracy: 0.8604 - val_loss: 1.5882 - val_accuracy: 0.4897\n",
      "Epoch 14/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4175 - accuracy: 0.8627 - val_loss: 1.5246 - val_accuracy: 0.5254\n",
      "Epoch 15/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4186 - accuracy: 0.8600 - val_loss: 1.5698 - val_accuracy: 0.5027\n",
      "Epoch 16/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4349 - accuracy: 0.8627 - val_loss: 1.4791 - val_accuracy: 0.4854\n",
      "Epoch 17/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4184 - accuracy: 0.8656 - val_loss: 1.5593 - val_accuracy: 0.4951\n",
      "Epoch 18/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4227 - accuracy: 0.8652 - val_loss: 1.5706 - val_accuracy: 0.5049\n",
      "Epoch 19/50\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4292 - accuracy: 0.8608 - val_loss: 1.4786 - val_accuracy: 0.5092\n",
      "Epoch 20/50\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4079 - accuracy: 0.8641 - val_loss: 1.6424 - val_accuracy: 0.4627\n",
      "Epoch 21/50\n",
      "244/244 [==============================] - 3s 10ms/step - loss: 0.4124 - accuracy: 0.8696 - val_loss: 1.7055 - val_accuracy: 0.5070\n",
      "Epoch 22/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4147 - accuracy: 0.8647 - val_loss: 1.5477 - val_accuracy: 0.4659\n",
      "Epoch 23/50\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4341 - accuracy: 0.8600 - val_loss: 1.6300 - val_accuracy: 0.4865\n",
      "Epoch 24/50\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4508 - accuracy: 0.8590 - val_loss: 1.8471 - val_accuracy: 0.3989\n",
      "Epoch 25/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4172 - accuracy: 0.8651 - val_loss: 1.5969 - val_accuracy: 0.4735\n",
      "Epoch 26/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3972 - accuracy: 0.8704 - val_loss: 1.7887 - val_accuracy: 0.4573\n",
      "Epoch 27/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4287 - accuracy: 0.8647 - val_loss: 1.4456 - val_accuracy: 0.5319\n",
      "Epoch 28/50\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4294 - accuracy: 0.8559 - val_loss: 1.5521 - val_accuracy: 0.4703\n",
      "Epoch 29/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4199 - accuracy: 0.8617 - val_loss: 1.6314 - val_accuracy: 0.4476\n",
      "Epoch 30/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4115 - accuracy: 0.8665 - val_loss: 1.8521 - val_accuracy: 0.4476\n",
      "Epoch 31/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4137 - accuracy: 0.8649 - val_loss: 1.6803 - val_accuracy: 0.4714\n",
      "Epoch 32/50\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4086 - accuracy: 0.8697 - val_loss: 1.5811 - val_accuracy: 0.4995\n",
      "Epoch 33/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4192 - accuracy: 0.8627 - val_loss: 1.6901 - val_accuracy: 0.5092\n",
      "Epoch 34/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4288 - accuracy: 0.8594 - val_loss: 1.5360 - val_accuracy: 0.5027\n",
      "Epoch 35/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4184 - accuracy: 0.8649 - val_loss: 1.8016 - val_accuracy: 0.4811\n",
      "Epoch 36/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4240 - accuracy: 0.8628 - val_loss: 1.7699 - val_accuracy: 0.4400\n",
      "Epoch 37/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4083 - accuracy: 0.8665 - val_loss: 1.7692 - val_accuracy: 0.4357\n",
      "Epoch 38/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4007 - accuracy: 0.8683 - val_loss: 1.9106 - val_accuracy: 0.4584\n",
      "Epoch 39/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4313 - accuracy: 0.8642 - val_loss: 1.6519 - val_accuracy: 0.4714\n",
      "Epoch 40/50\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4252 - accuracy: 0.8695 - val_loss: 1.7774 - val_accuracy: 0.4432\n",
      "Epoch 41/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4071 - accuracy: 0.8656 - val_loss: 1.7312 - val_accuracy: 0.4605\n",
      "Epoch 42/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4087 - accuracy: 0.8676 - val_loss: 1.7289 - val_accuracy: 0.4724\n",
      "Epoch 43/50\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4115 - accuracy: 0.8658 - val_loss: 1.8877 - val_accuracy: 0.4346\n",
      "Epoch 44/50\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4228 - accuracy: 0.8637 - val_loss: 1.8210 - val_accuracy: 0.4476\n",
      "Epoch 45/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4081 - accuracy: 0.8702 - val_loss: 1.7872 - val_accuracy: 0.4573\n",
      "Epoch 46/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4064 - accuracy: 0.8664 - val_loss: 1.6773 - val_accuracy: 0.4897\n",
      "Epoch 47/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4066 - accuracy: 0.8678 - val_loss: 1.7314 - val_accuracy: 0.4605\n",
      "Epoch 48/50\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4294 - accuracy: 0.8638 - val_loss: 1.7012 - val_accuracy: 0.4627\n",
      "Epoch 49/50\n",
      "244/244 [==============================] - 3s 10ms/step - loss: 0.4125 - accuracy: 0.8661 - val_loss: 1.7168 - val_accuracy: 0.4454\n",
      "Epoch 50/50\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4058 - accuracy: 0.8661 - val_loss: 1.7142 - val_accuracy: 0.4541\n",
      "Epoch 1/100\n",
      "244/244 [==============================] - 3s 10ms/step - loss: 0.4106 - accuracy: 0.8691 - val_loss: 1.6762 - val_accuracy: 0.4497\n",
      "Epoch 2/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4179 - accuracy: 0.8641 - val_loss: 1.9075 - val_accuracy: 0.4324\n",
      "Epoch 3/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4057 - accuracy: 0.8705 - val_loss: 1.8529 - val_accuracy: 0.4389\n",
      "Epoch 4/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4164 - accuracy: 0.8674 - val_loss: 1.9021 - val_accuracy: 0.4443\n",
      "Epoch 5/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4222 - accuracy: 0.8618 - val_loss: 1.8572 - val_accuracy: 0.4746\n",
      "Epoch 6/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4113 - accuracy: 0.8661 - val_loss: 1.7559 - val_accuracy: 0.4595\n",
      "Epoch 7/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4180 - accuracy: 0.8692 - val_loss: 1.7880 - val_accuracy: 0.4281\n",
      "Epoch 8/100\n",
      "244/244 [==============================] - 3s 10ms/step - loss: 0.4064 - accuracy: 0.8674 - val_loss: 1.8330 - val_accuracy: 0.4432\n",
      "Epoch 9/100\n",
      "244/244 [==============================] - 3s 13ms/step - loss: 0.4252 - accuracy: 0.8613 - val_loss: 1.8364 - val_accuracy: 0.4703\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.3966 - accuracy: 0.8733 - val_loss: 1.5687 - val_accuracy: 0.4822\n",
      "Epoch 11/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4425 - accuracy: 0.8569 - val_loss: 1.7198 - val_accuracy: 0.4422\n",
      "Epoch 12/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4157 - accuracy: 0.8654 - val_loss: 1.7573 - val_accuracy: 0.4584\n",
      "Epoch 13/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4154 - accuracy: 0.8667 - val_loss: 1.8465 - val_accuracy: 0.4432\n",
      "Epoch 14/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3865 - accuracy: 0.8764 - val_loss: 1.9771 - val_accuracy: 0.4249\n",
      "Epoch 15/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4119 - accuracy: 0.8674 - val_loss: 1.7057 - val_accuracy: 0.4616\n",
      "Epoch 16/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4139 - accuracy: 0.8672 - val_loss: 1.7644 - val_accuracy: 0.4378\n",
      "Epoch 17/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4102 - accuracy: 0.8692 - val_loss: 1.7062 - val_accuracy: 0.4627\n",
      "Epoch 18/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4041 - accuracy: 0.8651 - val_loss: 1.7836 - val_accuracy: 0.4670\n",
      "Epoch 19/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4071 - accuracy: 0.8674 - val_loss: 1.9248 - val_accuracy: 0.4281\n",
      "Epoch 20/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4158 - accuracy: 0.8673 - val_loss: 1.7581 - val_accuracy: 0.4443\n",
      "Epoch 21/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3986 - accuracy: 0.8690 - val_loss: 1.9267 - val_accuracy: 0.4086\n",
      "Epoch 22/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4181 - accuracy: 0.8652 - val_loss: 1.7817 - val_accuracy: 0.4368\n",
      "Epoch 23/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4143 - accuracy: 0.8706 - val_loss: 1.6332 - val_accuracy: 0.4649\n",
      "Epoch 24/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4034 - accuracy: 0.8710 - val_loss: 1.7470 - val_accuracy: 0.4627\n",
      "Epoch 25/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4071 - accuracy: 0.8686 - val_loss: 1.6338 - val_accuracy: 0.4724\n",
      "Epoch 26/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4154 - accuracy: 0.8658 - val_loss: 1.6947 - val_accuracy: 0.4616\n",
      "Epoch 27/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4122 - accuracy: 0.8663 - val_loss: 1.6673 - val_accuracy: 0.4454\n",
      "Epoch 28/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4176 - accuracy: 0.8655 - val_loss: 1.6136 - val_accuracy: 0.4530\n",
      "Epoch 29/100\n",
      "244/244 [==============================] - 3s 13ms/step - loss: 0.3957 - accuracy: 0.8699 - val_loss: 1.9913 - val_accuracy: 0.4357\n",
      "Epoch 30/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4083 - accuracy: 0.8668 - val_loss: 1.7223 - val_accuracy: 0.4562\n",
      "Epoch 31/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3973 - accuracy: 0.8758 - val_loss: 1.8688 - val_accuracy: 0.4497\n",
      "Epoch 32/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3900 - accuracy: 0.8749 - val_loss: 1.7988 - val_accuracy: 0.4530\n",
      "Epoch 33/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4002 - accuracy: 0.8729 - val_loss: 2.2086 - val_accuracy: 0.3935\n",
      "Epoch 34/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4054 - accuracy: 0.8641 - val_loss: 1.7271 - val_accuracy: 0.4519\n",
      "Epoch 35/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4095 - accuracy: 0.8650 - val_loss: 1.7615 - val_accuracy: 0.4270\n",
      "Epoch 36/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3785 - accuracy: 0.8750 - val_loss: 1.8664 - val_accuracy: 0.4541\n",
      "Epoch 37/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3913 - accuracy: 0.8765 - val_loss: 1.7962 - val_accuracy: 0.4378\n",
      "Epoch 38/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3996 - accuracy: 0.8688 - val_loss: 1.7192 - val_accuracy: 0.4086\n",
      "Epoch 39/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4101 - accuracy: 0.8676 - val_loss: 1.5798 - val_accuracy: 0.4919\n",
      "Epoch 40/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3868 - accuracy: 0.8743 - val_loss: 1.6490 - val_accuracy: 0.4886\n",
      "Epoch 41/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4033 - accuracy: 0.8709 - val_loss: 1.7938 - val_accuracy: 0.4465\n",
      "Epoch 42/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3898 - accuracy: 0.8706 - val_loss: 1.7418 - val_accuracy: 0.4724\n",
      "Epoch 43/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4100 - accuracy: 0.8714 - val_loss: 1.7448 - val_accuracy: 0.4324\n",
      "Epoch 44/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4085 - accuracy: 0.8700 - val_loss: 1.6244 - val_accuracy: 0.4876\n",
      "Epoch 45/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4056 - accuracy: 0.8705 - val_loss: 1.9285 - val_accuracy: 0.4314\n",
      "Epoch 46/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4032 - accuracy: 0.8737 - val_loss: 1.8410 - val_accuracy: 0.4335\n",
      "Epoch 47/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3919 - accuracy: 0.8700 - val_loss: 1.7290 - val_accuracy: 0.4411\n",
      "Epoch 48/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.4056 - accuracy: 0.8681 - val_loss: 1.5483 - val_accuracy: 0.4551\n",
      "Epoch 49/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.4001 - accuracy: 0.8717 - val_loss: 1.7405 - val_accuracy: 0.4465\n",
      "Epoch 50/100\n",
      "244/244 [==============================] - 3s 10ms/step - loss: 0.3928 - accuracy: 0.8696 - val_loss: 1.6123 - val_accuracy: 0.4865\n",
      "Epoch 51/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.3960 - accuracy: 0.8692 - val_loss: 2.0056 - val_accuracy: 0.3914\n",
      "Epoch 52/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3723 - accuracy: 0.8799 - val_loss: 1.7712 - val_accuracy: 0.4530\n",
      "Epoch 53/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4081 - accuracy: 0.8686 - val_loss: 1.6709 - val_accuracy: 0.4443\n",
      "Epoch 54/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.3859 - accuracy: 0.8738 - val_loss: 1.8400 - val_accuracy: 0.3784\n",
      "Epoch 55/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3906 - accuracy: 0.8734 - val_loss: 1.6369 - val_accuracy: 0.4746\n",
      "Epoch 56/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3726 - accuracy: 0.8799 - val_loss: 1.7322 - val_accuracy: 0.4595\n",
      "Epoch 57/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4059 - accuracy: 0.8728 - val_loss: 1.7174 - val_accuracy: 0.4573\n",
      "Epoch 58/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3899 - accuracy: 0.8800 - val_loss: 1.8052 - val_accuracy: 0.4346\n",
      "Epoch 59/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3967 - accuracy: 0.8718 - val_loss: 1.8623 - val_accuracy: 0.3924\n",
      "Epoch 60/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3831 - accuracy: 0.8724 - val_loss: 2.0689 - val_accuracy: 0.4292\n",
      "Epoch 61/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.3833 - accuracy: 0.8769 - val_loss: 2.2217 - val_accuracy: 0.4032\n",
      "Epoch 62/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3946 - accuracy: 0.8740 - val_loss: 1.8055 - val_accuracy: 0.4346\n",
      "Epoch 63/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4043 - accuracy: 0.8719 - val_loss: 1.7883 - val_accuracy: 0.4541\n",
      "Epoch 64/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4053 - accuracy: 0.8691 - val_loss: 1.9611 - val_accuracy: 0.4184\n",
      "Epoch 65/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3869 - accuracy: 0.8749 - val_loss: 1.8397 - val_accuracy: 0.4324\n",
      "Epoch 66/100\n",
      "244/244 [==============================] - 2s 7ms/step - loss: 0.4147 - accuracy: 0.8701 - val_loss: 2.0935 - val_accuracy: 0.4119\n",
      "Epoch 67/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3978 - accuracy: 0.8737 - val_loss: 1.6463 - val_accuracy: 0.4195\n",
      "Epoch 68/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3937 - accuracy: 0.8747 - val_loss: 1.7623 - val_accuracy: 0.4476\n",
      "Epoch 69/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.3940 - accuracy: 0.8705 - val_loss: 1.8219 - val_accuracy: 0.4324\n",
      "Epoch 70/100\n",
      "244/244 [==============================] - 2s 9ms/step - loss: 0.3816 - accuracy: 0.8786 - val_loss: 2.0274 - val_accuracy: 0.4086\n",
      "Epoch 71/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.3937 - accuracy: 0.8774 - val_loss: 1.9297 - val_accuracy: 0.4011\n",
      "Epoch 72/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4029 - accuracy: 0.8708 - val_loss: 1.8074 - val_accuracy: 0.4389\n",
      "Epoch 73/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4078 - accuracy: 0.8741 - val_loss: 1.7911 - val_accuracy: 0.4746\n",
      "Epoch 74/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3864 - accuracy: 0.8732 - val_loss: 2.1058 - val_accuracy: 0.4184\n",
      "Epoch 75/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3748 - accuracy: 0.8782 - val_loss: 1.8564 - val_accuracy: 0.4076\n",
      "Epoch 76/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3729 - accuracy: 0.8754 - val_loss: 1.7333 - val_accuracy: 0.4400\n",
      "Epoch 77/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3957 - accuracy: 0.8722 - val_loss: 1.7691 - val_accuracy: 0.4562\n",
      "Epoch 78/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.3926 - accuracy: 0.8751 - val_loss: 1.6784 - val_accuracy: 0.4432\n",
      "Epoch 79/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.3958 - accuracy: 0.8763 - val_loss: 1.7680 - val_accuracy: 0.4497\n",
      "Epoch 80/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3886 - accuracy: 0.8782 - val_loss: 1.7030 - val_accuracy: 0.4530\n",
      "Epoch 81/100\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3824 - accuracy: 0.8781 - val_loss: 2.2068 - val_accuracy: 0.3946\n",
      "Epoch 82/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.3940 - accuracy: 0.8728 - val_loss: 2.1082 - val_accuracy: 0.3665\n",
      "Epoch 83/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4054 - accuracy: 0.8702 - val_loss: 1.7892 - val_accuracy: 0.4486\n",
      "Epoch 84/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3933 - accuracy: 0.8727 - val_loss: 2.0566 - val_accuracy: 0.4249\n",
      "Epoch 85/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3780 - accuracy: 0.8784 - val_loss: 1.6409 - val_accuracy: 0.4551\n",
      "Epoch 86/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3871 - accuracy: 0.8738 - val_loss: 2.1457 - val_accuracy: 0.3438\n",
      "Epoch 87/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4081 - accuracy: 0.8704 - val_loss: 1.8747 - val_accuracy: 0.4497\n",
      "Epoch 88/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4016 - accuracy: 0.8727 - val_loss: 1.6995 - val_accuracy: 0.4541\n",
      "Epoch 89/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4009 - accuracy: 0.8709 - val_loss: 1.6695 - val_accuracy: 0.4454\n",
      "Epoch 90/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3878 - accuracy: 0.8727 - val_loss: 1.9979 - val_accuracy: 0.3968\n",
      "Epoch 91/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3932 - accuracy: 0.8705 - val_loss: 1.7179 - val_accuracy: 0.4508\n",
      "Epoch 92/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3808 - accuracy: 0.8786 - val_loss: 1.9516 - val_accuracy: 0.4032\n",
      "Epoch 93/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3789 - accuracy: 0.8786 - val_loss: 2.0207 - val_accuracy: 0.4205\n",
      "Epoch 94/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3885 - accuracy: 0.8760 - val_loss: 2.1991 - val_accuracy: 0.3697\n",
      "Epoch 95/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3644 - accuracy: 0.8856 - val_loss: 1.8653 - val_accuracy: 0.4054\n",
      "Epoch 96/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3874 - accuracy: 0.8758 - val_loss: 1.9139 - val_accuracy: 0.4086\n",
      "Epoch 97/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3771 - accuracy: 0.8786 - val_loss: 1.8314 - val_accuracy: 0.4270\n",
      "Epoch 98/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3978 - accuracy: 0.8737 - val_loss: 1.9356 - val_accuracy: 0.4270\n",
      "Epoch 99/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.3878 - accuracy: 0.8764 - val_loss: 2.0342 - val_accuracy: 0.4065\n",
      "Epoch 100/100\n",
      "244/244 [==============================] - 2s 8ms/step - loss: 0.4013 - accuracy: 0.8755 - val_loss: 1.7760 - val_accuracy: 0.4389\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gapmd\\workspace\\GitHub projects\\AC II - project\\AC II - project (git)\\CNN.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X53sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     validation_accuracies\u001b[39m.\u001b[39mappend(val_accuracy)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X53sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m validation_accuracies \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(validation_accuracies)    \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m best_n_epochs \u001b[39m=\u001b[39m n_epochs(np\u001b[39m.\u001b[39;49mwhere(validation_accuracies \u001b[39m==\u001b[39;49m \u001b[39mmax\u001b[39;49m(validation_accuracies)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(validation_accuracies)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest n_epochs: \u001b[39m\u001b[39m{\u001b[39;00mbest_n_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "i=1\n",
    "test_df = set_df.drop(set_df[set_df['fold'] != i+1].index)\n",
    "val_df = set_df.drop(set_df[set_df['fold'] != i+2].index)    \n",
    "train_df = set_df.drop(set_df[set_df['fold'] == i+1].index)\n",
    "train_df = set_df.drop(set_df[set_df['fold'] == i+2].index)\n",
    "\n",
    "X_train = np.array(train_df['audio'].tolist())\n",
    "X_val = np.array(val_df['audio'].tolist())\n",
    "X_test = np.array(test_df['audio'].tolist())\n",
    "y_train = np.array(train_df['label'].tolist())\n",
    "y_val = np.array(val_df['label'].tolist())\n",
    "y_test = np.array(test_df['label'].tolist())\n",
    "\n",
    "validation_accuracies = []\n",
    "\n",
    "n_epochs = [10,20,30,50,100]\n",
    "# n_epochs = [100,150,200]\n",
    "n_batch_size = 32\n",
    "for j in range(5):\n",
    "    tf.keras.backend.clear_session()\n",
    "    mlp.fit(X_train, y_train, batch_size=n_batch_size, epochs=n_epochs[j], validation_data=(X_val, y_val))\n",
    "    val_accuracy = mlp.evaluate(X_val,y_val,verbose=0)[1]   # getting accuracy\n",
    "    validation_accuracies.append(val_accuracy)\n",
    "\n",
    "validation_accuracies = np.array(validation_accuracies)    \n",
    "best_n_epochs = n_epochs[np.where(validation_accuracies == max(validation_accuracies))]\n",
    "print(validation_accuracies)\n",
    "print(f\"Best n_epochs: {best_n_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3660 - accuracy: 0.8845 - val_loss: 0.7727 - val_accuracy: 0.7549\n",
      "Epoch 2/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3723 - accuracy: 0.8781 - val_loss: 0.6971 - val_accuracy: 0.8076\n",
      "Epoch 3/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3729 - accuracy: 0.8831 - val_loss: 0.6277 - val_accuracy: 0.8064\n",
      "Epoch 4/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3617 - accuracy: 0.8841 - val_loss: 0.6744 - val_accuracy: 0.8076\n",
      "Epoch 5/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3644 - accuracy: 0.8829 - val_loss: 0.8366 - val_accuracy: 0.7331\n",
      "Epoch 6/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3653 - accuracy: 0.8831 - val_loss: 0.6520 - val_accuracy: 0.8087\n",
      "Epoch 7/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3786 - accuracy: 0.8818 - val_loss: 0.8879 - val_accuracy: 0.7503\n",
      "Epoch 8/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3744 - accuracy: 0.8809 - val_loss: 1.0900 - val_accuracy: 0.6919\n",
      "Epoch 9/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3655 - accuracy: 0.8857 - val_loss: 0.8079 - val_accuracy: 0.7663\n",
      "Epoch 10/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3639 - accuracy: 0.8874 - val_loss: 0.8774 - val_accuracy: 0.7148\n",
      "Epoch 11/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3629 - accuracy: 0.8897 - val_loss: 0.7582 - val_accuracy: 0.7778\n",
      "Epoch 12/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3685 - accuracy: 0.8824 - val_loss: 0.7950 - val_accuracy: 0.7789\n",
      "Epoch 13/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3823 - accuracy: 0.8808 - val_loss: 0.7034 - val_accuracy: 0.8076\n",
      "Epoch 14/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3590 - accuracy: 0.8820 - val_loss: 1.0983 - val_accuracy: 0.7022\n",
      "Epoch 15/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3622 - accuracy: 0.8847 - val_loss: 0.5821 - val_accuracy: 0.7881\n",
      "Epoch 16/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3864 - accuracy: 0.8814 - val_loss: 0.6516 - val_accuracy: 0.7915\n",
      "Epoch 17/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3670 - accuracy: 0.8813 - val_loss: 0.6425 - val_accuracy: 0.8144\n",
      "Epoch 18/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3734 - accuracy: 0.8822 - val_loss: 0.6707 - val_accuracy: 0.8076\n",
      "Epoch 19/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3752 - accuracy: 0.8798 - val_loss: 0.9036 - val_accuracy: 0.7331\n",
      "Epoch 20/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3612 - accuracy: 0.8866 - val_loss: 0.8462 - val_accuracy: 0.7595\n",
      "Epoch 21/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3642 - accuracy: 0.8824 - val_loss: 1.1639 - val_accuracy: 0.7136\n",
      "Epoch 22/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3876 - accuracy: 0.8778 - val_loss: 1.0597 - val_accuracy: 0.7022\n",
      "Epoch 23/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3989 - accuracy: 0.8754 - val_loss: 0.9912 - val_accuracy: 0.6976\n",
      "Epoch 24/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3622 - accuracy: 0.8833 - val_loss: 0.9018 - val_accuracy: 0.7262\n",
      "Epoch 25/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3796 - accuracy: 0.8781 - val_loss: 0.7196 - val_accuracy: 0.7755\n",
      "Epoch 26/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3605 - accuracy: 0.8845 - val_loss: 0.7383 - val_accuracy: 0.7652\n",
      "Epoch 27/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3670 - accuracy: 0.8817 - val_loss: 1.0407 - val_accuracy: 0.6896\n",
      "Epoch 28/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3501 - accuracy: 0.8883 - val_loss: 0.7803 - val_accuracy: 0.7835\n",
      "Epoch 29/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3677 - accuracy: 0.8837 - val_loss: 0.9678 - val_accuracy: 0.7423\n",
      "Epoch 30/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3667 - accuracy: 0.8815 - val_loss: 0.8299 - val_accuracy: 0.7297\n",
      "Epoch 31/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3762 - accuracy: 0.8832 - val_loss: 0.9770 - val_accuracy: 0.6037\n",
      "Epoch 32/100\n",
      "246/246 [==============================] - 3s 13ms/step - loss: 0.3680 - accuracy: 0.8846 - val_loss: 0.9341 - val_accuracy: 0.7365\n",
      "Epoch 33/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3645 - accuracy: 0.8898 - val_loss: 0.6419 - val_accuracy: 0.8110\n",
      "Epoch 34/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3779 - accuracy: 0.8815 - val_loss: 0.8693 - val_accuracy: 0.7331\n",
      "Epoch 35/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3710 - accuracy: 0.8838 - val_loss: 0.7313 - val_accuracy: 0.7583\n",
      "Epoch 36/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3616 - accuracy: 0.8843 - val_loss: 0.8491 - val_accuracy: 0.7457\n",
      "Epoch 37/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3630 - accuracy: 0.8854 - val_loss: 0.9146 - val_accuracy: 0.6793\n",
      "Epoch 38/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3662 - accuracy: 0.8818 - val_loss: 0.9699 - val_accuracy: 0.6907\n",
      "Epoch 39/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3686 - accuracy: 0.8840 - val_loss: 0.7396 - val_accuracy: 0.7835\n",
      "Epoch 40/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3589 - accuracy: 0.8860 - val_loss: 0.6569 - val_accuracy: 0.8076\n",
      "Epoch 41/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3582 - accuracy: 0.8829 - val_loss: 0.7282 - val_accuracy: 0.7572\n",
      "Epoch 42/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3653 - accuracy: 0.8842 - val_loss: 0.8273 - val_accuracy: 0.7400\n",
      "Epoch 43/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3647 - accuracy: 0.8856 - val_loss: 0.9351 - val_accuracy: 0.6907\n",
      "Epoch 44/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3561 - accuracy: 0.8786 - val_loss: 0.6830 - val_accuracy: 0.7892\n",
      "Epoch 45/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3600 - accuracy: 0.8850 - val_loss: 0.7554 - val_accuracy: 0.7766\n",
      "Epoch 46/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3550 - accuracy: 0.8855 - val_loss: 0.7163 - val_accuracy: 0.7617\n",
      "Epoch 47/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3677 - accuracy: 0.8866 - val_loss: 0.7770 - val_accuracy: 0.7869\n",
      "Epoch 48/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3669 - accuracy: 0.8851 - val_loss: 0.7210 - val_accuracy: 0.7755\n",
      "Epoch 49/100\n",
      "246/246 [==============================] - 3s 12ms/step - loss: 0.3627 - accuracy: 0.8859 - val_loss: 0.6972 - val_accuracy: 0.7789\n",
      "Epoch 50/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3567 - accuracy: 0.8884 - val_loss: 0.7321 - val_accuracy: 0.7732\n",
      "Epoch 51/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3572 - accuracy: 0.8864 - val_loss: 0.7178 - val_accuracy: 0.7617\n",
      "Epoch 52/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3426 - accuracy: 0.8894 - val_loss: 0.6836 - val_accuracy: 0.7869\n",
      "Epoch 53/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3563 - accuracy: 0.8855 - val_loss: 0.9071 - val_accuracy: 0.7285\n",
      "Epoch 54/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3658 - accuracy: 0.8820 - val_loss: 0.9642 - val_accuracy: 0.6919\n",
      "Epoch 55/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3569 - accuracy: 0.8917 - val_loss: 0.6219 - val_accuracy: 0.8179\n",
      "Epoch 56/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3743 - accuracy: 0.8834 - val_loss: 0.6963 - val_accuracy: 0.7755\n",
      "Epoch 57/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3431 - accuracy: 0.8934 - val_loss: 0.7105 - val_accuracy: 0.7663\n",
      "Epoch 58/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3732 - accuracy: 0.8837 - val_loss: 0.8380 - val_accuracy: 0.7503\n",
      "Epoch 59/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3504 - accuracy: 0.8920 - val_loss: 0.8979 - val_accuracy: 0.7411\n",
      "Epoch 60/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3630 - accuracy: 0.8856 - val_loss: 0.6497 - val_accuracy: 0.8041\n",
      "Epoch 61/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3608 - accuracy: 0.8840 - val_loss: 0.8064 - val_accuracy: 0.7595\n",
      "Epoch 62/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3583 - accuracy: 0.8912 - val_loss: 0.7665 - val_accuracy: 0.7022\n",
      "Epoch 63/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3432 - accuracy: 0.8907 - val_loss: 0.8207 - val_accuracy: 0.6919\n",
      "Epoch 64/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3576 - accuracy: 0.8873 - val_loss: 0.6904 - val_accuracy: 0.7950\n",
      "Epoch 65/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3493 - accuracy: 0.8918 - val_loss: 0.7832 - val_accuracy: 0.7537\n",
      "Epoch 66/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3635 - accuracy: 0.8855 - val_loss: 0.5889 - val_accuracy: 0.8236\n",
      "Epoch 67/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3603 - accuracy: 0.8860 - val_loss: 0.8190 - val_accuracy: 0.7595\n",
      "Epoch 68/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3920 - accuracy: 0.8778 - val_loss: 0.6786 - val_accuracy: 0.8133\n",
      "Epoch 69/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3656 - accuracy: 0.8856 - val_loss: 0.6408 - val_accuracy: 0.8190\n",
      "Epoch 70/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3651 - accuracy: 0.8857 - val_loss: 0.6497 - val_accuracy: 0.8099\n",
      "Epoch 71/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3562 - accuracy: 0.8841 - val_loss: 0.7623 - val_accuracy: 0.7709\n",
      "Epoch 72/100\n",
      "246/246 [==============================] - 3s 13ms/step - loss: 0.3721 - accuracy: 0.8834 - val_loss: 0.7038 - val_accuracy: 0.7847\n",
      "Epoch 73/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3666 - accuracy: 0.8840 - val_loss: 0.7691 - val_accuracy: 0.7755\n",
      "Epoch 74/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3586 - accuracy: 0.8859 - val_loss: 0.7427 - val_accuracy: 0.7927\n",
      "Epoch 75/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3610 - accuracy: 0.8827 - val_loss: 0.6553 - val_accuracy: 0.7847\n",
      "Epoch 76/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3564 - accuracy: 0.8884 - val_loss: 0.7397 - val_accuracy: 0.7766\n",
      "Epoch 77/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3513 - accuracy: 0.8888 - val_loss: 0.8242 - val_accuracy: 0.7079\n",
      "Epoch 78/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3625 - accuracy: 0.8868 - val_loss: 0.8987 - val_accuracy: 0.7675\n",
      "Epoch 79/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3580 - accuracy: 0.8887 - val_loss: 0.8657 - val_accuracy: 0.7308\n",
      "Epoch 80/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3461 - accuracy: 0.8868 - val_loss: 0.6918 - val_accuracy: 0.8259\n",
      "Epoch 81/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3752 - accuracy: 0.8842 - val_loss: 1.0580 - val_accuracy: 0.7125\n",
      "Epoch 82/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3579 - accuracy: 0.8857 - val_loss: 0.8623 - val_accuracy: 0.7434\n",
      "Epoch 83/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3667 - accuracy: 0.8883 - val_loss: 0.7040 - val_accuracy: 0.8167\n",
      "Epoch 84/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3572 - accuracy: 0.8856 - val_loss: 0.7603 - val_accuracy: 0.7961\n",
      "Epoch 85/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3604 - accuracy: 0.8876 - val_loss: 0.7653 - val_accuracy: 0.7102\n",
      "Epoch 86/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3619 - accuracy: 0.8847 - val_loss: 0.7584 - val_accuracy: 0.7824\n",
      "Epoch 87/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3609 - accuracy: 0.8842 - val_loss: 1.1077 - val_accuracy: 0.6690\n",
      "Epoch 88/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3297 - accuracy: 0.8926 - val_loss: 0.8696 - val_accuracy: 0.7663\n",
      "Epoch 89/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3509 - accuracy: 0.8873 - val_loss: 1.0304 - val_accuracy: 0.7423\n",
      "Epoch 90/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3452 - accuracy: 0.8917 - val_loss: 0.6955 - val_accuracy: 0.7801\n",
      "Epoch 91/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3674 - accuracy: 0.8845 - val_loss: 0.6700 - val_accuracy: 0.8133\n",
      "Epoch 92/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3469 - accuracy: 0.8920 - val_loss: 0.7979 - val_accuracy: 0.7721\n",
      "Epoch 93/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3482 - accuracy: 0.8896 - val_loss: 0.6877 - val_accuracy: 0.7686\n",
      "Epoch 94/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3683 - accuracy: 0.8860 - val_loss: 0.9961 - val_accuracy: 0.6964\n",
      "Epoch 95/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3283 - accuracy: 0.8949 - val_loss: 0.9323 - val_accuracy: 0.6838\n",
      "Epoch 96/100\n",
      "246/246 [==============================] - 2s 9ms/step - loss: 0.3639 - accuracy: 0.8875 - val_loss: 0.9250 - val_accuracy: 0.7468\n",
      "Epoch 97/100\n",
      "246/246 [==============================] - 2s 10ms/step - loss: 0.3752 - accuracy: 0.8803 - val_loss: 0.7181 - val_accuracy: 0.7938\n",
      "Epoch 98/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3398 - accuracy: 0.8932 - val_loss: 1.1533 - val_accuracy: 0.6884\n",
      "Epoch 99/100\n",
      "246/246 [==============================] - 3s 10ms/step - loss: 0.3629 - accuracy: 0.8890 - val_loss: 0.7112 - val_accuracy: 0.8110\n",
      "Epoch 100/100\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.3506 - accuracy: 0.8874 - val_loss: 0.6274 - val_accuracy: 0.8190\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 157, 320, 1), found shape=(None, 4000)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gapmd\\workspace\\GitHub projects\\AC II - project\\AC II - project (git)\\CNN.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m mlp_conf_matrices\u001b[39m.\u001b[39mappend(mlp_conf_matrix)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X34sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# training the CNN\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X34sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m cnn\u001b[39m.\u001b[39;49mfit(X_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X34sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m y_pred \u001b[39m=\u001b[39m cnn\u001b[39m.\u001b[39mpredict(X_test,n_batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/gapmd/workspace/GitHub%20projects/AC%20II%20-%20project/AC%20II%20-%20project%20%28git%29/CNN.ipynb#X34sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# performance metrics for MLP\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepboatqz1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\gapmd\\anaconda3\\envs\\UrbanSound\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 157, 320, 1), found shape=(None, 4000)\n"
     ]
    }
   ],
   "source": [
    "mlp_accuracies = []\n",
    "cnn_accuracies = []\n",
    "mlp_conf_matrices = []\n",
    "cnn_conf_matrices = []\n",
    "for i in range(10):\n",
    "    tf.keras.backend.clear_session()\n",
    "    test_df = set_df.drop(set_df[set_df['fold'] != i+1].index)\n",
    "    train_df = set_df.drop(set_df[set_df['fold'] == i+1].index)\n",
    "\n",
    "    X_train = np.array(train_df['audio'].tolist())\n",
    "    X_test = np.array(test_df['audio'].tolist())\n",
    "    y_train = np.array(train_df['label'].tolist())\n",
    "    y_test = np.array(test_df['label'].tolist())\n",
    "    \n",
    "    # training the MLP\n",
    "    n_epochs = 100\n",
    "    n_batch_size = 32\n",
    "    mlp.fit(X_train, y_train, batch_size=n_batch_size, epochs=n_epochs, validation_data=(X_test, y_test))\n",
    "    y_pred = mlp.predict(X_test,n_batch_size)\n",
    "    \n",
    "    # performance metrics for MLP\n",
    "    accuracy = mlp.evaluate(X_test,y_test,verbose=0)[1]\n",
    "    mlp_accuracies.append(accuracy)\n",
    "    mlp_conf_matrix = multilabel_confusion_matrix(y_test, np.rint(y_pred))    \n",
    "    mlp_conf_matrices.append(mlp_conf_matrix)\n",
    "    \n",
    "    \n",
    "    # # getting datasets with MFCCs\n",
    "    \n",
    "\n",
    "    # # training the CNN\n",
    "    # cnn.fit(X_train,y_train,epochs=5,batch_size=64)\n",
    "    # y_pred = cnn.predict(X_test,n_batch_size)\n",
    "    \n",
    "    # # performance metrics for CNN\n",
    "    # accuracy = cnn.evaluate(X_test,y_test,verbose=0)[1]\n",
    "    # cnn_accuracies.append(accuracy)\n",
    "    # cnn_conf_matrix = multilabel_confusion_matrix(y_test, np.rint(y_pred))\n",
    "    # cnn_conf_matrices.append(cnn_conf_matrix)\n",
    "    \n",
    "    print(f\"fold {i} done\")\n",
    "    \n",
    "mlp_avg_accuracy = np.mean(mlp_accuracies)\n",
    "mlp_std_dev_accuracy = np.std(mlp_accuracies)\n",
    "mlp_avg_conf_matrix = np.mean(mlp_conf_matrices,axis=0)\n",
    "\n",
    "cnn_avg_accuracy = np.mean(cnn_accuracies)\n",
    "cnn_std_dev_accuracy = np.std(cnn_accuracies)\n",
    "cnn_avg_conf_matrix = np.mean(cnn_conf_matrices,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[832.   5.]\n",
      "  [  1.  35.]]\n",
      "\n",
      " [[739.  34.]\n",
      "  [ 14.  86.]]\n",
      "\n",
      " [[773.   0.]\n",
      "  [ 64.  36.]]\n",
      "\n",
      " [[773.   0.]\n",
      "  [ 13.  87.]]\n",
      "\n",
      " [[777.   0.]\n",
      "  [ 48.  48.]]\n",
      "\n",
      " [[838.   0.]\n",
      "  [ 22.  13.]]\n",
      "\n",
      " [[753.   0.]\n",
      "  [  0. 120.]]\n",
      "\n",
      " [[781.   6.]\n",
      "  [ 74.  12.]]\n",
      "\n",
      " [[773.   0.]\n",
      "  [ 11.  89.]]\n",
      "\n",
      " [[773.   0.]\n",
      "  [  0. 100.]]]\n"
     ]
    }
   ],
   "source": [
    "mlp_conf_matrix = multilabel_confusion_matrix(y_test, np.rint(y_pred))\n",
    "conf_matrices = [mlp_conf_matrix,mlp_conf_matrix]\n",
    "avg_conf_matrix = np.mean(conf_matrices,axis=0)\n",
    "print(avg_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing accuracy per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(mlp_accuracies, labels=[1,2,3,4,5,6,7,8,9,10], vert=True, patch_artist=True)\n",
    "plt.title('Box Plot of Accuracy Across Folds')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
